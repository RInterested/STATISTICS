<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###$A^\top A$ ([Gramian matrix](https://en.wikipedia.org/wiki/Gramian_matrix)):
<br>

	
It is a key matrix structure because of the role it plays in orthogonal projection. Covariance matrices are just special cases.

$A^\top A$ is a covariance matrix - [you can define a multivariate normal distribution for which $A\top A$ is the covariance matrix](http://math.stackexchange.com/a/1896633/152225).

This is equivalent to talking about [**symmetric positive semidefinite matrices**](https://en.wikipedia.org/wiki/Positive-definite_matrix) (every s.p.s.d. matrix can be written as $A^\top A$ for some $A$). 

#####PROPERTIES:

 1. Symmetry
 2. Positive semidefinite-*ness* (can be [zero](http://stats.stackexchange.com/a/56833/67822))
 3. Real and positive eigenvalues
 4. The trace is positive (the trace is the sum of eigenvalues)
 5. The determinant is positive (the determinant is the product of the eigenvalues)
 6. The diagonal entries are all positive 
 7. Orthogonal eigenvectors
 8. Diagonalizable as $Q\Lambda Q^T$
 9. It is [possible to obtain a Cholesky decomposition](http://mathoverflow.net/q/155147/74150).
 10. Rank of $A^TA$ is the same as rank of $A$.
 11. $\text{ker}(A^TA)=\text{ker}(A)$

---

[COVARIANCE MATRIX](https://en.wikipedia.org/wiki/Covariance_matrix):

If the entries of a column vector:

$$X=\begin{bmatrix}X_1\\\vdots\\X_n \end{bmatrix}$$

are random variables with finite variance, then the covariance matrix $\Sigma$ is the matrix whose $(i,j)$ entry is the covariance

$$\Sigma_{ij}=cov(X_i,X_j) = E\left[ (X_i - \mu_i)(X_j - \mu_j\right] = E[X_i, X_j]-E[X]E[Y]$$

where $\mu_i = E(X_i)$

is the expected value of the $i$-th entry in the vector $X.$ In other words,

<img width=500, src = "https://cloud.githubusercontent.com/assets/9312897/22763943/9a1ccfa4-ee35-11e6-89b8-bc9fd34a9533.png">

A [more succint definition](http://math.stackexchange.com/a/257250/152225) is $\mathbb E\big((\mathbf X - \mu)(\mathbf X - \mu)^T\big)$ for a random vector \mathbf X \in \mathbb R^n with mean vector $\mu.$

---

From [this post](http://math.stackexchange.com/a/3871/152225):

When the data is centered (zero mean) the covariance matrix is $\mathbf X\mathbf X^\top.$

After obtaining SVD of a data matrix $X = U\Sigma V^\top$, the covariance matrix can be obtained as 

$$XX^\top= U\Sigma^2 U^\top$$

The square roots of the eigenvalues of $XX^\top$ are the singular values of $X.$

---

The sample covariance matrix is:

$$\mathbf{S}=\dfrac{1}{n-1}\sum_{j=1}^{n}(\mathbf{X}_{j}-\bar{\mathbf{X}})(\mathbf{X}_{j}-\bar{\mathbf{X}})^{T}$$

---

####CORRELATION MATRIX:

The covariance matrix of the standardized random variables $\displaystyle X_{i}/\sigma (X_{i})$ for $\displaystyle i=1,\dots ,n$.

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
