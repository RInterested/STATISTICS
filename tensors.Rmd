<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###MATRIX NOTATION:
<br>

This is a work in progress intended to motivate the need for tensors. 

---

From [this series on youtube](https://youtu.be/V5k_rDAoTvg):

Imagining a differential displacement vector in two different coordinate systems, $X$ and $Y:$

<img height="700" width="500" src="https://cloud.githubusercontent.com/assets/9312897/22399401/c603af22-e569-11e6-84ce-573d9a4c2a72.png">

What follows is prdicated on the assumption that we know the equations relating each component ($m$) in the $X$ coordinate system to the $Y$ coordinate frame:

$Y^n = f(X^m)$ and $X^p = g(Y^z).$

The change in coordinates of the differential displacement vector knowing the transformation equations is given by:

$\begin{align}
dy^1 &= \frac{\partial y^1}{\partial x^1} dx^1 + \frac{\partial y^1}{\partial x^2} dx^2 + \frac{\partial y^1}{\partial x^3} dx^3 + \cdots + \frac{\partial y^1}{\partial x^n} dx^n\\
dy^2 &= \frac{\partial y^2}{\partial x^1} dx^1 + \frac{\partial y^2}{\partial x^2} dx^2 + \frac{\partial y^2}{\partial x^3} dx^3+\cdots+\frac{\partial y^2}{\partial x^n} dx^n\\
\vdots\\
dy^d &= \frac{\partial y^d}{\partial x^1} dx^1 + \frac{\partial y^d}{\partial x^2} dx^2 + \frac{\partial y^d}{\partial x^3} dx^3+\cdots+\frac{\partial y^d}{\partial x^n} dx^n
\end{align}$

So any particular component in the new coordinate system would be of the form:

$$dy^n = \frac{\partial y^n}{\partial x^\color{blue}{m}} dx^{\color{blue}{m}} \tag{Einstein's convention}$$

Expressed in matrix form:

$$\begin{bmatrix}
dy^1\\dy^2\\dy^3\\\vdots\\dy^d
\end{bmatrix}=
{\begin{bmatrix}
\frac{\partial y^1}{\partial x^1} & \frac{\partial y^1}{\partial x^2} & \frac{\partial y^1}{\partial x^3} &\cdots& \frac{\partial y^1}{\partial x^n}\\
\frac{\partial y^2}{\partial x^1} & \frac{\partial y^2}{\partial x^2} & \frac{\partial y^3}{\partial x^3} &\cdots& \frac{\partial y^n}{\partial x^n}\\
\vdots&\vdots&\vdots&&\vdots\\
\frac{\partial y^d}{\partial x^1} & \frac{\partial y^d}{\partial x^2} & \frac{\partial y^d}{\partial x^3} &\cdots& \frac{\partial y^d}{\partial x^n}\\
\end{bmatrix}}
\large\color{red}{\begin{bmatrix}
dx^1\\dx^2\\dx^3\\\vdots\\dx^n
\end{bmatrix}}
$$

We can generalize to a vector $V$ (column vector in red), which can be transfored from $X$ to $Y$ coordinate systems as:

$$\bbox[yellow, 5px]{V^n_{(Y)} = \frac{\partial y^{n}}{\partial x^{\color{red}{m}}}\;V^{\color{red}{m}}_{(X)}}$$

Notice that in this case $n = d$ (the $d$ in the matrix above), while $m$ is a dummy index, but it is in this case equal to $n$. So $n = d$ is the [dimension](http://www.math.com/tables/oddsends/vectordefs.htm) of the vector in $Y$, or the number of rows of the transformation matrix; and $m$ is the number of columns, or the dimension of the vector in the $X$ coordinate system.

If we can find the vector component $n$ in $Y$ of $V$ by taking these types of derivatives, we talk about a **contravariant vector**. The components are expressed as a superscript.

Now let's take two contravariant vectors: $A_Y^m$ ($m$-th component of the $A$ vector in the $Y$ frame): $\large A_{(Y)}^m= \frac{\partial y^m}{\partial x^r} A_{(X)}^r$; and the second vector $\large B_{(Y)}^n= \frac{\partial y^n}{\partial x^s} B_{(X)}^s.$

If we multiply these two vectors together:

$$\large A^m_{(Y)} B^n_{(Y)}$$

we have to take the $d$ number of components that $A$ has and modify each one by the $d$ number of components $B$ has, expressing it as:

$$\large \bbox[10px, border:2px solid red]{T^{mn}_{(Y)} = \large A^m_{(Y)} B^n_{(Y)} =\frac{\partial y^m}{\partial x^r} \; \frac{\partial y^n}{\partial x^s}\; A_{(X)}^r\; B_{(X)}^s= \frac{\partial y^m}{\partial x^\color{blue}{r}} \; \frac{\partial y^n}{\partial x^\color{blue}{s}}\;T^{\color{blue}{rs}}_{(X)}}.$$

These are **contravariant tensors** of the second rank.

To move on to **covariant tensors** it is necessary to discuss what a **gradient vector** is:

<br>

<img width="500" height="500" src="https://cloud.githubusercontent.com/assets/9312897/22402162/543a3300-e5bb-11e6-9454-a0557067fb14.png">

So if a scalar $\varphi$ is a function of $X^1$ and $X^2$, and we see a differential displacement $\vec{dl}$, the change in $\varphi$ will be given by:

$$d\varphi =\underset{\text{grad. vec.}}{\underbrace{\frac{\partial \varphi}{\partial x^1}}}\,dx^1 + \underset{\text{grad. vec.}}{\underbrace{\frac{\partial \varphi}{\partial x^2}}}\,dx^2\tag 1$$


NOTICE THE FOLLOWING: The gradient vector is in the dual space, taking in a "regualar" vector and producing a scalar. In the case of the contravariant vector, a vector in a coordinate frame was transformed into also a vector a different frame.

We also have that 

$$\vec{dl}= dx^1 \vec{X^1} + dx^2 \vec{X^2}\tag 2$$ 

with $\vec{X^1}$ and $\vec{X^2}$ representing the unit vectors.

We want a vector that dotted with equation $(2)$ results in equation $(1).$ Keeping in mind that $\vec{X^1}$ and $\vec{X^2}$ are unit vectors, the vector we are looking for is the **gradient of the scalar $\varphi$**:
 
$$\vec \nabla \varphi= \frac{\partial \varphi}{\partial x^1}\,\vec{X^1} + \frac{\partial \varphi}{\partial x^2}\,\vec{X^2}\tag 3$$

Here's the dot product:

$$d\varphi=\vec{dl}\,\vec{\nabla}\varphi=\color{brown}{ \begin{bmatrix}dx^1 \vec{X^1} & dx^2 \vec{X^2} \end{bmatrix} \begin{bmatrix} \frac{\partial \varphi}{\partial x^1}\,\vec{X^1} \\ \frac{\partial \varphi}{\partial x^2}\,\vec{X^2} \end{bmatrix}}=\frac{\partial \varphi}{\partial x^1}\,dx^1 + \frac{\partial \varphi}{\partial x^2}\,dx^2$$

So,

$$d\varphi = \vec{dl}\,\vec{\nabla}\varphi$$

Generalizing equation $(3)$,

$$\vec\nabla\varphi=\underset{coord. comp. grad. vec.}{\underbrace{{\Large{\frac{\partial\varphi}{\partial x^\color{blue}{m}}}}}}\;\vec{X^\color{blue}{m}}$$

is the expression of the gradient in the $X$ coordinate frame. In the $Y$ coordinate frame it would be:

$$\vec\nabla\varphi={\Large{\frac{\partial\varphi}{\partial y^\color{blue}{n}}}}\;\vec{Y^\color{blue}{n}}$$

Applying the chain rule:

$$\color{red}{\frac{\partial \varphi}{\partial y^n}}= \frac{\partial \varphi}{\partial x^m} \frac{\partial x^m}{\partial y^n}=\frac{\partial x^m}{\partial y^n}\color{red}{\frac{\partial \varphi}{\partial x^m}}$$

This last equation relates the components of the *gradient vector* in the $X$ coordinate frame to the components in the $Y$ frame.

Notice that the arrangement of the dummy indices is:

$$\frac{\partial \varphi}{\partial y^n}= \frac{\partial x^{\color{red}{m}}}{\partial y^n}\frac{\partial \varphi}{\partial x^{\color{red}{m}}}$$


In matrix form:

$$\begin{bmatrix}
\frac{\partial \varphi}{\partial y^1}\\\frac{\partial \varphi}{\partial y^2}\\\frac{\partial \varphi}{\partial y^3}\\\vdots\\\frac{\partial \varphi}{\partial y^d}
\end{bmatrix}=
{\begin{bmatrix}
\frac{\partial x^1}{\partial y^1} & \frac{\partial x^2}{\partial y^1} & \frac{\partial x^3}{\partial y^1} &\cdots& \frac{\partial x^n}{\partial y^1}\\
\frac{\partial x^1}{\partial y^2} & \frac{\partial x^2}{\partial y^2} & \frac{\partial x^3}{\partial y^2} &\cdots& \frac{\partial x^n}{\partial y^2}\\
\vdots&\vdots&\vdots&&\vdots\\
\frac{\partial x^1}{\partial y^d} & \frac{\partial x^2}{\partial y^d} & \frac{\partial x^3}{\partial y^d} &\cdots& \frac{\partial x^n}{\partial y^d}\\
\end{bmatrix}}
\large\color{red}{\begin{bmatrix}
\frac{\partial \varphi}{\partial x^1}\\\frac{\partial \varphi}{\partial x^2}\\\frac{\partial \varphi}{\partial x^3}\\\vdots\\\frac{\partial \varphi}{\partial x^n}
\end{bmatrix}}
$$

This arrangement (red column vector - a gradient vector in coordinate system $X$) is the form that defines **covariant vectors** - for example $W:$

$$\bbox[yellow, 5px]{W^{(Y)}_n = \frac{\partial x^{\color{red}{m}}}{\partial y^n}\, W^{(X)}_{\color{red}{m}}}$$

Their components transform from one to another coordinate system like gradient vectors do. The components are subscripts!

LetÂ´s say we have two covariant vectors $A$ and $B$ with $d$ components:

$$C_m^{(y)}=\frac{\partial x^r}{\partial y^m} C_r^{(x)}$$

$$D_n^{(y)}=\frac{\partial x^s}{\partial y^n} D_s^{(x)}$$

Multiplying them,

$$C_m^{(y)}D_n^{(y)}=\frac{\partial x^r}{\partial y^m}{(y)}\frac{\partial x^s}{\partial y^n}C_r^{(x)}D_s^{(x)}$$

<br>

$$\Large \bbox[10px, border:2px solid red]{T_{mn}^{\small(Y)}= \frac{\partial x^{\color{blue}{r}}}{\partial y^m}\frac{\partial x^{\color{blue}{s}}}{\partial y^n}T_{\color{blue}{rs}}^{(x)}}$$

There are mixed tensors, such as:

$$\Large T^n_m{\small (Y)} =\frac{\partial x^{\color{red}{r}}}{\partial y^m}\frac{\partial y^n}{\partial x^{\color{blue}{s}}}T^{\color{blue}{s}}_{\color{red}{r}}\small (X)$$

---

In a generalized curvilinear coordinate system, the three lines in the diagram can represent the magnitude or position of [spherical coordinates](http://mathworld.wolfram.com/SphericalCoordinates.html):


<img width="1000" src="https://cloud.githubusercontent.com/assets/9312897/22622608/e8e9fa70-eb0c-11e6-8d1d-6acd6d35d1d4.png">

with the two angles involved (in $3$ dimensional space) assigned to the other two curvilinar coordinates - say $u_1$ represents the magnitude in spherical; $u_2$ is for the $\theta$ angle; and $u_3$ stands for $\phi.$

<img width=1000 src="https://cloud.githubusercontent.com/assets/9312897/22622871/7bdddf24-eb15-11e6-9a8f-09dede1b9061.png">

The $\varepsilon_i$ vectors are tangent to the different coordinate axes; while the $\epsilon_i$ vectors are perpendicular to the coordinates.

The assumption is that we know how to transform from Cartesian to curvilinear, just like with spherical coordinates (trigonometric formulas).

So we would have functions of the type:

$$u^1 = f(x^1,x^2,x^3)$$
$$u^2 = g(x^1,x^2,x^3)$$
$$u^3 = h(x^1,x^2,x^3)$$

and

$$x^1 = \hat f (u^1,u^2,u^3)$$
$$x^2 = \hat g (u^1,u^2,u^3)$$
$$x^3 = \hat h(u^1,u^2,u^3)$$

Therefore, the position vector $\vec r$ can be described as:

$$\vec r = x^1 \, \vec e_1 + x^2 \vec e_2 + x^3 \vec e_3$$ 

which is equivalent to

$$\vec r = \hat f (u^1,u^2,u^3) \, \vec e_1 + \hat g (u^1,u^2,u^3) e_2 + \hat h (u^1,u^2,u^3) e_3$$

and we can write the displacement of $\vec r:$

$$d\vec r = \frac{\partial \vec r}{\partial u^1} du^1+\frac{\partial \vec r}{\partial u^2}du^2+ \frac{\partial \vec r}{\partial u^3}du^3$$

Here is a key observation: the partial derivatives with respect to $u_i$ **are** the $\varepsilon_i$ **tangent vectors:**

$$\frac{\partial \vec r}{\partial u^i} = \varepsilon_i$$

<img width = 400 src="https://cloud.githubusercontent.com/assets/9312897/22623202/336aa128-eb1f-11e6-9b22-694507fe257a.png">

These are not unit vectors, but can be normalized dividing by the norm:

$$\lVert \frac{\partial \vec r}{\partial u^i}\rVert$$

This fact make it very easy to express a vector $\vec A$ as the tangent vectors multiplied by a scalar:

<img width= 300 src="https://cloud.githubusercontent.com/assets/9312897/22622899/71a38012-eb16-11e6-90c4-c6305118dd62.png">

$$\vec A = a(1)\, \varepsilon_1 + a(2) \, \varepsilon_2 + a(3) \, \varepsilon_3$$

In practice this is written as:

$$\vec A = a^1\, \varepsilon_1 + a^2 \, \varepsilon_2 + a^3 \, \varepsilon_3$$

because if we were to transform to another curvilinear frame it would transform as a *contravariant* vector. 

For instance, let's transform from curvilinear frame $\vec r(u^1, u^2, u^3)$ to $\vec r(u^{'1}, u^{'2}, u^{'3}),$ with $\vec r$ corresponding to the positional vector:

So in the $U$ system, $\large \vec \varepsilon_i=\frac{\partial \vec r}{\partial u^i}$; while for the $U'$ system, $\large \vec \varepsilon_i=\frac{\partial \vec r}{\partial u^{'i}}.$ They transform as:

$$\color{fuchsia}{\large \vec \varepsilon_j} = \frac{\partial \vec r}{\partial u^j}=  \frac{\partial u^{'i}}{\partial u^j} \frac{\partial \vec r}{\partial u^{'i}}= \color{fuchsia}{\frac{\partial u^{'i}}{\partial u^j} \vec \varepsilon'_i}$$

and

$$\large \vec \varepsilon'_i = \frac{\partial u^{j}}{\partial u^{'i}} \vec \varepsilon_j$$

The vector stays the same:

$$\large \vec A = a^{'i}\vec \varepsilon'_i = a^j \color{fuchsia}{\vec\varepsilon_j}= a^j \frac{\partial u'^i}{\partial u^j}\vec\varepsilon'_i$$

or, simplifying $\large \vec \varepsilon'_i:$

$$\large a'^{i} = \frac{\partial u'^i}{\partial u^\color{blue}{j}} a^{\color{blue}{j}},$$

which is a contravariant transformation.

On the other hand we can express $\vec A$ as the sum of the components in the different orthogonal $\varepsilon_i$ in the diagram above, in which case it would transform as a *covariant* vector.

---

#### RECIPROCAL BASIS:

Recap:

We can express the $\vec r$ vector in the curvilinear coordinates (diagrams above), as functions of the components of the individual curvilinear coordinates.

$$\vec r = \hat f (u^1,u^2,u^3) \, \vec e_1 + \hat g (u^1,u^2,u^3) \vec e_2 + \hat h (u^1,u^2,u^3) \vec e_3$$

and we can obtain the partial derivative with respect to any of the curvilinear coordinates, and we get one of the tangential vectors ($\varepsilon_i$):

$$\bbox[10px, border:2px solid aqua]{\varepsilon_i = \frac{\partial \vec r}{\partial u^i}= \frac{\partial x^1}{\partial u^i} \vec e_1 + \frac{\partial x^2}{\partial u^i} \vec e_2 + \frac{\partial x^3}{\partial u^i} \vec e_3}$$

We want to write the gradient of the cuvilinear coordinates $U^j$. Now remembering Eq. 3 above:

$$\vec \nabla \varphi= \frac{\partial \varphi}{\partial x^1}\,\vec{X^1} + \frac{\partial \varphi}{\partial x^2}\,\vec{X^2}\tag 3$$

we can write:

$$\bbox[10px, border:2px solid aqua]{\varepsilon^j =\vec \nabla u^j= \frac{\partial u^j}{\partial x^1}\,\vec e_1 + \frac{\partial u^j}{\partial x^2}\,\vec e_2 + \frac{\partial u^j}{\partial x^3}\,\vec e_3}$$

for $j=\{1,2,3\}$

Taking as example the spherical coordinates, and specifically the $r$ component (see 3D rendition above):

$r = \left((x^1)^2+(x^2)^2+(x^3)\right)^{1/2}$

we can take the partial with respect to each one of the Cartesian components:

$$\varepsilon^r =\frac{\partial \left((x^1)^2+(x^2)^2+(x^3)\right)^{1/2}}{\partial x^l}\,\vec e_1 + \frac{\partial \left((x^1)^2+(x^2)^2+(x^3)\right)^{1/2}}{\partial x^2}\,\vec e_2 + \frac{\partial \left((x^1)^2+(x^2)^2+(x^3)\right)^{1/2}}{\partial x^3}\,\vec e_3$$

For the angle $\phi = \tan^{-1}(\frac{x^2}{x^1})$,

$$\varepsilon^\phi =\frac{\partial \tan^{-1}(\frac{x^2}{x^1})}{\partial x^l}\,\vec e_1 + \frac{\tan^{-1}(\frac{x^2}{x^1})}{\partial x^2}\,\vec e_2 + \frac{\partial \tan^{-1}(\frac{x^2}{x^1})}{\partial x^3}\,\vec e_3$$

etc.

Dotting $\varepsilon_i$ with $\varepsilon^j:$

$$\bbox[10px, border:2px solid aqua]{\varepsilon_i \cdot \varepsilon^j} = \frac{\partial x^1}{\partial u^i}\frac{\partial u^j}{\partial x^1}+\frac{\partial x^2}{\partial u^i}\frac{\partial u^j}{\partial x^2}+\frac{\partial x^3}{\partial u^i}\frac{\partial u^j}{\partial x^3}
=\bbox[10px, border:2px solid aqua]{\frac{\partial u^j}{\partial u^i}}$$


---

Notice that it is not $3\,\frac{\partial u^j}{\partial u^i}$, because the partials add up to $1$: Let's consider a differential in general:

$$dw=\frac{\partial w}{\partial x^1} dx^1 + \frac{\partial w}{\partial x^2} dx^2+\frac{\partial w}{\partial x^3} dx^3$$

Now dividing by $dw:$

$$1=\frac{\partial w}{\partial x^1} dx^1 + \frac{\partial w}{\partial x^2} dx^2+\frac{\partial w}{\partial x^3} dx^3$$

we are adding partials together to get the entire component.

---


If $\large j=i$, $\large \varepsilon_i\cdot e^j = 1$

If $\large j \neq i$, $\large \varepsilon_i\cdot e^j$, and since the axes $(u^1, u^2, u^3)$ are independent, $\frac{\partial u^j}{\partial u^i}=0.$

Hence, we can express this as $\large\bbox[10px, border:2px solid aqua]{\varepsilon_i \cdot \varepsilon^j = \frac{\partial \vec r}{\partial u^i}\,\vec \nabla u^j=\delta_i^{\;j}}$

So these coordinate axes are independent of each other. $\large \vec \nabla u^j$ correspond to the orthogonal vectors to the generalized curvilinear coordinates symbolized by the $\vec\epsilon_i$ vectors on the diagram comparing tangential to orthogonal components.


<br><br>

---

A concrete explanation can be found [here](https://youtu.be/JY_lUOWClEY), and it proceeds as follows:

We have two inertial (no acceleration) coordinate systems, $S$ and $S'$, where $S'$ moves away from $S$ at constant velocity $v$ *along* the $x$ axis - for now we assume no rotation or movement along the other two axes, $y$ and $z$.

An event $P$ happens and it is recorded in $S$ as $(ct, x, y, z)$. Why $ct$? To express time in the same units as the space coordintes we multiply the velocity of light by the observation of time. Not important.

The same event $P$ will be expressed in relation to the $S'$ coordinates as $P=(ct', x', y',z').$

####Definition: CONTRAVARIANT VECTOR:

$x^\mu = (x^0,x^1,x^2,x^3)$, such that

$x^0= ct, \quad x^1=x, \quad x^2=y, \quad x^3=z$

So $\mu$ is an index, $\mu= 0,1,2,3.$

####Definition: COVARIANT VECTOR:

$x_\mu=(x_0,x_1,x_2,x_3)$, such that

$x_0=ct, \quad x_1=\color{red}{-}x, \quad\color{red}{-}y \quad\color{red}{-}z$

Why do we need this?

The contravariant and covariant vectors of the same event in relation to the frame $S'$ are:

$x'^\mu=(x^{'0}, x^{'1},x^{'2},x^{'3})\quad \text{contravariant}$

and

$x'_\mu=(x'_0, x'_1,x'_2,x'_3)\quad\text{covariant}$

---

Now we have to bring in the Lorentz transformations for the answer to make sense. They are the equations that change coordinates between $S$ and $S'$ taking into consideration time dilation and length contraction (along the $x$ axis only, since the frames are only moving with respect to each other along $x$):

The transformation of time from $t$ in frame $S$ to $t'$ in $S'$ is given by $t' = \gamma \left(t - \frac{v}{c^2}x\right)$; while the transformation of $x$ from $S$ to $S'$ obeys the equation $x' =\gamma(x - vt).$ This is explained [here](https://youtu.be/-O8EO50MyiA), and adopting the notation of contravariant and covariant vectors, and the $ct$ expression of $t$:

$x'^0 = \gamma\left(x^0 - \frac{v}{c} x^1\right)$

$x'^1 = \gamma\left(x^1 - \frac{v}{c} x^0\right)$

$x'^2 = x^2$

$x'^3 = x^3$

Here, $\large\gamma=\frac{1}{\sqrt{1-\frac{v^2}{c^2}}}$. Notice that the third and fourth dimension do not change in this simple model with two frames, one moving along the $x$ axis of the other at constant velocity.

We can express the change in frames in matrix form as:

$$\begin{bmatrix}x'^0\\x'^1\\x'^2\\x'^3\end{bmatrix}=\begin{bmatrix}\gamma&-\gamma\beta&0&0\\-\gamma\beta&\gamma&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix}\begin{bmatrix}x^0\\x^1\\x^2\\x^3\end{bmatrix}$$

with $\beta=\frac{v}{c}.$

If there is motion of $S'$ in more than just the $x$ difrection, the matriceal formula above can be generalized as:

$$\begin{bmatrix}x'^0\\x'^1\\x'^2\\x'^3\end{bmatrix}=\begin{bmatrix}\Lambda^0_{\; 0}&\Lambda^0_{\; 1}&\Lambda^0_{\; 2}&\Lambda^0_{\; 3}\\\Lambda^1_{\; 0}&\Lambda^1_{\; 1}&\Lambda^1_{\; 2}&\Lambda^1_{\; 3}\\\Lambda^2_{\; 0}&\Lambda^2_{\; 1}&\Lambda^2_{\; 2}&\Lambda^2_{\; 3}\\\Lambda^3_{\; 0}&\Lambda^3_{\; 1}&\Lambda^3_{\; 2}&\Lambda^3_{\; 3}\end{bmatrix}\begin{bmatrix}x^0\\x^1\\x^2\\x^3\end{bmatrix}$$

This can be express more succintly as:

$$\color{red}{x'^{\mu} = \Lambda^\mu_{\; \nu}x^\nu}$$

The same can be done with the covariant vector transformation:

$$\begin{bmatrix}x'_0\\x'_1\\x'_2\\x'_3\end{bmatrix}=\begin{bmatrix}\Lambda_0^{\; 0}&\Lambda_0^{\; 1}&\Lambda_0^{\; 2}&\Lambda_0^{\; 3}\\\Lambda_1^{\; 0}&\Lambda_1^{\; 1}&\Lambda_1^{\; 2}&\Lambda_1^{\; 3}\\\Lambda_2^{\; 0}&\Lambda_2^{\; 1}&\Lambda_2^{\; 2}&\Lambda_2^{\; 3}\\\Lambda_3^{\; 0}&\Lambda_3^{\; 1}&\Lambda_3^{\; 2}&\Lambda_3^{\; 3}\end{bmatrix}\begin{bmatrix}x_0\\x_1\\x_2\\x-3\end{bmatrix}$$

or

$$\color{blue}{x'_{\mu} = \Lambda_\mu^{\; \nu}x_\nu}$$

The Lorentz transformation $\Lambda$ is defined so that $x'^{\mu}x'_{\mu} = x^\mu x_mu$ (i.e. $\small \text{contravariant in S'} \times \text{covariant in S'}=\text{contravariant in S} \times \text{covariant in S}$).

We can express it as:

$$\Large \color{red}{x'^{\mu}}\,\color{blue}{x'_{\mu}} = x^\mu \,x_\mu= \color{red}{\Lambda^\mu_{\;\;\nu}\,x^\nu}\,\color{blue}{\Lambda_\mu^{\;\;\rho}\,x_\rho}= \Lambda^{\mu}_{\;\;\nu}\,\Lambda_\mu^{\;\;\rho}\,\,x^\nu \,x_\rho=\delta^\rho_{\;\;\nu}\,x^\nu\,x_\rho$$

Why does it work? Well, when $\rho=\nu=\mu$ the Kronecker product is one, and the final part of the equation above will be $x^\mu\,x_\mu,$ fulfilling the requisite of $x'^{\mu}x'_{\mu} = x^\mu x_\mu.$

Leading to the important result:

$$\large\color{orange}{\Lambda^\mu_{\;\;\nu}\,\Lambda_\mu^{\;\;\rho}=\delta^\rho_{\;\;\nu}}$$

---

OK. So now we have covariant and contravariant vectors expressing the event with respect to $S$ and $S'$... and Lorentz transformations... we can do this...

$$\large x^\mu x_\mu= x^0 x_0 + x^1 x_1 + x^2 x_2 + x^3 x_3 = c^2t^2 - \left(x^2 + y^2 + z^2 \right)=\color{red}{c^2t^2 -  \vec{x}\vec{x}}$$

$\vec{x}\vec{x}$ is the norm! And the result is a scalar (field value).

How to express this operation generally? We use the Einstein summation convention:

$$A^\mu B_\mu=A^0B_0+A^1B_1+A^2B_2+A^3B_3$$

where $A^\mu =(A^0,A^1,A^2,A^3)$ has a covariant vector $A_mu=(A_0,A_1,A_2,A_3) = (A_0,-A^1,-A^2,-A^3).$ And so does $B^\mu.$

---

###TENSORS:

<img WIDTH="900" src="https://cloud.githubusercontent.com/assets/9312897/22192324/14d7461c-e101-11e6-9cf6-05e5c4988aa7.png">

---

###CHANGE OF COORDINATES BETWEEN CARTESIAN TO OBLIQUE:

From [this video](https://youtu.be/AKPZkHvqTao?list=PL6oNjS6Kc-nTIPkICpvWC1lBEBRsXrIlg).


From [this introductory differential geometry series online](https://youtu.be/AKPZkHvqTao?list=PLD1448E024E04C24D) ushering in the concepts of **contravariant** ($A^i$) and **covariant** ($A_i$) of a vector as a change of coordinates:


In the setting of oblique coordinates, $y$ axis is tilted, but the $x$ axis stays unchanged:

<img src="http://i.stack.imgur.com/ESBuw.png" width="400">


The **contravariant** is calculated as:

Assigning to the pink segment the symbol $*$:

$\text{tan}(\alpha) = \frac{b}{*}$; and therefore, $* = \frac{b}{\text{tan}(\alpha)}$

and $c^u = a - * = a - \frac{b}{\text{tan}(\alpha)}$

while $c^v = \frac{b}{\text{sin}(\alpha)}$


$$H = \begin{bmatrix} 1 & - \frac{1}{\text{tan}(\alpha)} \\
                  0 & \frac{1}{\text{sin}(\alpha)}
\end{bmatrix}$$

Therefore,

$$\begin{bmatrix} 1 & - \frac{1}{\text{tan}(\alpha)} \\
                  0 & \frac{1}{\text{sin}(\alpha)}
\end{bmatrix}
\begin{bmatrix} c_x \\ c_y
\end{bmatrix}=
\begin{bmatrix} c^u \\ c^v
\end{bmatrix} = \begin{bmatrix} c^1 \\ c^2
\end{bmatrix}
$$

and

---

On the other hand, the **covariant** is calculated as:



<img src="http://i.stack.imgur.com/y1til.png" width="350">

where,

$c_u = a$ and 


$\bf c_v = a\,\text{cos}(\alpha) + b\, \text{sin}(\alpha)$

---

This expression is derived as follows:


<img src="https://cloud.githubusercontent.com/assets/9312897/22278003/0a71579c-e28e-11e6-8eba-220f242fc233.png" width="800">


The angles $\alpha$ and $\beta$ are complementary, and $\sin \alpha = \cos \beta$.

$\varphi= b \cos \beta = b \sin \alpha$

$\psi = a \cos \alpha$

$c_v = \psi + \varphi = a \cos \alpha + b \sin \alpha$

It follows that

$$M = \begin{bmatrix} 1 & 0 \\ \cos \alpha & \sin \alpha \end{bmatrix}$$

Therefore,

$$\begin{bmatrix} 1 & 0 \\
                  \cos\alpha & \sin \alpha
\end{bmatrix}
\begin{bmatrix} c_x \\ c_y
\end{bmatrix}=
\begin{bmatrix} c_u \\ c_v
\end{bmatrix} = \begin{bmatrix} c_1 \\ c_2
\end{bmatrix}
$$

---


<img src="https://cloud.githubusercontent.com/assets/9312897/18643415/fe9641fa-7e72-11e6-9cb7-648848df8627.png" width="600">

$G$ is the **METRIC TENSOR** turns the covariate into the contravariate:

\begin{align}
\bf G &= H\,M^{-1}\\
&= \frac{1}{\sin\alpha}\begin{bmatrix}1&-\frac{1}{\tan\alpha}\\0&\frac{1}{\sin\alpha}\end{bmatrix}
\begin{bmatrix} \sin\alpha & 0\\-\cos\alpha &1\end{bmatrix}\\
&=\frac{1}{\sin\alpha} \begin{bmatrix}\sin\alpha+\frac{\cos\alpha}{\tan\alpha} & -\frac{1}{\tan \alpha}\\ -\frac{\cos\alpha}{\sin\alpha} & \sin\alpha \end{bmatrix}\\
&=\frac{1}{\sin^2\alpha}\begin{bmatrix}1 & - \cos\alpha\\-\cos\alpha\end{bmatrix}
\end{align}

---

We can prove that the length of a vector in oblique coordinates is **INVARIANT** under the change of coordinates. 

$$L^2 = c_x^2 + c_y^2 = c_1\,c^1 + c_2\,c^2=u_k\,u^k$$

However, when we go from Cartesian to oblique we can't describe the vector as $A = (A_1,A_2)$ with the length calculated as the norm $A_1^2 + A_2^2$ (or as the dot product). Instead, we need covariant $(A_1,A_2)$ and contravariant $(A^1,A^2)$ components with the length calculated as the dot product (or scalar product) - i.e. $A_1A^1+A_2A^2.$

---

The $G$ matrix turns the variant into the contravariant: from subscript indices to superscript indices.

The Einstein notation is: $g^{ij}\,u_j$, meaning the sum over all the $j$ dimensions.

The "famous" metrix tensor is:

$$dS^2 = g^{ij}\,dx_i\,dx_j= dx^j\,dx_j = dx^1dx_1 +dx^2dx_2$$

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
