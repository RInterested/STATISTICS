<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###TENSOR INDICIAL NOTATION:
<br>

####**Definition of geometric vectors:**

Directed segment (i.e. length and angle). It is not an ordered set of numbers. A vector cannot be decomposed into basis vectors, like in linear algebra. Geometric vector are just pictures.

Without a coordinate system the vectors would just be; with a coordinate system, $\color{blue}{\text{vectors become a function of coordinates.}}$

They can be added with the parallelogram rule. They can be subtracted. And because they can be subtracted, they can also be differentiated:

<img width="450" src="https://cloud.githubusercontent.com/assets/9312897/18772533/c5892896-8115-11e6-9529-50ca40029fd1.png">

If there is no change in length the derivative of a directional vector is orthogonal to it:

${\bf v}(t)\cdot{\bf v}(t) = \text{constant}$

The dot vector is an OK operation because it is the length of the vectors times the cosine of the angle between them.

Differentiating, and using the product rule:

$$\frac{d{\bf v}(t)}{dt}{\bf v}(t) + {\bf v}(t)\frac{d{\bf v}(t)}{dt}=0$$

Hence,

$${\bf v'}(t)\cdot {{\bf v}(t)}=0 \tag*{$\blacksquare$}$$

#####Covariant basis:

<img width="300" src="https://cloud.githubusercontent.com/assets/9312897/18786004/bfdbcbc0-8169-11e6-82de-ea41cff457b5.png">

A directional or positional vector $R$ becomes a function of two variables ($r$ and $\theta$) in this polar coordinate system. In general the vector becomes a function of the coordinates $R(Z^1,Z^2)$.

Differentiating will yield the **coordinate vectors of the covariant basis** by definition:

$\large {\bf e_r}= \frac{\partial R(r,\theta)}{\partial r}$

$\large {\bf e_\theta}=\frac{\partial R(r,\theta)}{\partial \theta}$

Here is the intuition behind the derivative definition in the Cartesian system:

<img width="300" src="https://cloud.githubusercontent.com/assets/9312897/18787526/3737b77c-8171-11e6-8663-70f44e9d0d41.png">

Halving the denominator in the $\Delta x$, also halves the vector, yielding the unit vector $i$ along the limit process in $\large \hat i = \frac{\partial R(x,y)}{\partial x}$.

Going back to the radial coordinate system, $e_r$, is going to be the unit vector that points in the radial direction. It is described in words or pictorially - NOT with closed formulas, such as $(r \cos (\theta), r \sin(\theta)).$

At every point, $e_r$ is the unit of length and radially oriented; while $e_\theta$ has length $r$ and is tangential:

<img width="700" src="https://cloud.githubusercontent.com/assets/9312897/18789687/df1c2d1c-8179-11e6-8cf1-561902d59e12.png">

---

From [here](https://youtu.be/hiYgYWJEaMk?t=27m50s) and [here](https://youtu.be/rtEk1at3sVk?t=25m24s):

In the Cartesian coordinate system the arc length of a curve can be parametrized with respect to time:

<img width="500" src="https://cloud.githubusercontent.com/assets/9312897/18792403/633819da-8184-11e6-89d3-c7b3f4cf5283.png">

$$\text{arc length}= S(t) = \int_{t_0}^t \sqrt{\frac{dR}{dt}\cdot \frac{dR}{dt}}dt$$

The curve now can be fully defined in terms of the covariate coordinates $Z^1$ and $Z^2$ at every point in the arc length $s$. The function would be the derivative over $s$ of:

$$R(s) = \left(Z^1(s), Z^2(s)\right)$$

The unit tangent (unit vector tangential to the curve) is:

$$T(s) = \frac{dR(s)}{ds}=\color{blue}{\frac{\partial R}{\partial Z^1}}\frac{dZ^1}{dS} + \color{orange}{\frac{\partial R}{\partial Z^2}}\frac{dZ^2}{dS} = \frac{d Z^1}{ds}\color{blue}{{\bf e_1}} + \frac{dZ^2}{ds} \color{orange}{{\bf e_2}}$$

---

Starting off with a function $F(\color{green}{a},\color{green}{b},\color{green}{c}$ with three variables $\color{green}{a}$, $\color{green}{b}$ and $\color{green}{c}$, where $\color{green}{a} = \color{red}{a}(\mu, \nu)$, $\color{green}{b} = \color{red}{b}(\mu,\nu)$ and $\color{green}{c}=\color{red}{c}(\mu,\nu)$, and symbolizing the role of $a$, $b$ and $c$ as independent variables in $\color{green}{\text{green}}$, and the role of the same letters as functions in $\color{red}{\text{red}}$:

$$f(\mu,\nu)= F\left(\color{red}{a}(\mu, \nu),\color{red}{b}(\mu, \nu), \color{red}{c}(\mu, \nu)\right)$$

$$\frac{\partial f}{\partial \mu}= \frac{\partial F}{\partial \color{red}{a}}\frac{\partial \color{red}{a}}{\partial \mu}    +   \frac{\partial F}{\partial \color{red}{b}}\frac{\partial \color{red}{b}}{\partial \mu}    +    \frac{\partial F}{\partial \color{red}{c}}\frac{\partial \color{red}{c}}{\partial \mu}$$

Trying to simplify notation,

$F(\color{green}{a},\color{green}{b},\color{green}{a}) = F(\color{green}{a^1},\color{green}{a^2},\color{green}{a^3}) = F(\color{green}{a^i}) = F(\color{green}{a})$

and

$f(\mu,\nu) = F(\color{red}{a}(\mu, \nu))$

resulting in:

\begin{align}
\frac{\partial f}{\partial \mu}&= \frac{\partial F}{\partial \color{red}{a^1}}\frac{\partial \color{red}{a^1}}{\partial \mu}    +   \frac{\partial F}{\partial \color{red}{a^2}}\frac{\partial \color{red}{a^2}}{\partial \mu}    +    \frac{\partial F}{\partial \color{red}{a^3}}\frac{\partial \color{red}{a^3}}{\partial \mu}\\
&=\displaystyle \sum_{i=1}^3 \frac{\partial F}{\partial \color{red}{a^i}}\frac{\partial \color{red}{a^i}}{\partial \mu}\\
&=\frac{\partial F}{\partial \color{red}{a^i}}\frac{\partial \color{red}{a^i}}{\partial \mu}\tag{$\text{Einstein's notation}$}
\end{align}

and

$$\frac{\partial f}{\partial \nu} = \frac{\partial F}{\partial \color{red}{a^i}}\frac{\partial \color{red}{a^i}}{\partial \nu}\tag{$\text{Einstein's notation}$}$$

---

From [Wikipedia](https://en.wikipedia.org/wiki/Einstein_notation): when and index variable appears twice in a single term and is not otherwise defined, it implies summation of that term over the values of the index. So where the indices can range over the set $\{1,2,3\}$,

$$\large y = \sum_{i=1}^3 c_i\,x^i= c_1x^1 + c_2x^2 + c_3x^3$$

is simplified to:

$$\large y =c_\color{red}{i}\,x^\color{red}{i}$$

The upper indices are not exponents - they are the indices of the coordinates, coefficients or basis vectors: typically $x^1,x^2,x^3$ are equivalent to $x,y,z.$

---

We can do the same for the independent variables $\mu$ and $\nu$, and just express them as $\mu^1$, $\mu^2$, etc. Or, even simpler, we can combine the two preceding expressions into:

$$f(\mu) =F(a(\mu))$$

and

$$\frac{\partial f}{\partial \color{green}{\mu^\alpha}}=\frac{\partial F}{\partial \color{red}{a^i}}\frac{\color{red}{a^i}}{\partial \color{green}{\mu^\alpha}}$$

$\large \color{green}{\alpha}$ is the **live or free index** (different identities; one for each value of $\alpha$). $\large\color{red}{i}$ represents summation (**dummy, contracted or silent index**).

---

###Example: 

####New coordinate system $\bf(r,\theta)= \left(Z^{1'},Z^{2'}=Z^{i'}\right)$ in terms of the old $\bf (x,y)=\left(Z^1, Z^2\right)=Z^i$ variables:

<br>

$$\Large \color{green}{r}=\color{red}{r}\left( x(\color{green}{r},\color{green}{\theta}), 
y(\color{green}{r},\color{green}{\theta})
\right)\tag 1$$

**All** $r$'s in green are independent variables. $x$ and $y$ are the original variables, which depend on $r$ and $\theta$, i.e. $x(\color{green}{r},\color{green}{\theta})$ and $y(\color{green}{r},\color{green}{\theta})$.  $\color{red}{r}$ is a function - in this case is $\sqrt{x^2 + y^2}$.

Similary,

$$\Large\color{green}{\theta}=\color{red}{\theta}\left( x(\color{green}{r},\color{green}{\theta}), 
y(\color{green}{r},\color{green}{\theta})
\right) \tag 2$$

is such that $\color{red}{\theta}$ is a function: $\arctan(y,x)$.

Differentiating to obtain the **Jacobian of the transformation**:

From Eq.1:

$\Large \frac{\partial\color{red}{r}}{\partial x}  
\frac{\partial x}{\partial \color{red}{r}}
+ \frac{\partial\color{red}{r}}{\partial y}  
\frac{\partial y}{\partial \color{red}{r}}   = 1\tag{1,1}$

$\Large\frac{\partial\color{red}{r}}{\partial x}  
\frac{\partial x}{\partial \color{red}{\theta}}
+ \frac{\partial\color{red}{r}}{\partial y}  
\frac{\partial y}{\partial \color{red}{\theta}}   = 0\tag{1,2}$

From Eq.2:

$\Large\frac{\partial\color{red}{\theta}}{\partial x}  
\frac{\partial x}{\partial \color{red}{r}}
+ \frac{\partial\color{red}{\theta}}{\partial y}  
\frac{\partial y}{\partial \color{red}{r}}   = 0\tag{2,1}$

$\Large\frac{\partial\color{red}{\theta}}{\partial x}  
\frac{\partial x}{\partial \color{red}{\theta}}
+ \frac{\partial\color{red}{\theta}}{\partial y}  
\frac{\partial y}{\partial \color{red}{\theta}}   = 1\tag{2,2}$


If we move on to the $\bf Z^i$ notation:

$$\Large \color{green}{Z^{i'}} = \color{red}{Z^{i'}}\left(Z\right)$$

$\Large \bf Z^1=x$ and $\Large \bf Z^2=y$.

$\color{red}{Z^{1'}}$ is $\sqrt{x^2 + y^2}$ for $\color{green}{\Large \bf Z^{1'}} = r$, and $\color{red}{Z^{2'}}$ is $\arctan(y,x)$ for $\Large \bf \color{green}{Z^{2'}}=\theta.$

Although Eq. 1 and 2 are really:

$$\Large \color{red}{Z^{i'}}\left(\color{red}{Z}(\color{green}{Z'})\right) = \Large \color{green}{Z^{i'}}\tag{3}$$

... wording this..."by taking the $r$, $\theta$ ($\color{red}{Z^{i'}}$) and lookng at how they depend on $x$ and $y$ ($\color{red}{\large Z}$), and plugging the dependence of $x$, $y$ on $r$ and $\theta$, i.e. ($\large \cdot(\color{green}{Z'})$), you recover the $r$ and $\theta$.

The derivative will be expressed as:

$$\Large \frac{\partial Z^{i'}}{\partial Z^{j'}}\tag{4}$$ 

signifying...

For $i=1$ and $j=1$:

$$\frac{\partial r}{\partial r} =  1 \tag{1,1}$$

For $i=1$ and $j=2$:

$$\frac{\partial r}{\partial \theta} =  0 \tag{1,2}$$

$$\vdots$$

So when $i=j$ the partial derivative will be $1$; otherwise, $0$. This is the [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta). Therefore and from Eq. 3:

$$\Large \frac{\partial Z^{i'}}{\partial Z^{j'}}=\Large\delta_{j'}^{i'}\tag{5}$$ 

Re-expressing the partial derivatives of Eq.1 and Eq.2 above:

$$\Large \frac{\partial Z^{i'}}{\partial Z^i} \frac{\partial Z^i}{\partial Z^{j'}} = \Large \delta^{i'}_{j'}\tag{6}$$

---

In indicial notation the matrix multiplication $\bf AB=C$, corresponding to $\sum_k A_{ik}B_{kj}=C_{ij}$:

<br>

$$\bf AB = \bf A^i_k B^k_j = C^i_j$$

This makes the product "commutative":

$$ \bf A^i_k B^k_j = B^k_j A^i_k$$

---

The Jacobian is expressed as:

$\begin{bmatrix} 
\frac{\partial r}{\partial x} &
\frac{\partial r}{\partial y}\\
\frac{\partial \theta}{\partial x} &
\frac{\partial \theta}{\partial y}
\end{bmatrix}$

can now be denoted by:

$$\large J_i^{i'}=\frac{\partial \color{red}{Z^{i'}}}{\partial \color{green}{Z^i}}$$

Reexpressing Eq. 6:

$$\mathbf J_i^{i'} \mathbf J^i_{j'}= \Large \delta^{i'}_{j'}\tag{7} $$

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
