---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###COPULAS:
<br>

The objective is to look at dependency structure in a vector of random variables, knowing their marginal distributions.

We want to convert the random variables and convert them to $U(0,1).$

It is well known that a real-valued, continuous, and strictly monotone function of
a single variable possesses an inverse on its range. It is also known that one can
drop the assumptions of continuity and strict monotonicity (even the assumption of
considering points in the range) to obtain the notion of a **generalized inverse**. One can often work with generalized inverses as one does with ordinary inverses. 

For an increasing function $T : \mathbb R \to \mathbb R$ with $T(−\infty) = \lim_{x \downarrow −\infty} T(x)$ and $T(\infty) =\lim_{x \uparrow \infty} T(x),$ the **generalized inverse** 

$T^- : \mathbb R \to  \bar{\mathbb R} = [−\infty, \infty]$ 

of $T$ is defined by

$$T^-(y)=\inf\{x\in \mathbb R: T(x) \geq y\}, y\in \mathbb R,$$

with the convention that $\inf \emptyset =\infty.$ If $T:\mathbb R\to [0,1]$ is a distribution function, $T^-:[0,1]\to \bar {\mathbb R}$ is also called a **quantile function** of $T.$

In this context $T$ is the cdf $F.$ We have that $F(F^-(y))=y.$ And $X \sim F^-(u).$


<img src="https://user-images.githubusercontent.com/9312897/32376482-4523cf7a-c07b-11e7-912f-6cb97bb4ac11.png" width=500>

[From here on, instead of $y$ we will use $u$] 

So if we have a vector of random variables, $\{X_1, \dots,X_n\}$ and we think of them as the variable $u,$ we have that $X_i = F^-_i(u)$ and $F(X_i) = U(0,1).$ So each variable will be transformed as $\{F_1,\dots,F_n\}.$


<img src="https://user-images.githubusercontent.com/9312897/32380482-8e5057d0-c086-11e7-90e9-1aa30a78260f.png" width=500>

We can say that $F_i(X_i) \sim U_i$ and $F_i^- (U_i) \sim X_i.$


We can look at the vector $\vec u =(F_1(X_1), \dots, F_n(X_n)),$ and call the joint distribution of vector \vec u$ the **copula**: 

$$\Pr(U_1 \leq u_1, \dots, U_n \leq u_n).$$

So the copula is a joint distribution function on $[0,1]^d$ with marginal distributions standard uniform.

For example, *if we assume independence* between $X_i$ variables,

$$C(\vec u) = \Pr(U_1 \leq u_1, \dots, U_d \leq u_d) = \Pr(F_1(X_1)\leq u_1, \dots, F_d(X_d) \leq u_d)=\Pi_{i=1}^d (F_i(X_i)\leq u_i)= \Pi_{i=1}^d u_i.$$

This is the independence copula.


---

Objectives:

Sklar's theorem:

Given $F, F_1, \dots, F_d,$ what is $C?$

Given $C, F_1,\dots, F_d,$ what is $F?$

---

Given a joint distribution $F$ with marginal distributions $F_1, F_2, \dots, F_d,$ there exists a copula, $C,$ such that $C(F_1(x_1), \dots, F_d(x_d))= F(x_1, \dots, x_d).$

If $F$ is continuous, $C$ is unique and given by 

$$C(u_1,\dots,u_d) = F(F^-_1(u_1),\dots,F^-_d(u_d))$$

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
