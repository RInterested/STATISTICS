<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###APPLICATION OF THE CHI SQUARE TEST:


<br><br>

####GENERAL TEST STATISTIC (regardless of specific application):

The chi square statistic is denoted as $\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the observed number of cases in category $i$, and $E_i$ is the expected number of cases in category $i$. 


The null and alternative hypotheses for each chi square test are:

$H_o: O_i=E_i$ and $H_1:O_i\neq E_i$.

The theoretical distribution is a continuous distribution, but the $\chi^2$ statistic is obtained in a discrete manner, on the basis of discrete differences betweenthe observed and expected values. Hence, there is a need for asymptotics: The condition in which the chi square statistic is approximated by the theoretical chi square distribution, is that the sample size is reasonably large: there should be at least $5 \,expected \,cases$ per category. There may be no observed values in one or more categories, and this causes no difficulty with the test. But there must be around 5 or more expected cases per category. If there are considerably less than this, adjacent categories may be grouped together to boost the number of expected cases.

The test statistic follows a multinomial distribution.

<br>



####TEST of INDEPENDENCE of TWO VARIABLES:

<br>

It is based on the analysis of a cross classification on a contingency table to test the possible dependency or relationship between variables.

There is a conceptual distinction between the test of independence and the [chi-square test of homogeneity][3], although there are no practical mathematical consequences. A test of independence would condition on both margins, whereas in a test of homogeneity the marginals of one of the categories are assumed not be random variables, but rather fixed by design. These theoretical distinctions aside, both can be lumped under the terms *chi-square test of association* (for two-way contingency tables) or *chi-square k-independent samples comparison test* (for more two categories). This is in contradistinction to the **one-sample chi-square test of agreement**, essentially a GOF test.


For larger samples (> 5 expected frequency count in each cell) the $\chi^2$ provides an approximation of the [significance value][1]. The test is based on calculating the *expected frequency counts* obtained by cross-multiplying the marginals (assuming normal distribution of the marginals, it makes sense that we end up with a $\chi^2$ distributed test statistic, since if $X\sim N(\mu,\sigma^2)$, then $X^2\sim \chi^2(1))$:

The goodness of fit test examines only one variable, while the test of independence is concerned with the relationship betweentwo variables. Like the goodness of fit test, the chi square test of independence is verygeneral, and can be used with variables measured on any type of scale, nominal, ordinal, interval or ratio. As with the GOF test the expected number of cases in each cell should be more than $5$.

The chi square test for independence is conducted by assuming that there is no relationship between the two variables being examined. The alternative hypothesis is that there is some relationship between the variables.

The chi square statistic used to conduct this test is the same as in the goodness of fit test:

$\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the  numbers of cases in each cell ofthe cross classi√òcation table, and $E_i$ is the expected number of cases.

The chi square statistic computed from the observed and expected values is calculated, and if this statistic is inthe region of rejection of the null hypothesis, then the assumption of no relationship between $X$ and $Y$ is rejected.

<br><br>

####Example 1:

The example given in the [test of proportions entry](http://rinterested.github.io/statistics/tests_of_proportions.html) is a valid example.

<br>

####Example 2:

Is there a relationship between sex and class?

```{r}
tab <- matrix(c(33,153,103,16,29,181,81,14), nrow =4)
dimnames(tab)<-list(Social.Class = c("Upper","Middle","Working","Lower"), Sex = c("Male", "Female"))
addmargins(tab)

```

Under the NULL hypothesis the two variables are unrelated and therefore:

$\small P(\text{Upper} \,\text{and}\, \text{Male}) = P(\text{Upper})P(\text{Male})$

$\small P(\text{Upper}\,\text{and}\,\text{Male}) =\frac{62}{610}\frac{305}{610}$

and the number of expected cases for the cell:

$610 \, \frac{62}{610}\frac{305}{610}$

The general formula for the degrees of freedom is the number of rows minus 1, times the number of columns minus 1.

```{r}
chisq.test(tab)
```

The independence of sex and class cannot be rejected.

<br>

####Example 3:

The best illustration is Agresti's example with husband and wife marital satisfaction:

```{r}
sex_satis <- matrix(c(7,2,1,2,7,8,5,8,2,3,4,9,3,7,9,14), nrow = 4)
dimnames(sex_satis) = list(wife = c("Never","Sometimes","Often","Always"), husband = c("Never","Sometimes","Often","Always"))
sex_satis
```

It is to be excpected that, in general, the satisfaction of a spouse moves together with the other. 

If we run a $\chi^2$ test we get,

```{r, warning=FALSE}
chisq.test(sex_satis, correct = T)
```

that with a probability of $<\,5\%$ of making a type I error, the counts in the table cannot be considered independent - we reject the $H_o$ hypothesis of independence, and assume that there is an association between the satisfaction of the husband and the wife.

In this test, the margins are considered fixed, and the expected counts calculated as follows:

<img HEIGHT="800" WIDTH="500" src="https://cloud.githubusercontent.com/assets/9312897/10900528/0f965f56-81af-11e5-9e52-7f15234f9917.png">

$n_{i+}*n_{+j}/n$.

The test statistic will be:

$\sum \frac{(O-E)^2}{E}$ with $\small df = (rows-1)*(cols - 1)$

The exact p-value can be obtained with a Monte Carlo simulation:

```{r,echo=F, warning=F}
library(gsheet)
data <- read.csv(text =  gsheet2text('https://docs.google.com/spreadsheets/d/1w575Rfj2QY30wWc6piunwcyDOfYQfL-5KSulJIQ3ego/edit?usp=sharing',format ='csv'))
data <- data[,-1]
```

We need first the dataset in long format:

```{r}
head(data)
```

If we perform first the chi square test on the dataset:

```{r, warning = F}
chisq.test(table(data),correct = F)$statistic
```

... and then we permute either the husband or the wife (swingers?), calculating the chi square value every time, and record the percentage of time that it will be greater than the original chi statistic:

```{r, warning = F}
baseline <- chisq.test(table(data),correct = F)$statistic
dat <- data
statistic <- 0
for(i in 1: 1000){dat[,2] <- sample(dat[,2])
statistic[i] <- chisq.test(table(dat), correct = F)$statistic}
(p_value <- length(statistic[statistic>=baseline])/length(statistic))
```


So, under the assumption of independence (swingers) only in this small fraction of case would we obtain such a high chi square value to begin with. Hence, we can reject the hypothesis of independence, and assume there is dependence between the husbands' and wives' satisfaction.

We can calculate this "exact" test (really a generalization of Fisher's exact test) with the command:

```{r}
chisq.test(table(data), simulate.p.value = T)
```


[1]: https://en.wikipedia.org/wiki/Fisher%27s_exact_test
[3]: http://www.r-bloggers.com/comparison-of-two-proportions-parametric-z-test-and-non-parametric-chi-squared-methods/

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
