<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>


###TYPES OF TIME SERIES:
<br>
    
#####**WHITE NOISE:**

$\large y_t = \epsilon_t$

Sequence of identically distributed variables $\epsilon_i$ with $\epsilon_i \sim N(0,\sigma^2)$

Here is an example with plots of the autocorrelation function  `acf` telling us how the last value is correlated to the immediate prior values, as well as the partial autocorrelation function `pacf`, showing whether there is any residual correlation of $y_t$ to the prior values of $y$ after eliminating $y_{t-1}$.

```{R,fig.width = 7, fig.height = 3}
set.seed(0)
a = rnorm(100)
plot.ts(a, col="tan4", ylab="white noise")
abline(h=0, col="red3", lty=2)
acf(a, main="", xlab="")
pacf(a, main="", xlab="")
```

#####**RANDOM WALK:**

$\large y_t = y_{t-1}+\epsilon_t=\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_1$

```{R,fig.width = 7, fig.height = 3}
b = cumsum(rnorm(100))
plot.ts(b, col="tan2", ylab="random walk")
abline(h=0, col="red3", lty=2)
acf(b, main="", xlab="")
pacf(b, main="", xlab="")
```

#####**AUTOREGRESSIVE 1 or AR(1):**

$\large y_t = \color{red}{\phi}\, y_{t-1}+c+\epsilon_t$

When $\phi < 1$ the series is "covariance stationary". Notice how the influence of the immediate prior position will be, to a certain extent, countered by the effect of $\phi$: the larger $\phi$ is the closer `AR` will be to a random walk; and the smaller it is, the more similar to white noise.

Leaving the constant $c=1$, and set $\phi=0.6$, here is an example:

```{R,fig.width = 7, fig.height = 3}
c = arima.sim(n=100,list(ar=0.6),innov=rnorm(100))
plot.ts(c, col="chocolate", ylab="AR(1)")
abline(h=0, col="red3", lty=2)
acf(c, main="", xlab="")
pacf(c, main="", xlab="")
```

Notice how the lag 1 ($y_{t-1}$) value is significant. But if we suppress this dependency, there is no further correlation upstream (except for some statistical random outliers). Also, the `acf` starts off comparing $t$ to $t$, whereas the first line on the `pacf` plot already compares $t$ to $t-1$.

#####**AUTOREGRESSIVE 2 or AR(2):**

For any $\text{AR(n)}$:

$\large y_t = \color{red}{\phi_1}\, y_{t-1}+
\color{blue}{\phi_2}\, y_{t-2} +\cdots+
\color{gray}{\phi_n}\, y_{t-n}+
c+\epsilon_t$

In `AR(2)` we have two lags:

```{R,fig.width = 7, fig.height = 3}
d = arima.sim(n=100,list(ar=c(0.6,-.5)),innov=rnorm(100))
plot.ts(d, col="chocolate", ylab="AR(2)")
abline(h=0, col="red3", lty=2)
acf(d, main="", xlab="")
pacf(d, main="", xlab="")
```

Notice the negative second lag related to the $-0.5$ AR value. Notice how the acf just gives us a significant correlation without the sign.

#####**MOVING AVERAGE or MA:**

$\large y_t = k+\epsilon_t+\color{red}{\theta}\,\epsilon_{t-1}$

which can clearly be extended to $MA(n)$.

Here is an `MA(2)` process:

```{R,fig.width = 7, fig.height = 3}
e = arima.sim(n=100,list(ma=c(.88,-.48)),innov=rnorm(100))
plot.ts(e, col="chocolate", ylab="MA(2)")
abline(h=0, col="red3", lty=2)
acf(e, main="", xlab="")
pacf(e, main="", xlab="")
```

There are a number of significant partial autocorrelation values. The `acf` gives us the expected $1$ for the first value (comparint $t$ to $t$), but the following two values should also be significant.

#####**MIXED MODELS or ARMA Models:**

$\large y_t = C + \color{red}{\phi_1}\, y_{t-1}+
\color{blue}{\phi_2}\, y_{t-2} +\cdots+
\color{gray}{\phi_n}\, y_{t-n}+ \epsilon_t +
\color{red}{\theta_1}\, \epsilon_{t-1}+
\color{blue}{\theta_2}\, \epsilon_{t-2} +\cdots+
\color{gray}{\theta_m}\, y_{t-m}$

```{R,fig.width = 7, fig.height = 3}
f = arima.sim(n=100,list(ar=c(0.6,-.5), ma=c(.88,-.48)),innov=rnorm(100))
plot.ts(f, col="chocolate", ylab="MA(2)")
abline(h=0, col="red3", lty=2)
acf(f, main="", xlab="")
pacf(f, main="", xlab="")
```

---

In one single shot:

```{R, fig.width = 8, fig.height = 8}
par(mfrow=c(3,3))
plot.ts(a, col="tan4", ylab="white noise")
abline(h=0, col="red3", lty=2)
plot.ts(b, col="tan2", ylab="random walk")
abline(h=0, col="red3", lty=2)
plot.ts(c, col="chocolate", ylab="AR(1)")
abline(h=0, col="red3", lty=2)
plot.ts(d, col="chocolate", ylab="AR(2)")
abline(h=0, col="red3", lty=2)
plot.ts(e, col="chocolate", ylab="MA(2)")
abline(h=0, col="red3", lty=2)
plot.ts(f, col="chocolate", ylab="ARIMA(2)")
abline(h=0, col="red3", lty=2)
```

<br>


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
