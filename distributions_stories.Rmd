---
title: 'Stories of Distributions'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---


### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Distribution Stories:

---

#### Geometric:

The geometric distribution represents the probability of having $x$ Bernoulli($p$) failures until first success? “How many trials are needed until your first success?”

---

#### Negative binomial:

Models the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures $r$ occurs.

---

##### Hypergeometric distribution:

n probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of $k$ successes (random draws for which the object drawn has a specified feature) in $n$ draws, without replacement.


---

#### Poisson:

---

In probability theory, a Poisson process is a stochastic process that counts the number of events and the time points at which these events occur in a given time interval. The time between each pair of consecutive events has an **exponential distribution** with parameter $\lambda$ and each of these inter-arrival times is assumed to be independent of other inter-arrival times. The process is named after the French mathematician Siméon Denis Poisson and is a good model of radioactive decay, telephone calls and requests for a particular document on a web server among many other phenomena.

The Poisson process is a continuous-time process; the sum of a Bernoulli process can be thought of as its discrete-time counterpart. A Poisson process is a pure-birth process, the simplest example of a birth-death process. It is also a point process on the real half-line.

Probability of getting $5$ e-mails (n Poisson events) in $1$ day, given an average rate. Mnemonic: Probability of getting $n$ fishes - “poissons” in French.

---

#### Exponential:

---

Probability of having to wait $1$ hour before the next e-mail arrives (Poisson event). Parameter: 

Describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. It is the continuous analogue of the **geometric distribution** (number of Bernoulli trials before getting the first success, e.g. Heads), and it has the key property of being memoryless.

---

#### Erlang:

---

The Erlang distribution is the distribution of a sum of $k$ independent exponential variables with mean $1/\lambda$  each. Equivalently, it is the distribution of the time until the $k$-th event of a Poisson process with a rate of $\lambda.$ 

---

#### Gamma:

---

The gamma distribution is frequently used to model waiting times. For instance, in life testing, the waiting time until death is a random variable that is frequently modeled with a gamma distribution.

Let $X_{1},X_{2},\dots ,X_{n}$ be $n$ independent and identically distributed random variables following an exponential distribution with rate parameter $λ,$ then $\sum_{i}X_{i} ~ \operatorname{Gamma}(n, λ)$ where $n$ is the shape parameter and $λ$ is the rate, and $\bar {X}={\frac {1}{n}}\sum_{i}X_{i}\sim \operatorname{Gamma} (n,n\lambda )$ where the rate changes $nλ.$

Probability of having to wait $1$ day to get $5$ e-mails (time to $n$-th Poisson event). We add inter-arrival times (exponential lambda). This fact: that the gamma distribution represents the sum of exponential distributions (convolution) is proved by deriving the mgf of the gamma. 

Parameters: $n$ ($a$ or $K$ or $\alpha$) (shape) and $\lambda$ (or $\beta$ or $1/\theta$) (scale). It’s the continuous analogue of the **negative binomial** (i.e. sum of geometric) = number of Bernoulli trials before reaching n successes (eg. $4$ heads).

---

#### Chi square:

---

The chi square distribution is a special case of the gamma distribution.

A squared (transformed) standard normal distribution will be distributed as a chi square of $1$ degree of freedom. If $Z_1, Z_2, \dots, Z_k$ are standard normal variables, $Z_1^2 + Z_2^2 + \cdots + Z_n^2 \sim \chi^2_k.$

The chi square cames into place in the distribution of squared differences between observed and estimated values. It will only play a role in multiple inferential tests, including testing variance.

---

#### Beta:

---

In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. The beta distribution is a suitable model for the random behavior of percentages and proportions.

---

#### Multinomial & Categorical:

---

The [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution#Example) is a generalization of the binomial. In the binomial, there are $n$ independent trials or experiments, and we add the number of successes. In each trial there are only two possibilities: success or failure - each trial or experiment is a Bernoulli trial.

In a multinomial distribution there are also $n$ experiments, but the outcome of each experiment is not S or F, but rather $K$ possible **categories**. For instance, we survey $n=15$ people, asking them whether they intend to vote Democrat, Republican or Independent, i.e. $K = 3.$ Knowing the percentage of supporters for each option in the general popoulation we can calculate the probability of the event $(D=7, R=5, I=3).$

If $K=2$ we are back to $(0,1)$ binary outcomes with $n$ experiments, which is the definition of the binomial distribution.

If $n=1$ and $K = 2,$ we have a Bernouilli experiment.

If $n=1$ but we have more than $2$ categories, we are dealing with a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution). The categorical distribution is the generalization of the Bernoulli distribution for a categorical random variable, i.e. for a discrete variable with more than two possible outcomes, such as the roll of a die, $K=6$. On the other hand, the categorical distribution is a special case of the multinomial distribution, in that it gives the probabilities of potential outcomes of a single drawing, $(n=1),$ rather than multiple drawings.

The parameters specifying the probabilities of each possible outcome are constrained only by the fact that each must be in the range 0 to 1, and all must sum to 1.


---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
