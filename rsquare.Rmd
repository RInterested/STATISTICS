---
title: 'R-Squared in OLS'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---


### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### R-Squared: SST, SSE & SSR:

---

See also [this post](https://rinterested.github.io/statistics/df_betas.html).

From these Wikipedia [definitions](https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions):

\begin{align}
\text{SST}_{\text{otal}} &= \color{red}{\text{SSE}_{\text{rrors}}}+\color{blue}{\text{SSR}_{\text{egression}}}\\
\end{align}

or, equivalently,

\begin{align}
\sum(y_i-\bar y)^2 &=\color{red}{\sum(y_i-\hat y)^2}+\color{blue}{\sum(\hat y_i -\bar y)^2}
\end{align}

and

$\large \text{R}^2 = 1 - \frac{\text{SSE}_{\text{rrors}}}{\text{SST}_{\text{otal}}}$

So if the model explained all the variation, $\text{SSE}_{\text{rrors}}=\sum(y_i-\hat y_i)^2=0$, and $\bf R^2=1.$

From Wikipedia:

> Suppose $r = 0.7$ then $R^2 = 0.49$ and it implies that $49\%$ of the
> variability between the two variables have been accounted for and the
> remaining $51\%$ of the variability is still unaccounted for.

---

Proof by example:

    fit = lm(mpg ~ wt, mtcars)
    summary(fit)$r.square
    [1] 0.7528328
    > sse = sum((fitted(fit) - mean(mtcars$mpg))^2)
    > ssr = sum((fitted(fit) - mtcars$mpg)^2)
    > 1 - (ssr/(sse + ssr))
    [1] 0.7528328

---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**

