<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###APPLICATION OF THE CHI SQUARE TEST:


<br><br>

####GENERAL TEST STATISTIC (regardless of specific application):

The chi square statistic is denoted as $\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the observed number of cases in category $i$, and $E_i$ is the expected number of cases in category $i$. 


The null and alternative hypotheses for each chi square test are:

$H_o: O_i=E_i$ and $H_1:O_i\neq E_i$.

The theoretical distribution is a continuous distribution, but the $\chi^2$ statisticis obtained in a discrete manner, on the basis of discrete differences betweenthe observed and expected values. Hence, there is a need for asymptotics: The condition in which the chi square statistic is approximated by the theoretical chi square distribution, is that the sample size is reasonably large: there should be at least $5 \,expected \,cases$ per category. There may be no observed values in one or more categories, and this causes no difficulty with the test. But there must be around 5 or more expected cases per category. If there are considerably less than this, adjacent categories may be grouped together to boost the number of expected cases.

The test statistic follows a multinomial distribution.

<br><br>

####1. GOODNESS-OF-FIT TEST:

<br>

This test concerns itself with checking whether the sample is consistent with a theorized population distribution. Examples:

1. Is there the same number of customers each day visiting a store (the answer will determine staffing needs)? In other words, is the distribution uniform?

2. Have the grades of the students been allocated using a bell distribution?

3. Does a sample follow the distribution of the population in a science project?

When dealing with continuous variable there will be a need to discretize by grouping the observations into $K$ bins or categories through, for instance, quantiles. The total of the number of cases has to be made equal to the total number of expected values. Importantly, the degrees of freedom are $k-1$. Large values of the chi square statistic will lead to rejecting the null hypothesis.

Example 1:

Does the distribution of ages in a sample matches that expected in the population:

```{r}
age <- c("20-24", "25-34", "35-44")
Observed <- c(103, 216, 171)
Expec_Perc. <- c(18, 50, 32)
(dat <- data.frame(age, Observed, Expec_Perc.))
```

In other words, $H_o$: The proportion of cases in category $i$ is $p_i$; $i= 1, 2, 3, \dots k$. The alternative hypothesis is that at least one of the proportions is not as hypothesized.

We can calculate the expected counts manually:

$\small E_1= 490*\frac{18}{100}= 490*0.18 = 88.2$

$\small E_2= 490*\frac{50}{100}= 490*0.50 = 245.0$

$\small E_3= 490*\frac{32}{100}= 490*0.32 = 156.8$

with 88.2, 245.0 and 156.8 made to add to 490: 88 + 245 + 157 = 490.

The TS (test statistic) is:

$\small TS = (103 - 88)^2 / 88 + (216 - 245)^2 / 245 + (171 - 157)^2 / 157 = 7.2378$.

The degrees of freedom are $k - 1 = 2$.

However, this rounding doesn't take place when the values are calculated in [R]:

$\small TS =(103 - 88.2)^2 / 88.2 + (216 - 245)^2 / 245 + (171 - 156.8)^2 / 156.8 = 7.202069$

Check it out:

```{r}
chisq.test(dat[,2], p = dat[,3]/100, correct = F)
```

Clearly the value is too asymptotic ($\small p<0.05$) to not reject the equality of distributions, ie. No the sample is not distributed as in the theoretical population.

Example 2:

Has the number of suicides in a region stayed the same over time?

This is the data:

```{r}
years <- c(1978:1989)
suicides <- c(164, 142, 153, 171, 171, 148, 136, 133, 138, 132, 145, 124)
dat <- data.frame(years, suicides); as.data.frame(t(dat))
```

The assumption is that the frequency is the same across years, and the test is:

```{r}
chisq.test(dat$suicides, p = replicate(length(dat$suicides), 
                              1/length(dat$suicides)))
```

Therefore, we don't have enough evidence to reject the idea that there may not be too much difference between years.

Example 3:

There is a self-perpetuating *calendar effect* among youth soccer players whereby little players trying out for a team have more of a chance to prove themselves and make the team if they are born in January than in November or December, a difference that is significant at an early age.

The birth months of a league of professional players can be analysed with a GOF test:

```{r}
month_bins <- c("Jan to Mar", "Apr to Jun", "Jul to Sep", "Oct to Dec")
players   <- c(84,77,35,34)
dat <- data.frame(month_bins, players)
dat
chisq.test(dat$players, p = replicate(length(dat$players),
                                      1/length(dat$players)))
```

... a powerful argument in favor of the calendar effect.

However, this effect may be due to seasonal differences in the number of births. Hence, if the proportion of births in different months is:

```{r}
births <- c(0.242,0.260,0.258,0.240)
(dat <- cbind(dat,births))
chisq.test(dat$players, p = births)
```

... a different distribution than that predicted, further supporting the calendar effect.
<br>
Example 4: COMPARING DISTRIBUTIONS:

We can use the GOF test to check whether the random generation of numbers by a software program is as promised, or to see if the results of a test have been graded on a bell curve.

The degrees of freedom in this case are calculated with the formula: $\small df \,=\, bins\,(or\,categories) - 1 - number\,of\,parameters\,of\,the\,distribution$. For example, two parameters define the normal distribution, whereas only one defines the binomial.

As an example of how the chi-square goodness-of-fit test for a normal distribution can be used, the 5-year annualized return rates achieved by 259 funds can be assessed for normality:

```{r, warning=F}
library(gsheet)
data <- read.csv(text = gsheet2text('https://docs.google.com/spreadsheets/d/1VQP4jwknEQPnbRFOCeFoNdvieHg9BSVrPzMDwXJbsL4/edit?usp=sharing', format ='csv'))
funds <- data[data$Objective=="Growth",c("Fund","FiveYrRtn")]
head(funds)
mean(funds$FiveYrRtn);sd(funds$FiveYrRtn)
```



<br>

####2. TEST of INDEPENDENCE of TWO VARIABLES:

<br>

It is based on the analysis of a cross classification on a contingency table to test the possible dependency or relationship between variables.

The assumptions on which these tests are based are minimal, although a certain minimum sample size is usually re-quired. The variables which are being examined can be measured at anylevel, nominal, ordinal, interval, or ratio. The test can therefore be used in most circumstances.


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
