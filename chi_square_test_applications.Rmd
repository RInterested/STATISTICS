<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###APPLICATION OF THE CHI SQUARE TEST:


<br><br>

####GENERAL TEST STATISTIC (regardless of specific application):

The chi square statistic is denoted as $\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the observed number of cases in category $i$, and $E_i$ is the expected number of cases in category $i$. 


The null and alternative hypotheses for each chi square test are:

$H_o: O_i=E_i$ and $H_1:O_i\neq E_i$.

The theoretical distribution is a continuous distribution, but the $\chi^2$ statisticis obtained in a discrete manner, on the basis of discrete differences betweenthe observed and expected values. Hence, there is a need for asymptotics: The condition in which the chi square statistic is approximated by the theoretical chi square distribution, is that the sample size is reasonably large: there should be at least $5 \,expected \,cases$ per category. There may be no observed values in one or more categories, and this causes no difficulty with the test. But there must be around 5 or more expected cases per category. If there are considerably less than this, adjacent categories may be grouped together to boost the number of expected cases.

The test statistic follows a multinomial distribution.

<br><br>

####1. GOODNESS-OF-FIT TEST:

<br>

This test concerns itself with checking whether the sample is consistent with a theorized population distribution. Examples:

1. Is there the same number of customers each day visiting a store (the answer will determine staffing needs)? In other words, is the distribution uniform?

2. Have the grades of the students been allocated using a bell distribution?

3. Does a sample follow the distribution of the population in a science project?

When dealing with continuous variable there will be a need to discretize by grouping the observations into $K$ bins or categories through, for instance, quantiles. Importantly, the degrees of freedom are $k-1$. Large values of the chi square statistic will lead to rejecting the null hypothesis.
<br>

####Example 1:

Does the distribution of ages in a sample matches that expected in the population:

```{r}
age <- c("20-24", "25-34", "35-44")
Observed <- c(103, 216, 171)
Expec_Perc. <- c(18, 50, 32)
(dat <- data.frame(age, Observed, Expec_Perc.))
```

In other words, $H_o$: The proportion of cases in category $i$ is $p_i$; $i= 1, 2, 3, \dots k$. The alternative hypothesis is that at least one of the proportions is not as hypothesized.

We can calculate the expected counts manually:

$\small E_1= 490*\frac{18}{100}= 490*0.18 = 88.2$

$\small E_2= 490*\frac{50}{100}= 490*0.50 = 245.0$

$\small E_3= 490*\frac{32}{100}= 490*0.32 = 156.8$

The TS (test statistic) is:

$\small TS = (103 - 88)^2 / 88 + (216 - 245)^2 / 245 + (171 - 157)^2 / 157 = 7.2378$.

The degrees of freedom are $k - 1 = 2$.

This rounding doesn't take place when the values are calculated in [R]:

$\small TS =(103 - 88.2)^2 / 88.2 + (216 - 245)^2 / 245 + (171 - 156.8)^2 / 156.8 = 7.202069$

Check it out:

```{r}
chisq.test(dat[,2], p = dat[,3]/100, correct = F)
```

Clearly the value is too asymptotic ($\small p<0.05$) to not reject the equality of distributions, ie. No the sample is not distributed as in the theoretical population.
<br>

####Example 2:

Has the number of suicides in a region stayed the same over time?

This is the data:

```{r}
years <- c(1978:1989)
suicides <- c(164, 142, 153, 171, 171, 148, 136, 133, 138, 132, 145, 124)
dat <- data.frame(years, suicides); as.data.frame(t(dat))
```

The assumption is that the frequency is the same across years, and the test is:

```{r}
chisq.test(dat$suicides, p = replicate(length(dat$suicides), 
                              1/length(dat$suicides)))
```

Therefore, we don't have enough evidence to reject the idea that there may not be too much difference between years.
<br>

####Example 3:

There is a self-perpetuating *calendar effect* among youth soccer players whereby little players trying out for a team have more of a chance to prove themselves and make the team if they are born in January than in November or December, a difference that is significant at an early age.

The birth months of a league of professional players can be analysed with a GOF test:

```{r}
month_bins <- c("Jan to Mar", "Apr to Jun", "Jul to Sep", "Oct to Dec")
players   <- c(84,77,35,34)
dat <- data.frame(month_bins, players)
dat
chisq.test(dat$players, p = replicate(length(dat$players),
                                      1/length(dat$players)))
```

... a powerful argument in favor of the calendar effect.

However, this effect may be due to seasonal differences in the number of births. Hence, if the proportion of births in different months is:

```{r}
births <- c(0.242,0.260,0.258,0.240)
(dat <- cbind(dat,births))
chisq.test(dat$players, p = births)
```

... a different distribution than that predicted, further supporting the calendar effect.

<br>

####Example 4: COMPARING DISTRIBUTIONS:

We can use the GOF test to check whether the random generation of numbers by a software program is as promised, or to see if the results of a test have been graded on a bell curve.

The degrees of freedom in this case are calculated with the formula: $\small df \,=\, bins\,(or\,categories) - 1 - number\,of\,parameters\,of\,the\,distribution$. For example, two parameters define the normal distribution, whereas only one defines the binomial.

As an example of how the chi-square goodness-of-fit test for a normal distribution can be used, the 5-year annualized return rates achieved by 158 growth funds can be assessed for normality:

```{r, warning=F}
library(gsheet)
data <- read.csv(text = gsheet2text('https://docs.google.com/spreadsheets/d/1VQP4jwknEQPnbRFOCeFoNdvieHg9BSVrPzMDwXJbsL4/edit?usp=sharing', format ='csv'))
funds <- data[data$Objective=="Growth",c("Fund","FiveYrRtn")]
head(funds)

# Since we are going to test for normality the two parameters to estimate are:

mean(funds$FiveYrRtn); sd(funds$FiveYrRtn)
```

The null and alternative hypotheses are:

$H_o$: The 5-year annualized return rates follow a normal distribution.

$H_1$: The 5-year annualized return rates do not follow a normal distribution.

Segmenting in intervals the five-year returns:

```{r, warning = F}
library(ggplot2)
intervals <- cut_interval(funds$FiveYrRtn, length = 5)
tab <- as.matrix(table(intervals))
colnames(tab) <- c("OBS.FR.")
Upp.Bound <- c(-5,0,5,10,15,20,25,30)
Z.vals <- round((Upp.Bound - replicate(8, mean(funds$FiveYrRtn)))/replicate(8, sd(funds$FiveYrRtn)),4)
area.below <- round(pnorm(Z.vals,lower.tail=T),5)
area.bin <- round(c(area.below[1],diff(area.below)),5)
EXP.FR. <- round(area.bin * length(funds$FiveYrRtn))
DELTAsq <- (as.data.frame(tab)[,1] - EXP.FR.)^2/EXP.FR.
(dat<-data.frame(tab, Upp.Bound, Z.vals, area.below, area.bin, EXP.FR., DELTAsq))

# The TEST STATISTIC IS:

sum(DELTAsq)
```

The TEST STATISTIC is infinite because we have expected frequencies below 1. So we have to combine bins:

```{r}
OBS.FR. <- c(4,15,58,61,16,4)
Upp.Bound <-c(0,5,10,15,20,25)
area.bin <- round(c(pnorm(Z.vals[2]),diff(area.below[2:7])),5)
EXP.FR. <- round(area.bin * length(funds$FiveYrRtn),3)
dat <- dat[2:7,]
dat$OBS.FR. <- OBS.FR.
dat$Upp.Bound <- Upp.Bound
dat$area.bin <- area.bin
dat$EXP.FR. <- EXP.FR.
dat$DELTAsq <- (OBS.FR. - EXP.FR.)^2/EXP.FR.
row.names(dat) <-c("(-10,0]","(0,5]","(5,10]","(10,15]",
"(15,20]","(20,30)")
dat

# The TEST STATISTIC:

sum(dat$DELTAsq)
```

Now we know that we have finally $k=6$ bins, and the normal distribution has two parameters, we end up with $df=6-1-2=3$, and...

```{r}
qchisq(0.95, df = 3)
```

we can't reject the hypothesis that the returns follow a normal distribution.

This test is sensitive to the choice of bins. There is no optimal choice for the bin width (since the optimal bin width depends on the distribution). Most reasonable choices should produce similar, but not identical, results. For the chi-square approximation to be valid, the expected frequency should be at least 5. This test is not valid for small samples, and if some of the counts are less than five, you may need to combine some bins in the tails.

The **Shapiro-Wilk test**, proposed in 1965, calculates a $W$ statistic that tests whether a random sample, $x_1,x_2,\dots,x_n$ comes from (specifically) a normal distribution . Small values of $W$ are evidence of departure from normality and percentage points for the $W$ statistic, obtained via Monte Carlo simulations, were reproduced by Pearson and Hartley (1972). This test has done very well in comparison studies with other goodness of fit tests.

The chi-square test is an alternative to the Anderson-Darling and **Kolmogorov-Smirnov** goodness-of-fit tests. The chi-square goodness-of-fit test can be applied to discrete distributions such as the binomial and the Poisson. The Kolmogorov-Smirnov and Anderson-Darling tests are restricted to continuous distributions.

The distribution of the K-S test statistic itself does not depend on the underlying cumulative distribution function being tested. Another advantage is that it is an exact test (the chi-square goodness-of-fit test depends on an adequate sample size for the approximations to be valid). Despite these advantages, the K-S test has several important limitations:

1. It only applies to continuous distributions.
2. It tends to be more sensitive near the center of the distribution than at the tails.
3. Perhaps the most serious limitation is that the distribution must be fully specified. That is, if location, scale, and shape parameters are estimated from the data, the critical region of the K-S test is no longer valid. It typically must be determined by simulation.

<br>

####2. TEST of INDEPENDENCE of TWO VARIABLES:

<br>

It is based on the analysis of a cross classification on a contingency table to test the possible dependency or relationship between variables.

The goodness of fit test examines only one variable, while the test of independence is concerned with the relationship betweentwo variables. Like the goodness of fit test, the chi square test of independence is verygeneral, and can be used with variables measured on any type of scale, nominal, ordinal, interval or ratio. As with the GOF test the expected number of cases in each cell should be more than $5$.

The chi square test for independence is conducted by assuming that thereis no relationship between the two variables being examined. The alternative hypothesis is that there is some relationship between the variables.

The chi square statistic used to conduct this test is the same as in the goodness of fit test:

$\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the  numbers of cases in each cell ofthe cross classiØcation table, and $E_i$ is the expected number of cases.

The chi square statistic computed fromthe observed and expected values is calculated, and if this statistic is inthe region of rejection of the null hypothesis, then the assumption of norelationship between X and Y is rejected.

####Example 1:

Is there a relationship between sex and class?

```{r}
tab <- matrix(c(33,153,103,16,29,181,81,14), nrow =4)
dimnames(tab)<-list(Social.Class = c("Upper","Middle","Working","Lower"), Sex = c("Male", "Female"))
addmargins(tab)

```

Under the NULL hypothesis the two variables are unrelated and therefore:

$P(Upper \,and\, Male) = P(Upper)P(Male)$

$P(Upper\,and\,Male) =\frac{62}{610}\frac{305}{610}$

and the number of expected cases for the cell:

$610 \, \frac{62}{610}\frac{305}{610}$

The general formula for the degrees of freedom is the number of rows minus 1, times the number of columns minus 1.

```{r}
chisq.test(tab)
```

The independence of sex and class cannot be rejected.

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
