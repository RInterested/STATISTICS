---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###REGRESSION OPTIMIZATION:
<br>

The cost function is not necessary in OLS, but it comes into play when using regularization.

The cost function would be generally expressed as:

$J(\hat \beta)= (y - {\bf X}\hat \beta)^\top(y- {\bf{X} \hat \beta})= \displaystyle \sum_{i=1}^n (y_i - x_i^\top\hat \beta)^2= \sum_{i=1}^n(y_i - \hat y_i)^2$

Expanding the quadratic in matrix notation:

$$J(\hat \beta)= (y - {\bf X}\hat \beta)^T(y- {{\bf X} \hat \beta})= y^Ty + \color{red}{\hat \beta^T\,X^TX\,\hat \beta} - 2y^TX\hat \beta$$

The term in red is a positive semidefinite matrix. A positive definite matrix fulfills the requirement, $x^TAx>0$. The other two terms are scalars.

To differentiate the cost function to obtain a minimum we need two pieces of information:

$\frac{\partial {\bf A}\hat \beta}{\partial \hat \beta}={\bf A}^T$ (the derivative of a matrix with respect to a vector); and $\frac{\partial \hat \beta^T{\bf A}\hat \beta}{\partial \hat \beta}= 2{\bf A}^T\hat \beta$ (derivative of a quadratic form with respect to a vector).

$$\frac{\partial J(\hat \beta)}{\partial \hat \beta}=\frac{\partial}{\partial\hat \beta}\left[y^Ty + \color{red}{\hat \beta^T\,X^TX\,\hat \beta} - 2y^TX\hat \beta \right]=0 +2 \color{red}{X^TX\,\hat \beta}-2X^Ty$$

which gives:

$$2X^TX\hat \beta = 2X^Ty$$

$$\hat \beta = (X^TX)^{-1}X^Ty$$
---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
