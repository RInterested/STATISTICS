---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###STATISTICAL LEARNING or NON-LINEAR DIMENSION-REDUCTION ALGORITHM:
<br>

From this [MMDS lecture](https://youtu.be/ddhbjCLIjho), an this [parallel presentation](https://youtu.be/kTJoFLcdtn8).

Manifold learning addresses the problem of high-dimensional data. Dimensionality reduction improves computational analysis, and it is beneficial as long as it preserves most of the important information.

An $n = 698$ dataset of $64 \times 64$ gray images of faces, where the faces only vary in terms of the head moving up and down or left to right, a 2D manifold is defined.


Steps:

1. Input data iid points $p_1,p_2,\dots,p_n.$
2. Construct neighborhood graph where $p,p'$ are neighbors iff $\Vert p - p' \Vert^2\leq \epsilon.$ Neighbors will have an edge connecting them.
3. Represent the graph as an $n \times n$ matrix with leading eigenvectors as the coordinates $\phi(p_{1:n}).$ The only information taken into consideration is the distances between neighbors.
4. Construct diffusion maps (This step varies between algorithms):

<img src="https://user-images.githubusercontent.com/9312897/31342297-4c0d874a-acda-11e7-8bf0-d1aca667953f.png">

One type of diffusion maps is the LAPLACIAN EIGENMAPS:

1. Construct similarity matrix:

$$S = \left [ S_{pp'}\right ]_{p,p'} \in \mathcal D$$

with $S_{pp'}= \exp\left(-\frac{1}{2} \Vert p - p' \Vert^2 \right)$ iff $p,p'$ are neighbors.

2. Construct a Laplacian matrix

$$L = I - T^{-1}S$$

with $T=\text{diag}(S1).$

3. Calculate smallest eigenvectors of $L,$ labelled $\psi^{1,\dots,m}.$
4. Cordinates of $p\in \mathcal D$ are $\psi^1(p),\psi^2(p),\dots,\psi^m(p).$



---


From this [youtube video](https://youtu.be/1jlYn7M5wSk)



<img src="https://user-images.githubusercontent.com/9312897/31319071-f5024968-ac2a-11e7-857d-ab926930aa95.png">

<img src="https://user-images.githubusercontent.com/9312897/31319081-1d893d2e-ac2b-11e7-95b1-c1216cf44329.png">

<img src="https://user-images.githubusercontent.com/9312897/31319084-29241366-ac2b-11e7-86fe-803b1d28ee6c.png">

<img src="https://user-images.githubusercontent.com/9312897/31319086-3328e58a-ac2b-11e7-8249-86f534a4683c.png">

The Laplacian operator has to do with partial derivatives.

In the usual case, the *gradient* applied to a *function* $f$ is

$$\text{grad}(f) =\left< \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}.\frac{\partial f}{\partial z}\right >$$

in this case the basis is $\vec i, \vec j, \vec k;$ however in the case under study the basis is formed by the partials $\partial i =\frac{\partial}{\partial i}.$

This gradient determines a vectorial field $X$

$$X = \text{grad}(f) =\left< \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z}\right >$$

*Divergence* applied to this vector field, 

$$\text{div}(X)=\frac{\partial X_1}{\partial x}+\frac{\partial X_2}{\partial y}+\frac{\partial X_3}{\partial z}$$

it will produce the Laplacian of the function:

$$\Delta f = \text{div}\left(\text{grad}(f)\right)= \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}+\frac{\partial^2 f}{\partial z^2}$$

---

I presume we would need [exponential maps](https://en.wikipedia.org/wiki/Exponential_map_(Riemannian_geometry)) to draw geodesic lines on the manifold.


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
