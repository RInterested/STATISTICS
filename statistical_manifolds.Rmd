---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###STATISTICAL LEARNING or NON-LINEAR DIMENSION-REDUCTION ALGORITHM:
<br>

From this [MMDS lecture](https://youtu.be/ddhbjCLIjho), an this [parallel presentation](https://youtu.be/kTJoFLcdtn8).

Manifold learning addresses the problem of high-dimensional data. Dimensionality reduction improves computational analysis, and it is beneficial as long as it preserves most of the important information.

An $n = 698$ dataset of $64 \times 64$ gray images of faces, where the faces only vary in terms of the head moving up and down or left to right, a 2D manifold is defined. In this case the different orientations are shown on an isomap embedding:

<img src="https://user-images.githubusercontent.com/9312897/31346046-c76e23d0-ace5-11e7-95d1-4b9477c2022f.png">

with each ellipse showing one of the $698$ faces.

---


###Steps in the process:

1. Input data iid points $p_1,p_2,\dots,p_n.$
2. Construct neighborhood graph where $p,p'$ are neighbors iff $\Vert p - p' \Vert^2\leq \epsilon.$ Neighbors will have an edge connecting them.
3. Represent the graph as an $n \times n$ matrix with leading eigenvectors as the coordinates $\phi(p_{1:n}).$ The only information taken into consideration is the distances between neighbors.
4. Construct diffusion maps (This step varies between algorithms):

<img src="https://user-images.githubusercontent.com/9312897/31342297-4c0d874a-acda-11e7-8bf0-d1aca667953f.png">

The question that arises, then, is which of these embeddings is "correct."


One type of diffusion maps is the LAPLACIAN EIGENMAPS:

1. Construct similarity matrix:

$$S = \left [ S_{pp'}\right ]_{p,p'} \in \mathcal D$$

with $S_{pp'}= \exp\left(-\frac{1}{2} \Vert p - p' \Vert^2 \right)$ iff $p,p'$ are neighbors.

2. Construct a Laplacian matrix

$$L = I - T^{-1}S$$

with $T=\text{diag}(S1).$

3. Calculate smallest eigenvectors of $L,$ labelled $\psi^{1,\dots,m}.$
4. Cordinates of $p\in \mathcal D$ are $\psi^1(p),\psi^2(p),\dots,\psi^m(p).$





---


From this [youtube video](https://youtu.be/1jlYn7M5wSk)



<img src="https://user-images.githubusercontent.com/9312897/31319071-f5024968-ac2a-11e7-857d-ab926930aa95.png" width="500">


####Riemann metric:

A Riemannian metric on a differentiable manifold $M$ is a mapping that assigns to each point of the manifold $M$ an inner product $g_p$ on the tangent space $T_pM;$ thus, for every two vector fields $X_p, Y_p \in M$, the function $g_p(X_p, Y_p)=\left<X_p,Y_p\right>_p$ is smooth. An $M$ manifold provided with a Riemannian metric is called Riemannian manifold. The metric does not depend on the choice of local coordinates. In addition it can be expressed

$$g_p(X_p, Y_p) =\sum_{i,j}^n v_i\,w_j\,\left< \partial_{i\vert p, \partial_{j\vert p}}  \right>$$

with $\large \partial_i =\frac{\partial}{\partial x_i},$

where $v_i$ and $w_j$ are the coefficients in the representation of $X_p$ and $Y_p$ in the canonical basis of the tangent space $T_p(M)$ given by $\{\partial_{1\vert p},\dots,\partial_{n\vert p}\}.$ The terms $g_{ij}(p)$ represent the entries of the matrix $g(p),$ which is symmetric and definite positive.

The existence of the metric allows defining the length of vectors (of curves). The curve for which the shortest distance is presented is called geodesic.



<img src="https://user-images.githubusercontent.com/9312897/31319084-29241366-ac2b-11e7-86fe-803b1d28ee6c.png" width="500">

####Laplacian operator:

Let $(M,g)$ be a Riemannian manifold. For any smooth function $f$ over $M,$ the gradient is defined as the vector field $\text{grad}(f)$ in $T(M)$ that satisfies $\left<\text{grad}(f), X(f) \right>_g= X(f)$ for all $X\in T(M).$ In local coordinates, the gradient is written as 

$$(\text{grad(f)})_i =\sum_j g^{ij}\,\frac{\partial f}{\partial x_j},$$ where $g^{ij}$ are the components of the inverse matrix of $g=[g_{ij}].$ Then

$$\text{grad}(f) = \sum_{i,j=1}^n  \underbrace{\left(g^{ij} \frac{\partial f}{\partial x_j} \right)}_{\text{grad}(f)}\partial_i$$

In the same local coordinates, the divergent of $X$ is written

$$\text{div}(X) = \frac{1}{\sqrt{\det g}}\sum_i \frac{\partial}{\partial x_i}\left(\sqrt{\det g}\,X_i\right)$$
The Laplacian or Laplace-Beltrami operator on $(M,g)$ of a smooth function $f: M\to \mathbb R$ is defined as:

$$\Delta_g \, f = \text{div(grad)}(f))= \frac{1}{\sqrt{\det g}}\sum_j \frac{\partial}{\partial x_j}\left(\sum_i g^{ij}\sqrt{\det g}\,\frac{\partial f}{\partial X_i}\right)$$


The Laplacian operator has to do with partial derivatives.

In the usual case, the *gradient* applied to a *function* $f$ is

$$\text{grad}(f) =\left< \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}.\frac{\partial f}{\partial z}\right >$$

in this case the basis is $\vec i, \vec j, \vec k;$ however in the case under study the basis is formed by the partials $\partial i =\frac{\partial}{\partial i}.$

This gradient determines a vectorial field $X$

$$X = \text{grad}(f) =\left< \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z}\right >$$

*Divergence* applied to this vector field, 

$$\text{div}(X)=\frac{\partial X_1}{\partial x}+\frac{\partial X_2}{\partial y}+\frac{\partial X_3}{\partial z}$$

it will produce the Laplacian of the function:

$$\Delta f = \text{div}\left(\text{grad}(f)\right)= \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}+\frac{\partial^2 f}{\partial z^2}$$

---

I presume we would need [exponential maps](https://en.wikipedia.org/wiki/Exponential_map_(Riemannian_geometry)) to draw geodesic lines on the manifold.


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
