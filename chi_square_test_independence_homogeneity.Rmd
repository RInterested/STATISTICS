<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>



###APPLICATION OF THE CHI SQUARE TEST:


<br><br>

####GENERAL TEST STATISTIC (regardless of specific application):

The chi square statistic is denoted as $\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the observed number of cases in category $i$, and $E_i$ is the expected number of cases in category $i$. 


The null and alternative hypotheses for each chi square test are:

$H_o: O_i=E_i$ and $H_1:O_i\neq E_i$.

The theoretical distribution is a continuous distribution, but the $\chi^2$ statistic is obtained in a discrete manner, on the basis of discrete differences betweenthe observed and expected values. Hence, there is a need for asymptotics: The condition in which the chi square statistic is approximated by the theoretical chi square distribution, is that the sample size is reasonably large: there should be at least $5 \,\text{expected cases}$ in at least $80\%$ of the cells (with all the cells having expected values $> 1$). If there are considerably less than this, adjacent categories may be grouped together to boost the number of expected cases.

<br>

**Concepts:**

• The __GOODNESS of FIT (GoF) TEST__ is a way of determining whether a set of categorical data came from a claimed discrete distribution or not. The null hypothesis is that they did and the alternate hypothesis is that they didn't. It answers the question: $\color{blue}{\text{are the frequencies I observe for my categorical variable consistent with my theory?}}$ The goodness-of-fit test expands the one-proportion z-test. The one-proportion z-test is used if the outcome has only two categories. The goodness-of-fit test is used if you have two or more categories. In the goodness-of-fit test __there is only one observed variable__, which makes it different from the test of independence (two variables recorded for every observational unit).

• The __TEST of HOMOGENEITY__ is a way of determining whether two or more sub-groups of a population share the same distribution of a single categorical variable. For example, do people of different races have the same proportion of smokers to non-smokers, or do different education levels have different proportions of Democrats, Republicans, and Independent. ***The test of homogeneity expands on the two-proportion z-test.*** The two proportion z-test is used when the response variable has only two categories as outcomes and we are comparing two groups. ***The homogeneity test is used if the response variable has several outcome categories, and we wish to compare two or more groups.***

• The __TEST of INDEPENDENCE__ is a way of determining whether ***two categorical variables*** are associated with one another in the population, like race and smoking, or education level and political affiliation. 


---

• Homogeneity and independence sound the same. The difference is conceptual:

####Test of independence:

1. Observational units are collected at random from a population and two categorical variables are observed for each unit. 
2. Two variables are observed for each observational unit.
3. The ***margins are considered fixed***. The test conditions on both margins - No cross-multiplication of margins.

<br>
      Example:
      
      Couples are sampled and "husband satisfaction" and "wife satisfaction are measured."

<br>

####Test of homogeneity: 

1. The data are collected by randomly sampling from ***each sub-group separately***. (Say, 100 blacks, 100 whites, 100 American Indians, and so on.) The null hypothesis is that each sub-group shares the same distribution of ***another categorical variable.*** (Say, "chain smoker", "occasional smoker", "non-smoker".) 
2. The margins of the variables are considered random variables distributed normally, and we cross-multiply to get expected frequencies.


<br>
      Examples:
      
      1. Patients on drug A and patients in drug B would be sub-groups of the variable ("treatment"), and each assessed for the variable "heartburn".
      
      2. Relationship between "sex" and "class". Take samples of both genders and check their socioeconomic class.



---

###TEST of HOMOGENEITY:

<br>

It is based on the analysis of a cross classification on a contingency table to test the possible dependency or relationship between variables.

There is a conceptual distinction between the test of independence and the [chi-square test of homogeneity][3], see [here](http://stats.stackexchange.com/q/101181/67822) and [here](http://stats.stackexchange.com/q/77634/67822),  although there are no practical mathematical consequences. A test of independence would condition on both margins, whereas in a test of homogeneity the marginals of one of the categories are assumed not be random variables, but rather fixed by design. These theoretical distinctions aside, both can be lumped under the terms *chi-square test of association* (for two-way contingency tables) or *chi-square k-independent samples comparison test* (for more two categories). This is in contradistinction to the **one-sample chi-square test of agreement**, essentially a GOF test.

For larger samples (> 5 expected frequency count in each cell) the $\chi^2$ provides an approximation of the [significance value][1]. The test is based on calculating the *expected frequency counts* obtained by cross-multiplying the marginals (assuming normal distribution of the marginals, it makes sense that we end up with a $\chi^2$ distributed test statistic, since if $X\sim N(\mu,\sigma^2)$, then $X^2\sim \chi^2(1))$:

The goodness of fit test examines only one variable, while the test of independence is concerned with the relationship betweentwo variables. Like the goodness of fit test, the chi square test of independence is verygeneral, and can be used with variables measured on any type of scale, nominal, ordinal, interval or ratio. As with the GOF test the expected number of cases in each cell should be more than $5$.

The chi square test for independence is conducted by assuming that there is no relationship between the two variables being examined. The alternative hypothesis is that there is some relationship between the variables.

The chi square statistic used to conduct this test is the same as in the goodness of fit test:

$\chi^2=\displaystyle \sum_i \frac{(O_i-E_i)^2}{E_i}$ where $O_i$ is the  numbers of cases in each cell ofthe cross classiØcation table, and $E_i$ is the expected number of cases.

The chi square statistic computed from the observed and expected values is calculated, and if this statistic is inthe region of rejection of the null hypothesis, then the assumption of no relationship between $X$ and $Y$ is rejected.

<br>

####Example 1:

The example given in the [test of proportions entry](http://rinterested.github.io/statistics/tests_of_proportions.html) is a valid example.

<br>

####Example 2:

Is there a relationship between sex and class?

```{r}
tab <- matrix(c(33,153,103,16,29,181,81,14), nrow =4)
dimnames(tab)<-list(Social.Class = c("Upper","Middle","Working","Lower"), Sex = c("Male", "Female"))
addmargins(tab)

```

Under the NULL hypothesis the two variables are unrelated and therefore:

$\small P(\text{Upper} \,\text{and}\, \text{Male}) = P(\text{Upper})P(\text{Male})$

$\small P(\text{Upper}\,\text{and}\,\text{Male}) =\frac{62}{610}\frac{305}{610}$

and the number of expected cases for the cell:

$610 \, \frac{62}{610}\frac{305}{610}$

The general formula for the degrees of freedom is the number of rows minus 1, times the number of columns minus 1.

```{r}
chisq.test(tab)
```

The independence of sex and class cannot be rejected.

<br>

####TEST of INDEPENDENCE of TWO VARIABLES:

<br>

The best illustration is Agresti's example with husband and wife marital satisfaction:

```{r}
sex_satis <- matrix(c(7,2,1,2,7,8,5,8,2,3,4,9,3,7,9,14), nrow = 4)
dimnames(sex_satis) = list(wife = c("Never","Sometimes","Often","Always"), husband = c("Never","Sometimes","Often","Always"))
sex_satis
```

It is to be excpected that, in general, the satisfaction of a spouse moves together with the other. 

If we run a $\chi^2$ test we get,

```{r, warning=FALSE}
chisq.test(sex_satis, correct = T)
```

that with a probability of $<\,5\%$ of making a type I error, the counts in the table cannot be considered independent - we reject the $H_o$ hypothesis of independence, and assume that there is an association between the satisfaction of the husband and the wife.

In this test, the margins are considered fixed, and the expected counts calculated as follows:

<img HEIGHT="800" WIDTH="500" src="https://cloud.githubusercontent.com/assets/9312897/10900528/0f965f56-81af-11e5-9e52-7f15234f9917.png">

What this is saying is that the expected values in the cells are conditioned on both margins (row and column), and it should be read as along the lines as, for example: the expected value of "never/never" is the probability that the wife "never" experiences satisfaction ($19/91$)  ***times*** (independent of) the probability that the husband "never" experiences pleasure ($12/91$). Then we calculate based on the resultant probability the expected frequency in the cell by multiplying by the total number of cases ($91$).


In contradistinction in the test of homogeneity (as in the example above with antacids - table reproduced below this paragraph) we are saying that, for instance, getting the result "Drug A/heartburn" is the probability of "heartburn" ($156/368$) (one single variable). Then we calculate the expected frequency by multiplying this times the number of patients taking Drug A ($178$).

```{r, warning=FALSE, echo=FALSE}
Antacid <- matrix(c(64, 178 - 64, 92, 190 - 92), nrow = 2)    
dimnames(Antacid) = list(Symptoms = c("Heartburn", "Normal"),
                        Medication = c("Drug A", "Drug B"))
addmargins(Antacid)
```

---

The test statistic will be:

$\sum \frac{(O-E)^2}{E}$ with $\small df = (rows-1)*(cols - 1)$

The exact p-value can be obtained with a Monte Carlo simulation:

```{r,echo=F, warning=F}
library(gsheet)
data <- read.csv(text =  gsheet2text('https://docs.google.com/spreadsheets/d/1w575Rfj2QY30wWc6piunwcyDOfYQfL-5KSulJIQ3ego/edit?usp=sharing',format ='csv'))
data <- data[,-1]
```

We need first the dataset in long format:

```{r}
head(data)
```

If we perform first the chi square test on the dataset:

```{r, warning = F}
chisq.test(table(data),correct = F)$statistic
```

... and then we permute either the husband or the wife (swingers?), calculating the chi square value every time, and record the percentage of time that it will be greater than the original chi statistic:

```{r, warning = F}
baseline <- chisq.test(table(data),correct = F)$statistic
dat <- data
statistic <- 0
for(i in 1: 1000){dat[,2] <- sample(dat[,2])
statistic[i] <- chisq.test(table(dat), correct = F)$statistic}
(p_value <- length(statistic[statistic>=baseline])/length(statistic))
```


So, under the assumption of independence (swingers) only in this small fraction of case would we obtain such a high chi square value to begin with. Hence, we can reject the hypothesis of independence, and assume there is dependence between the husbands' and wives' satisfaction.

We can calculate this "exact" test (really a generalization of Fisher's exact test) with the command:

```{r}
chisq.test(table(data), simulate.p.value = T)
```


[1]: https://en.wikipedia.org/wiki/Fisher%27s_exact_test
[3]: http://www.r-bloggers.com/comparison-of-two-proportions-parametric-z-test-and-non-parametric-chi-squared-methods/

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
