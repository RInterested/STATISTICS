<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###SUPPORT VECTOR MACHINES:
<br>

From this [MIT video](https://youtu.be/_PwhiWxHK8o).

It is predicated upon drawing a decision boundary line leaving as ample a margin to the first positive and negative examples as possible:

<br>

<img width="700" src="https://cloud.githubusercontent.com/assets/9312897/16438253/f52790e8-3d7b-11e6-8846-56d147c60e0f.png">


As in the illustration above, if we select an orthogonal vector such that $\large \lVert w \rVert=1$ we can establish a decision criterium for any unknown example $\bf u$ to be catalogued as positive of the form:

$\large \color{blue}{w}  \cdot {\bf u}  \geq c$, corresponding to a value that would place the projection beyond the decision line in the middle of the street. Notice that $\color{blue}{w}  \cdot {\bf u} = {\bf u} \cdot \color{blue}{w}$. 

An equivalent condition would be:

$$\Large\color{blue}{w}\cdot  u + b \geq 0 \tag{decision line or rule: Eq.1}$$

with $c = - b$ the sample is positive.

So we need $b$ and $\color{blue}{w}$ to have a decision rule. To get there we need *constraints*. 

First constraint we are going to impose is that for any positive sample $x_+$ (we could have used $u_+$), $\color{blue}{w}\cdot \bf x_+ + b \geq 1$; and for negative samples, $\color{blue}{w}\cdot \bf x_- + b \leq -1$. 

<br>

<img width="500" src="https://cloud.githubusercontent.com/assets/9312897/22224716/9563046a-e18d-11e6-8400-2df9a1ab9d3e.png">

<br>

In the division boundary or hyperplane (**median**) the value would be $0$, leaving the values of the different strips as:


<img width="700" src="https://cloud.githubusercontent.com/assets/9312897/22255139/88c5086a-e224-11e6-86e6-53dc0002efcf.png">

<br>

---

To bring these two inequalities together, we can introduce the variable $y_i$ so that $y_i=+1$ for positive examples, and $y_i=-1$ if the examples are negative, and conclude $\large y_i (x_i\cdot \color{blue}{w} + b) -1\geq 0$. 

So we establish that this has to be greater than zero, *but* if the example is on the hyperplanes that maximize the margin of separation between the decision hyperplane and the tips of the support vectors, in this case a lines), then:

$$\Large y_i \,(x_i\cdot \color{blue}{w} + b) -1 = 0\tag{Eq.2}$$


Notice that this is equivalent to requiring that $y_i \,(x_i\cdot \color{blue}{w} + b)  = 1.$

<br>
<img width="500" src="https://cloud.githubusercontent.com/assets/9312897/22209100/440e9898-e153-11e6-87cf-dd47a34f67d7.png">

---

<br>


Second constraint: the distance of the desicion hypeplane to the tips of the support vectors ("the street") will be maximized:


<img height="500" width="500" src="https://cloud.githubusercontent.com/assets/9312897/16401058/1d780816-3caf-11e6-8477-610b6bcf9b76.png">

Again, assuming a unit vector perpendicular to the decision boundary, the dot product with the difference between two "bordering" plus and minus examples is the width of "the street":

$$\Large \text{width}= (x_+ \,{\bf -}\, x_-) \cdot \frac{w}{\lVert w \rVert} \tag{the width of the street}$$

<br>

Now, key observation: on the equation above $x_+$ and $x_-$ *are* in the gutter (on hyperplanes maximizing the separation)! Therefore, for the positive example: $\large ({\bf x_i}\cdot \color{blue}{w} + b) -1 = 0$, or $\large {\bf x_+}\cdot \color{blue}{w} = 1 - b$; and for the negative example: $\large {\bf x_-}\cdot \color{blue}{w} = -1 - b$. So, reformulating the width of the street:

<br>

$\text{width}=(x_+ \,{\bf -}\, x_-) \cdot \frac{w}{\lVert w \rVert}= \frac{x_+\cdot w \,{\bf -}\, x_-\cdot w}{\lVert w \rVert}=\frac{1-b-(-1-b)}{\lVert w \rVert}= \frac{2}{\lVert w \rVert} \tag{the width of the street: Eq.3}$

<br>

So now we just have to maximize the width of the street - maximize $\large  \frac{w}{\lVert w \rVert}$ (or maximize $\Large  \frac{2}{\lVert w \rVert}$, or minimize $\Large  \lVert w \rVert)$, or minimize:

$$\large \text{min}\frac{1}{2}\lVert w \rVert^2 \tag{minimization objective: Eq.4}$$

which is mathematically convenient.

So we want to minimize this expression based on some constraints. Hence we need a Lagrange multiplier (going back to equations 2 and 4):

<br>

$$\large \mathscr{L} = \frac{1}{2} \lVert w \rVert^2 - \sum \lambda_i \Big[y_i \, \left( x_i\cdot \color{blue}{w} + b \right) -1\Big]\tag{Lagrange: Eq.5}$$


Differentiating,

<br>

$\large \frac{\partial \mathscr{L}}{\partial \color{blue}{w} }= \color{blue}{w} - \sum \lambda_i y_i x_i = 0$. Therefore, 

$$\large\color{blue}{w} = \sum \lambda_i \,\, y_i \,\,  x_i\tag{Orthonormal vector: Eq.6}$$

$\Large \frac{\partial \mathscr{L}}{\partial b}=-\sum \lambda_i y_i = 0$, which means that

$\Large \sum \lambda_i \, y_i = 0\tag{zero sum multipliers times labels: Eq.7}$

Pluging equation [Eq.6] back into [Eq.5], 

<br>

$$\large \mathscr{L} = \frac{1}{2} \color{purple}{\left(\sum \lambda_i y_i  x_i \right) \,\left(\sum \lambda_j y_j  x_j \right)}- \color{green}{\left(\sum \lambda_i y_i  x_i\right)\cdot \left(\sum \lambda_j y_j  x_j \right)} - \sum \lambda_i y_i b +\sum \lambda_i$$

<br>

The penultimate term is zero as per equation [Eq.7].

Therefore,


$$\large \mathscr{L} = \boxed{ \sum \lambda_i - \frac{1}{2}\displaystyle \sum_i \sum_j  \lambda_i \lambda_j\,\, y_i y_j \,\,  x_i \cdot x_j}\tag{final Lagrangian: Eq.8}$$

Hence, the optimization depends on the dot product of pairs of examples.

Going back to the "decision rule" equation [Eq.1] above, and using [Eq.6]:

<br>

$$\large \sum \lambda_i y_i  x_i\cdot u + b \geq 0\tag{final decision rule: Eq.9}$$


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
