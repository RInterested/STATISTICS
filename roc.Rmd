---
output: 
  html_document:
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

### ROC curves and AUC:

There are reference answers on-line [here](https://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture21.htm) [here](http://stats.stackexchange.com/q/14153/67822), [here](http://stats.stackexchange.com/a/85413/67822) and [here](http://stackoverflow.com/q/23131897/4089351).


DEFINITIONS:

$\text{Sensitivity or TP Rate} = \frac{TP}{TP\, +\, FN}$

![](https://user-images.githubusercontent.com/9312897/87492385-77d23100-c618-11ea-9ff3-a2005e57bd0d.png)

---

$\text{Specificity} = \frac{TN}{TN\, +\, FP}$

![](https://user-images.githubusercontent.com/9312897/87492580-d9929b00-c618-11ea-8bcd-51066f50e9f2.png)

---

$\text{FP Rate or Fall-out} = \frac{FP}{TN\, +\, FP} = 1 - \text{Specificity}$

![](https://user-images.githubusercontent.com/9312897/87492732-3726e780-c619-11ea-94f8-fcb76e3b826b.png)

---

$\text{FN Rate or Miss rate} = \frac{FN}{TP\, +\, FN}$

![](https://user-images.githubusercontent.com/9312897/87493173-38a4df80-c61a-11ea-97fa-36da8852a929.png)


---

$\text{PPV or Precision} = \frac{TP}{TP\, +\, FP}$

![](https://user-images.githubusercontent.com/9312897/87493392-b7018180-c61a-11ea-948c-7335de534166.png)

---

$\text{NPV or Negative predictive value} = \frac{TN}{FN\, +\, TN}$

![](https://user-images.githubusercontent.com/9312897/87493624-2d05e880-c61b-11ea-927d-18f5795dc8db.png)

---

$\text{FOR or False omission rate} = \frac{FN}{FN\, +\, TN}$

![](https://user-images.githubusercontent.com/9312897/87493729-735b4780-c61b-11ea-8910-48b7fc9d7db6.png)

---

$\text{FDR or False discovery rate} = \frac{FP}{TP\, +\, FP}$

![](https://user-images.githubusercontent.com/9312897/87493975-f086bc80-c61b-11ea-8fd7-5a6e25127b3f.png)


---

In addition we can get the following rates using the entire table:

$\text{Prevalence} = \frac{TP+FN}{TP\, +\, FP +\, TN +\, FN}$


![](https://user-images.githubusercontent.com/9312897/87494088-2f1c7700-c61c-11ea-94de-6734c3b27a92.png)

---

$\text{Accuracy or frac correct classif.} = \frac{TP+TN}{TP\, +\, FP +\, TN +\, FN}$

![](https://user-images.githubusercontent.com/9312897/87494311-a9e59200-c61c-11ea-99b9-3e6c2debaa49.png)

---

$\text{frac incorrect classif.} = \frac{FP+FN}{TP\, +\, FP +\, TN +\, FN}$

![](https://user-images.githubusercontent.com/9312897/87494408-ed400080-c61c-11ea-8e2d-9fc7da6de66c.png)


---

Utilizing the PSA antigen data from [here](http://research.fhcrc.org/content/dam/stripe/diagnostic-biomarkers-statistical-center/files/psa2b.csv), where `fpsa` stands for "free PSA" and `tpsa` stands for "total PSA". There are repeated measures in time `t` at different `age`-s. The outcome is `d` and is coded as `0` or `1`:

```{r, warning = F, message=FALSE, echo=FALSE}
library(gsubfn)
library(sqldf)
library(tcltk)
library(pROC)
library(ROCR)
library(Epi)
library(gsheet)
```

The following is the dataset from an age-matched case-control study: each diagnosed case was assigned a control matched to case by date of birth. There are 70 non diagnosed and 71 diagnosed:

```{r, warning=F}
data = read.csv(text = gsheet2text('https://docs.google.com/spreadsheets/d/1ba1IYjX79EEmgBmsEzZgNs6Joe4v-SECWZUhbfhewhc/edit?usp=sharing', format ='csv'))
data = sqldf("select id, d, min(t), fpsa, tpsa, age from 'data'\n group by id")
data$age_group <- cut(data$age, breaks = c(45,55,65,75), labels = c("(45,55]","(55,65]","(65,75]"))
colnames(data)[2] = "outcome"
head(data)
```

Generating ROC curves with pROC:

```{r, fig.width=6, fig.height=4, message=FALSE}
# With total PSA:
par(pty="s")
r = roc(data$outcome, data$tpsa)
plot.roc(r, print.auc=TRUE, auc.polygon=TRUE, auc.polygon.col=rgb(.3,0,.8,0.2), print.thres=TRUE)
plot.roc(smooth(r), add=TRUE, col=2)
```

Notice that the thresholds used to construct the ROC plot, together with the calculated sensitivities and specificities are stored in `roc`:

```{r}
rock = pROC::roc(data$outcome, data$tpsa)
str(rock)

# Let's take a peek under the hood:

roc_scaffolding = data.frame(rock$thresholds, rock$sensitivities, rock$specificities)
head(roc_scaffolding)
```

In addition to the PSA we can stratify by patient's age:

```{r, message = F, fig.width=5, fig.height=3}
# With age groups:

plot.roc(x = data$outcome[data$age_gr == "(45,55]"], 
      predictor = data$tpsa[data$age_gr == "(45,55]"], 
      print.auc = TRUE, col = "green", print.auc.col = "green", 
      print.auc.y = 0.97, print.auc.x = 0.5)

plot.roc(x = data$outcome[data$age_gr == "(55,65]"], 
      predictor = data$tpsa[data$age_gr == "(55,65]"], 
      print.auc = TRUE, col = "blue", print.auc.col = "blue", add = TRUE, 
      print.auc.y = 0.82, print.auc.x = 0.7)

plot.roc(x = data$outcome[data$age_gr == "(65,75]"], 
      predictor = data$tpsa[data$age_gr == "(65,75]"], 
      print.auc = TRUE, col = "red", add = TRUE, print.auc.col = "red", 
      print.auc.y = 0.7, print.auc.x = 0.8)
```

And with ROCR:

```{r, fig.width=5, fig.height=5}
# With ROCR:
pred <- prediction(data$tpsa, data$outcome)
perf <- performance(pred, "tpr", "fpr")
# performance metrics TPR: True Positive Ratio FPR: False Positive Ratio
plot(perf, col = "green")
abline(0, 1, col = "grey")
auc <- performance(pred, "auc")
legend("bottomright", paste(round(as.numeric(auc@y.values), digits = 2)), col = c("green"), pch = c(3))
```

---

Alternatively, the `Epi` package can be used. In this case we are dealing with predicting the clinical [outcome in patients with subarachnoid hemorrage](http://link.springer.com/article/10.1007%2Fs00134-009-1641-y).

```{r}
data(aSAH)
head(aSAH)
rock = ROC(form = outcome ~ s100b, data=aSAH, plot = "ROC", MX = T)
```

Notice the values in the $18$-th row of the `ROC$res` printout - they are the values printed on plot above as `lr.eta` values:

```{r}
rock$res[18,]
```    

The ***maximized sum threshold or MST*** or the [**probability** of the outcome](http://rinterested.github.io/statistics/logistic_regression.html) is the `lr.eta` - in other words, the sigmoid function:



<br>
$$\large \mathbb{E}[Y_i|X_i]=\frac{1}{1 + e^{-(\beta_0+\beta_1\times\text{s100b})}}$$
The `lr.eta` corresponds to the probability at the input corresponding to the [Youden's index](https://stackoverflow.com/a/38532555/4089351).

<br>

```{r, fig.width=6, fig.height=5}
rc <- ROC(form = outcome ~ s100b, data=aSAH, plot="sp") # Saving the ROC for later
ROC(form = outcome ~ s100b, data=aSAH, plot="sp" ) # Getting the ROC plot
# The following lines just literally color the black lines just printed:
lines(rc$res$lr, rc$res$spec, lwd = 2, type="s", col="red") #specificity
lines(rc$res$lr, rc$res$sens, lwd = 2, type="s", col="green") # sensitivity
lines(rc$res$lr, rc$res$pvp,  lwd = 2, type="s", col="orange") # pvp
lines(rc$res$lr, rc$res$pvn,  lwd = 2, type="s", col="magenta") # pvn
```


The maximal combination of sensitivity and specificity is given by:

```{r}
## optimal combination of sensitivity and specificity:

which.max(rowSums(rc$res[, c("sens", "spec")])) # Again the same value from row 18!
```

This is not the way a cut-off value is selected to report the result of a test as positive in clinical practice. Utilities (loss/cost) are needed to solve for the optimum cutoff on predicted risk." See comments [here](http://stats.stackexchange.com/a/67563/67822). 

Given that the parameters of the logistic regression are...

```{r}
rc$lr
```

this "optimal sensitivity/specificity" value in row 18 corresponds to:

$$0.30426295405785 =\frac{1}{1+e^{-(-1.759+4.904\times \text{s100b})}}$$

and

$$\text{s100b}= 0.1900327$$

A plausible value of `s100b`:

```{r}
summary(aSAH$s100b) # Remember that not all values are used as cutoff points.
```

that will optimize sensitivity and specificity.

If we wanted to get the [cutoff points](http://stackoverflow.com/a/38532555/4089351), 
we'd have to use `sort(unique(aSAH))`. Let's see the value in position $18$...

```{r}
(points = c("Inf",sort(unique(aSAH$s100b))))[18]
```

<br>

---

#### DETERMINING A THRESHOLD VALUE:

From [here](https://www.researchgate.net/publication/321363535_ThresholdROC_Optimum_Threshold_Estimation_Tools_for_Continuous_Diagnostic_Tests_in_R):

For the two-state setting, we used a dataset from Kapaki, Paraskevas, Zalonis, and Zournas(2003), which contains measurements of tau protein levels in the cerebrospinal ﬂuid of $49$ control subjects and $49$ patients with Alzheimer’s disease (AD). The authors reported that the cut-oﬀ of 317 led to an optimal combination of sensitivity $(0.88)$ and speciﬁcity $(0.96),$ producing $R = 1.$ We calculated an alternative threshold for this dataset using the functions in ThresholdROC. This approach accounts for speciﬁc characteristics of the problem by choosinga reasonable combination of costs based on clinical criteria. We set the costs corresponding to correct classiﬁcation at zero (that is, $\text{CTP = CTN = 0}$) given that there are no consequences when correct decisions are made. The costs corresponding to false classiﬁcations were set at $1$ (i.e., $\text{CFP = CFN = 1}$), placing the same weight on false positives and false negatives. The value for AD prevalence was set at $0.2$ based on the literature (Tsolaki, Fountoulakis, Pavlopoulos, Chatzi, and Kazis 1999; Ferri et al. 2005). To determine if the measurements from both the control and diseased groups could be assumed to follow a normal distribution in deciding the method to use for threshold estimation, we applied Shapiro-Wilk’s test to the measurements, obtaining $p < 0.01$ in both cases. Since the data failed the normality tests, the empirical method was used. Thus, using `thres2()`, the threshold that minimizes the cost function for these cost and prevalence values was estimated to be $384.45,$ corresponding to a sensitivity of $0.76$ and a speciﬁcity of $1.$ Using a conﬁdence level of $95\% $ and applying the bootstrap methodology with $1000$ resamples, the conﬁdence intervals of the threshold were $(304.40,464.51)$ for the bootstrap method based on the normal distribution and $(295.37,444.03)$ for the percentile technique. 

```{r, fig.height = 4, fig.width = 10}

require(ThresholdROC)
k1 = AD[AD$Status==0,'Tau']
k2 = AD[AD$Status==1,'Tau']
rho = 0.2   # Disease prevalence
FPC = 1     # False negative cost
FNC = 1     # False positive cost

costs = matrix(c(0,0,FPC,FNC),2,2,byrow=T)
thresh <- thres2(k1, k2, rho, R=NULL, costs, "empirical", ci.method='boot', extra.info=T)

par(mfrow=c(1,3))
plot(thresh)
plotCostROC(thresh)
```


If we can't tolerate any false positives:

```{r, fig.height = 4, fig.width = 10}
par(mfrow=c(1,3))
FPC = 1000
FNC = 1     # If the cost of missing any cases is huge:
costs = matrix(c(0,0,FPC,FNC),2,2,byrow=T)
thresh <- thres2(k1, k2, rho, R=NULL, costs, "empirical", ci.method='boot', extra.info=T)
plot(thresh)
plotCostROC(thresh)
```

If the cost of missing any cases is huge:

```{r, fig.height = 4, fig.width = 10}
par(mfrow=c(1,3))
FPC = 1  
FNC = 1000
costs = matrix(c(0,0,FPC,FNC),2,2,byrow=T)
thresh <- thres2(k1, k2, rho, R=NULL, costs, "empirical", ci.method='boot', extra.info=T)
plot(thresh)
plotCostROC(thresh)

```


---

#### MANUAL REPRODUCTION:

<br>
```{r, warning=F, message=FALSE}
data("aSAH")
(data <- aSAH[1:5,]) # We take only the first x rows - for instance 5
contrasts(data$outcome)
fit <- glm(outcome ~ s100b, data, family = "binomial") # This is the logistic regression that we will use as a predictor.
```

---

[ODDS RATIOS:](https://drive.google.com/file/d/0Bwl-HpVJ_5PeVE5BTU5QVUlxRGs/view?usp=sharing)

$$\color{blue}{\text{odds(Y=1)}} = \frac{\Pr\,(Y=1)}{1\,-\,\Pr\,(Y=1)} = e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p} = e^{\,\beta_o}\,e^{\,\beta_1x_1}\,e^{\,\beta_2x_2}\,\cdots\,e^{\,\beta_px_p}$$

Therefore a unit increase in $x_1$ increases the odds $\large e^{\,\beta_1}$ times. The factors $\large e^{\,\beta_i}$ are the **ODDS RATIO**'s. On the other hand, $\beta_i$ (the coefficients) are the **LOG ODDS-RATIO**.

```{r, warning=F, message=FALSE}

exp(coef(fit))
exp(confint(fit))
```

---

#### PROBABILITIES:

PROBABILITIES given again by the sigmoid function:


$$\color{green}{\text{p(Y = 1)}} = \frac{\color{blue}{\text{odds(Y=1)}}}{1\,+\,\color{blue}{\text{odds(Y=1)}}}=\frac{e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}}{1 \,+\,e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}}=\frac{1}{1 \,+\,e^{-(\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}}$$

In a `glm()` (logistic regression) model, `predict()` simply gives you the LINEAR EQUATION ($\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$).

```{r, warning=F, message=FALSE}
probabilities = exp(predict(fit)) / (  1 + exp(predict(fit))  ) 
```

These are the predicted probabilities (not the odds or the results of the logistic regression) for each value of s100b in the dataset.

Let's get the thresholds from the pROC package:

```{r, warning=F, message=FALSE}
(threshold = roc(data$outcome, data$s100b)$thresholds) 
```

Here we have them... Compare them to s100b... Parallel, but not quite:

```{r, warning=F, message=FALSE}
sort(data$s100b)
```

Let's add the predicted probabilities:

```{r}
data$probabilities <- probabilities

# and order the dataset just to take a peek at what it looks like...

(data[with(data, order(s100b)), ])

# Now let's what happens with different cutoff points...

cut <- rep(0, length(threshold)) # Establishing cutoff points empty vector
for (i in 1:length(threshold)){
cut[i] <- exp(predict(fit,data.frame(s100b=threshold[i]))) / (1 + exp(predict(fit,data.frame(s100b=threshold[i]))))
}
cut
```

The cuts are the different probabilities of having a good outcome based on the `s100b` theresholds. We have to decide what probability is "high enough" to classify all values with a higher probability as "POOR" outcome. Sometimes we'll be right; others wrong - it is a "supervised" learning process.

These are going to be the cutoff points:

```{r}
cut[is.na(cut)] <- 1.01 * max(data$probabilities) # These is to get rid of NA's.
cut
```

* For `cut[1] = 0`, all outcomes predicted as "Poor" (`1`): 
$\text{sens} = \frac{1\text{TP}}{1\text{ "Poor"}}=1/1$, $\text{spec} = \frac{0\text{TN}}{4\text{"Good"}}=0/4$

* For `cut[2] = 0.02880979`, there will $4$ predicted as "Poor" (`1`):
$\text{sens} = \frac{1\text{TP}}{1\text{"Poor"}}=1/1$, $\text{spec} = \frac{1\text{TN}}{4\text{"Good"}}=1/4$

* For `cut[3] = 0.1639541`, there will $3$ predicted as "Poor" (`1`):
$\text{sens} = \frac{1\text{TP}}{1\text{"Poor"}}=1/1$, $\text{spec} = \frac{2\text{TN}}{4\text{"Good"}}=2/4$

* For `cut[4] = 0.3122394`, there will $1$ predicted as "Poor" (`1`):
$\text{sens} = \frac{0\text{TP}}{1\text{"Poor"}}=0/1$, $\text{spec} = \frac{3\text{TN}}{4\text{"Good"}}=3/4$

* For `cut[5] = 0.36256518`, there will $0$ predicted as "Poor" (`1`):
$\text{sens} = \frac{0\text{TP}}{1\text{"Poor"}}=0$, $\text{spec} = \frac{4\text{TN}}{4\text{"Good"}}=4/4$

Coordinates then are:

$(1,0)$, $(1, 1/4)$, $(1, 1/2)$, $(0,3/4)$, $(0,1)$

<img HEIGHT="400" WIDTH="400" src="https://cloud.githubusercontent.com/assets/9312897/17070019/b7ed3494-5027-11e6-843e-89e4c2950dbe.png">
<br>


In this particular example, then, the third cutoff or threshold point $\color{red}{0.115}$, associated with a probability of $0.16395$ would result in the highest combination of sensitivity and specificity.

---

<br>

If the probabilities of "success"" (outcome 1 or, most commonly, "disease") predicted by the logistic regression order the values perfectly along the actual presence of "success", we could end up with a system like:

<br>
<img HEIGHT="700" WIDTH="800" src="https://cloud.githubusercontent.com/assets/9312897/13262903/3dd94fe8-da35-11e5-9690-fcf7541c5cbf.png">
<br>

In contradistinction, in a really bad test the predicted probabilities would bear no relation to the actual presence of the outcome, and the ROC curve would form a perfect diagonal:

<br>
<img HEIGHT="700" WIDTH="800" src="https://cloud.githubusercontent.com/assets/9312897/13262887/2a45b4e4-da35-11e5-9ae1-4307c7edfe26.png">
<br>

<br>
In general, though, the shape of an ROC curve will tend to be something like:

<br>
<img HEIGHT="700" WIDTH="800" src="https://cloud.githubusercontent.com/assets/9312897/13266022/08c94e98-da44-11e5-8ace-15a74052bc64.png">
<br>


<br>
```{r, message =F, warning = F}
# Let's automate:

data$outcome <- as.numeric(data$outcome) - 1
data[with(data, order(probabilities)), ]

positive <- 0
negative <- 0
tru_pos <- 0
tru_neg <- 0
for (i in 1:length(cut)){
sum(data$outcome)
positive[i] <- nrow(data[data$probabilities > cut[i], ])
negative[i] <- nrow(data) - positive[i]

# tru_pos will be the sum of the actual "1" or "successes" on the rows with values in the predictor function higher than the cutoff point:

tru_pos[i]  <- sum(data[data$probabilities > cut[i], ]$outcome)

# tru_neg will be the sum of the actual "1" or "successes" on the rows with values in the predictor function lower than the cutoff point (these are FN) subtracted from all the negatives:

tru_neg[i]  <- negative[i] - sum(data[data$probabilities < cut[i], ]$outcome)
}

sens   <- tru_pos / sum(data$outcome)
specif <- tru_neg / (length(data$outcome) - sum(data$outcome))
(rock = cbind(cut, positive, negative,tru_pos,tru_neg, sens, specif))

par(mfrow=c(1,2))

# What we want:

plot.roc(data$outcome, data$s100b, 
    auc.polygon = TRUE, 
    auc.polygon.col=rgb(.35,0.31,0.61, alpha = 0.4), 
    auc.polygon.border=rgb(.35,0.31,0.61, 0.4))


# what we got will be superimposed as a red line on top of the ROC generated by R:

plot.roc(data$outcome, data$s100b,lwd=15,col="gray75")
rock <- rbind(rep(0,7), rock)
lines(rock[,6] ~ rock[,7],xlim=rev(range(rock[,7])), lwd = 2,col ="red") 
polygon(rock[,6] ~ rock[,7], xlim=rev(range(rock[,7])), col=rgb(.9,0.1,0.1, alpha = 0.4))
segments(1,0,0,1)
segments(1,0,0,1)
```

Now for the entire dataset:

```{r, echo=F, message=F, warning = F}
data <- aSAH
fit <- glm(outcome ~ s100b, data, family = "binomial")
threshold <- function(predict, response) {
  r <- roc(response, predict)
  r$thresholds
}
threshold <- threshold(data$s100b, data$outcome)
data$predicted <- exp(predict(fit)) / (  1 + exp(predict(fit))  )
cut <- rep(0, length(threshold)) # Establishing cutoff points
for (i in 1:length(threshold)){
cut[i] <- exp(predict(fit,data.frame(s100b=threshold[i]))) / (1 + exp(predict(fit,data.frame(s100b=threshold[i]))))
}
cut[is.na(cut)] <- 1.01 * max(data$predicted)
data$outcome <- as.numeric(data$outcome) - 1

positive <- 0
negative <- 0
tru_pos <- 0
tru_neg <- 0
for (i in 1:length(cut)){
sum(data$outcome)
positive[i] <- nrow(data[data$predicted > cut[i], ])
negative[i] <- nrow(data) - positive[i]
tru_pos[i]  <- sum(data[data$predicted > cut[i], ]$outcome)
tru_neg[i]  <- (negative[i] - sum(data[data$predicted < cut[i], ]$outcome)) 
               + (positive[i] - tru_pos[i])
}

sens <- tru_pos/sum(data$outcome)
specif <- tru_neg / (length(data$outcome) - sum(data$outcome))
rock = cbind(cut, positive, negative,tru_pos,tru_neg, sens, specif)

par(mfrow=c(1,2))

# What we want:

plot.roc(data$outcome, data$s100b, 
    auc.polygon = TRUE, 
    auc.polygon.col=rgb(.35,0.31,0.61, alpha = 0.4), 
    auc.polygon.border=rgb(.35,0.31,0.61, 0.4))


# what we got will be superimposed as a red line on top of the ROC generated by R:

plot.roc(data$outcome, data$s100b,lwd=15,col="gray75")
rock <- rbind(rep(0,7), rock)
lines(rock[,6] ~ rock[,7],xlim=rev(range(rock[,7])), lwd = 2,col ="red") 
polygon(rock[,6] ~ rock[,7], xlim=rev(range(rock[,7])), col=rgb(.9,0.1,0.1, alpha = 0.4))
segments(1,0,0,1)
segments(1,0,0,1)
```

We can calculate the AUC manually:

```{r}
AUC = function(sens,specif){
    partitions <- 1 / length(threshold[2:(length(threshold) - 1)])
    rect <- partitions * sens[2:((length(sens) - 1) )]
    triang <- partitions * sens[2:((length(sens) - 1) )] / 2
    sum(rect) + sum(triang)
}
AUC(sens,specif)
roc(data$outcome, data$s100b, print.auc = T)

```


---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
