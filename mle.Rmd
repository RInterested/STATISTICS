<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###MAXIMUM LIKELIHOOD ESTIMATION (MLE):
<br>

From [this post](https://www.r-bloggers.com/maximum-likelihood/):

We want to estimate the mean and variance of the stem diameters (in mm) of Pinus radiata trees based on twelve observations, and using a normal model:

The point of departure is the pdf of the normal distribution:

$$f_X(x \vert \mu,\sigma) = \frac{1}{\sqrt{2\pi}\,\sigma}\exp{\frac{-(x-\mu)^2}{2\sigma^2}}$$

> [Wikipedia](en.wikipedia.org/wiki/Maximum_likelihood_estimation) Now we look at this function from a different perspective by considering the observed values $x_1, x_2,\dots, x_n$ to be fixed "parameters" of this function, whereas $\theta$ will be the function's variable and allowed to vary freely; this same function will be called the likelihood:

$$\mathcal L( \mu,\sigma ; x_i)=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\,\sigma}\exp{\frac{-(x_i-\mu)^2}{2\sigma^2}}$$

Here the observed values $x_i$ are fixed, and the function depends on the parameters $\mu$ and $\sigma$. The idea is to maximize the value of the formula. However, multiplying small fractions becomes numerically unstable. In comes the log-likelihood function:

the log-likelihood:

$$\ln \mathcal {L}(\theta \,;\,x_{1},\ldots ,x_{n})=\sum _{i=1}^{n}\ln f_X(x_{i}\vert \theta )$$

The average log-likelihood function is denoted as $\hat l$:

$$\hat l=\frac {\ln \mathcal L}{n}$$

The method of maximum likelihood estimates Î¸0 by finding a value of $\theta$ that maximizes $\hat l (\theta ;x)$. This method of estimation defines a maximum likelihood estimator (MLE) of $\theta$:

$$\{{\hat {\theta }_{\mathrm {mle} }\}\subseteq \{{\underset {\theta \in \Theta }{\operatorname {arg\,max} }}\ {\hat {l}}(\theta \,;\,x_{1},\ldots ,x_{n})\},}$$

In many instances, there is no closed form, and an computational or [iterative procedures](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Iterative_procedures) will be used. Here is a link to Andrew Ng's [gradient descent](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) will be used.

---

\begin{align}\log \mathcal L( \mu,\sigma ; x_i) &= \sum_{i=1}^n \log\left(\frac{1}{\sqrt{2\pi}\,\sigma}\exp{\frac{-(x_i-\mu)^2}{2\sigma^2}}\right)\\
&= \sum_{i=1}^n\log\left( \frac{1}{\sqrt{2\pi}\,\sigma}\right)+\sum_{i=1}^n\log\left(\exp{\frac{-(x_i-\mu)^2}{2\sigma^2}}\right)\\
&= \sum_{i=1}^n\log\left( (2\pi)^{-1/2}\right) + \sum_{i=1}^n \log\left( (\sigma)^{-1}\right)+\sum_{i=1}^n\log\left(\exp{\frac{-(x_i-\mu)^2}{2\sigma^2}}\right)\\
&=-\frac{n}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \mu)^2
\end{align}

Now taking the partial derivative with respect to $\mu$:

$$\frac{\partial \log L}{\partial \mu}=  \frac{1}{2\sigma^2} 2 \sum_{i=1}^n(x_i - \mu)$$

and setting it to zero:

$$0 =  \sum_{i=1}^n(x_i - \mu)$$

$$\mu =  \frac{\sum_{i=1}^nx_i}{n}$$

Actually, this is the mean, and it is the **sample** mean providing and estimation of $\mu$ (i.e. $\hat \mu$).

We can also take the partial with respect to $\sigma$:

\begin{align}\frac{\partial \log L}{\partial \sigma}&= -\frac{n}{\sigma}+\frac{2}{2}\sum_{i=1}^n(x_i - \mu)^2 \sigma^{-3}\\
&=-\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum_{i=1}^n(x_i - \mu)^2\\
\end{align}

And setting it to zero:

$$0 = -\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum_{i=1}^n(x_i - \mu)^2$$

Hence,

$$\sigma^2 =\frac{\sum_{i=1}^n (x_i - \mu)^2}{n}$$

which is really a biased estimate of the variance in the population, $\sigma$ (i.e. $s^2$). This bias arises because maximum likelihood estimates do not take into account the loss of degrees of freedom when estimating fixed effects.


```{r}
diams = c(150, 124, 100, 107, 170, 144,
          113, 108, 92, 129, 123, 118)

# A simple function to calculate log-likelihood.
# Values will be easier to manage
loglike = function(data, mu, sigma) {
  loglike = 0
    for(obs in data){
      loglike = loglike +
                log(1/(sqrt(2*pi)*sigma) *
                       exp(-1/2 * (obs - mu)^2/(sigma^2)))
    }
    return(loglike)
}
# Let's try some combinations of parameters and
# plot the results
params = expand.grid(mu = seq(50, 200, 1),
                     sigma = seq(10, 40, 1))
params$logL = with(params, loglike(diams, mu, sigma))
summary(params)

 
library(lattice)
contourplot(logL ~ mu*sigma, data = params, cuts = 20)

# Zooming in
params = expand.grid(mu = seq(120, 126, 0.01),
                     sigma = seq(18, 25, 0.1))
params$logL = with(params, loglike(diams, mu, sigma))
summary(params)
 
contourplot(logL ~ mu*sigma, data = params, cuts = 10)


# Compare to:

mean(diams); sd(diams)
```

---

##### BINOMIAL EXAMPLE:

Let's see what the likelihood function looks like with a [binomial experiment](https://youtu.be/2xCKuRXWAQ0) yielding $3 \text{ heads}$ and $2 \text{ tails}:$ 

```{r}
p = seq(0, 1, 0.001)
L = p^3 * (1 - p)^2
plot(p, L, type = "l", xlab="p", ylab="L(p(x))")
max(L)
d = cbind(p,L)
(MLE = d[which.max(d[,2]), 1])
arrows(x0 = MLE, y0 = 0, x1 = MLE, y1 = max(L))
abline(h = 0); abline(h = max(L))

# As for the log-likelihood:

loglik = 3 * log(p) + 2 * log(1 - p)
plot(p, loglik, type = "l")
d = cbind(p,loglik)
MLE = d[which.max(d[,2]), 1]
arrows(x0 = MLE, y0 = -20, x1 = MLE, y1 = max(loglik))
abline(h = -20); abline(h = max(loglik))
```

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
