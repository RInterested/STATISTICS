---
title: 'Tensor examples'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>
<br><br>


### PRACTICAL EXAMPLES AND REVIEW OF TENSORS:

From this question in [Math.SE](https://math.stackexchange.com/q/4932883/994433):

#### Tensor Contraction:

##### Original Question:

According to [Wikipedia](https://en.wikipedia.org/wiki/Tensor_product#Tensor_product_of_vector_spaces#Tensor_product_of_linear_maps) if the bases vectors are fixed the two tensors $A$ and $B$ result in the tensor product given by the Kronecker multiplication: 

$$A = \begin{bmatrix} 
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2}
\end{bmatrix}$$

$$B = \begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix}$$

$$\small T = A \otimes B =\small
\begin{bmatrix} 
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2}
\end{bmatrix} \otimes
\begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix} \\=\begin{bmatrix}
a_{1,1}\begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix} & 
a_{1,2}\begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix} \\
a_{2,1}\begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix} &
a_{2,2}\begin{bmatrix} 
b_{1,1} & b_{1,2} \\
b_{2,1} & b_{2,2}
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix} 
a_{1,1}b_{1,1} & a_{1,1}b_{1,2} & a_{1,2}b_{1,1} & a_{1,2}b_{1,2} \\
a_{1,1}b_{2,1} & a_{1,1}b_{2,2} & a_{1,2}b_{2,1} & a_{1,2}b_{2,2} \\
a_{2,1}b_{1,1} & a_{2,1}b_{1,2} & a_{2,2}b_{1,1} & a_{2,2}b_{1,2} \\
a_{2,1}b_{2,1} & a_{2,1}b_{2,2} & a_{2,2}b_{2,1} & a_{2,2}b_{2,2}
\end{bmatrix}
$$

I presume that $T$ would have two upper and two lower indices, as in $T^{ij}{}_{kl}$. 

Now I should be able to contract $i$ and $k,$ equivalently of taking the trace of the $A$ matrix and multiplying it by $B$, i.e. $\mathop{Tr}(A)B$. This would correspond to $T^{ij}{}_{il}$. 

Given that each matrix $A$ and $B$ is $2 \times 2$, this would have to reduce the last matrix above from its $4 \times 4$ dimensions.

How would this be implemented in a given numerical calculation if there is no possibility to recover the initial coefficients of $A$ and $B$?

The entries involved appear in the final matrix already multiplied with entries of $B$:

$$
\begin{bmatrix} 
\color{red}{a_{1,1}}b_{1,1} & \color{red}{a_{1,1}}b_{1,2} & a_{1,2}b_{1,1} & a_{1,2}b_{1,2} \\
\color{red}{a_{1,1}}b_{2,1} & \color{red}{a_{1,1}}b_{2,2} & a_{1,2}b_{2,1} & a_{1,2}b_{2,2} \\
a_{2,1}b_{1,1} & a_{2,1}b_{1,2} & \color{red}{a_{2,2}}b_{1,1} & \color{red}{a_{2,2}}b_{1,2} \\
a_{2,1}b_{2,1} & a_{2,1}b_{2,2} & \color{red}{a_{2,2}}b_{2,1} & \color{red}{a_{2,2}}b_{2,2}
\end{bmatrix}
$$

I am not asking for an answer along the lines of "that is why using matrices is not a good idea", but I want to understand why there is no contradiction, and how this would work in higher order tensors by extension.

---

##### Answer:

> The Wikipedia article is slightly confusing by omitting some brackets from the final tensor. It looks like a $4\times4$ matrix, i.e., a rank $2$ tensor with $4^2=16$ entries, but actually it is a $2\times2\times2\times2$ tensor of rank $4$ having $2^4=16$ entries. Contraction happens by taking the trace of four $2\times2$ "submatrices" separately:

$$\begin{bmatrix} 
\hbox{Tr}\begin{bmatrix}a_{1,1}b_{1,1} & a_{1,2}b_{1,1}\\
a_{2,1}b_{1,1} & a_{2,2}b_{1,1}\end{bmatrix}
&
\hbox{Tr}\begin{bmatrix}a_{1,1}b_{1,2} & a_{1,2}b_{1,2} \\
a_{2,1}b_{1,2} & a_{2,2}b_{1,2}
\end{bmatrix}
\\
\hbox{Tr}\begin{bmatrix}a_{1,1}b_{2,1} & a_{1,2}b_{2,1}\\
a_{2,1}b_{2,1} & a_{2,2}b_{2,1}\end{bmatrix}
&
\hbox{Tr}\begin{bmatrix}a_{1,1}b_{2,2} & a_{1,2}b_{2,2}\\
a_{2,1}b_{2,2} & a_{2,2}b_{2,2}\end{bmatrix}
\end{bmatrix}$$

---

##### Comments:

This makes perfect sense. Thank you! As a quick follow-up question, what would be the mechanics if instead I wanted to contract $j$ and $l$, or $i$ and $l$ in $T^{ij}{}_{kl}$? (Unless it is worth a different question, or can't be answered in a quick couple of lines).

---

> You would still take traces of 4 submatrices at a time, only different ones. 

---

Interesting how all the info from the entries not involved in the contraction is literally thrown out... Tempting to say "no trace of it". So all that matters is the correct arrangements of the indices involved, which outside the constraints of a matrix, it would not be relevant... 

---

> Any contraction throws out information from all components where the two indices involved in the contraction have different values. The ordinary trace of a square matrix is just the simplest example of that.

---

What about [this answer](https://math.stackexchange.com/a/2811377/152225)?

##### Question:


So I'm having trouble to compute tensor contractions with "actual" numbers from the matrix representations of the tensors. I have only seen abstract theoretical examples on the internet so I'm asking for a bit of help on how to find the contractions given the expressions of the tensors and the pair of indices where we will carry out the contraction. I'll show a simple example and I hope you can help me.

Let's suppose we have two tensors of the type (1,1) (that means 1 contravariant, 1 covariant). They will be called Y and Z and knowing their coordinate forms, we can represent them through matrices in this way:

Y = \begin{pmatrix}1&-1\\2&3\end{pmatrix}

Z = \begin{pmatrix}-1&0\\1&2\end{pmatrix}

Now, we could compute the Kronecker product Y  x  Z in order to get a type (2,2) tensor (2 contravariant, 2 covariant). The result would be:

\begin{pmatrix}-1&0&1&0\\1&2&-1&-2\\-2&0&-3&0\\2&4&3&6\end{pmatrix}

So, how could we get the contractions of this (2,2) tensor (with actual numbers instead of the parameters that you see in other examples) ?

Note that here we have 2 contravariant indices and 2 covariant indices so there are 4 possible contractions depending on the pair of indices that you choose, I guess.
As far as I know, all possible contractions with the selection of pairs of indices are the following:

- 1st covariant index and 1st contravariant index.<br/>
- 1st covariant index and 2nd contravariant index.<br/>
- 2nd covariant index and 1st contravariant index.<br/>
- 2nd covariant index and 2nd contravariant index.<br/>

So what matrices would be the result of those contractions. Just in case I didn't state it clearly before, these matrices are the representation of tensor of which we know the coefficients of their coordinate forms (explicit forms in a specific basis, and that info is known). So we want to perform contractions in the last 4x4 matrix that I showed (which represents a (2,2) tensor) and there a 4 different possibilities. My question has to do with the fact that I can't find a way to do this with actual numbers and I can't figure out the result for each possible contraction (I also don't know what the differences would be in the calculation of a contraction with a certain pair of indices or another).

I would really appreciate that someone could find a specific result for the contractions that I proposed (I guess the calculation is easy but I just don't know how it could be done). Thank you really much for reading.


##### Answer:

Representing tensors using matrix notation is often confusing, but let's assume that

$Y = \begin{pmatrix}y^1_1&y^1_2\\v^2_1&y^2_2\end{pmatrix}$

and similarly for Z. If $W = Y \times Z$ then the components of $W$ are

$w^{ik}_{jl} = y^i_j z^k_l$

You have represented W as a 4x4 matrix, but it would be more accurate to represent it as a 2x2 matrix, each of whose entries is another 2x2 matrix.

Anyway, the four possible contractions of W are:

(1):  $w^{ik}_{il} = y^i_i z^k_l$

(2):  $w^{ik}_{jk} = y^i_j z^k_k$

(3):  $w^{ik}_{ji} = y^i_j z^k_i$

(4):  $w^{ij}_{jl} = y^i_j z^j_l$

In terms of matrix operations:

(1) is the component representation of $Tr(Y)Z$

(2) is the component representation of $Tr(Z)Y$

(3) is the component representation of $YZ$

(4) is the component representation of $ZY$

---

> The answer that you refer to, remains correct. We only appear to take 4 different traces; if you apply the distributive law then we are only taking the trace of $A$, multiplied by the four components of $B$ separately. But I wrote it this way because you asked for a computation that did not rely on separate knowledge of $A$ or $B$.

---

Sounds correct:

The matrix of traces in the answer is

$$\begin{bmatrix}
a11b11 + a22b11 & a11b12 + a22b12 \\
a11b21 + a22b21 & a11b22 + a22b22
\end{bmatrix} =
(a11 + a22) 
\begin{bmatrix}
b11 & b12 \\
b21 &   b22
\end{bmatrix}
= \mathop(Tr) B$$

$$a_{11}b_{11} + a_{22}b_{11} = -1 + (-3) = -4$$

$$a_{11}b_{12} + a_{22}b_{2} = 0 + 0 = 0$$
$$a_{11}b_{21} + a_{22}b_{21} =1 + 3 = 4$$
$$a_{11}b_{22} + a_{22}b_{22} = 2+ 6 = 8$$

are the traces in your answer, and correspond to $\text{Tr}(Y)Z=4 \begin{pmatrix}-1&0\\1&2\end{pmatrix}$


---

> I have to qualify my above comment that it is always traces of submatrices. That holds if you contract over the two indices of $A$ or over the two indices of $B$, but if you contract over, say, $\color{red}i$ and $\color{orange}l$ -  $T^{{\color{red}i}j}{}_{k{\color{orange}l}}$ - then you would have a matrix product.





or



---


From [this question](https://math.stackexchange.com/q/2437459/152225):

If we have to (co)-vectors in $V^*:$ 

$$\beta=\begin{bmatrix}1 &2 &3 \end{bmatrix}$$ and  $$\gamma=\begin{bmatrix}2 &4 &6 \end{bmatrix}$$ 

the $(2,0)$-tensor $\beta\otimes \gamma$ is the outer product:

$$\beta\otimes_o \gamma=\begin{bmatrix}2\,e^1\otimes e^1&4\,e^1\otimes e^2&6\,e^1\otimes e^3\\4\,e^2\otimes e^1&8\,e^2\otimes e^2&12\,e^2\otimes e^3\\6\,e^3\otimes e^1&12\,e^3\otimes e^2&18\,e^3\otimes e^3\end{bmatrix}$$

Now if apply this tensor product on the vectors

$$v=\begin{bmatrix}1\\-1\\5\end{bmatrix}, \; w = \begin{bmatrix}2\\0\\3\end{bmatrix}$$

$$\begin{align} (\beta \otimes \gamma)[v,w]&=\\[2ex]
& 2 \times  1 \times 2   \quad+\quad    4 \times   1  \times  0   \quad +\quad    6  \times  1  \times 3 \\
+\;&4 \times -1 \times 2  \quad + \quad   8 \times  -1  \times  0   \quad + \quad  12  \times -1  \times 3 \\
+\;&6 \times  5 \times 2  \quad + \quad  12 \times   5  \times  0  \quad  + \quad  18  \times  5  \times 3 \\[2ex]
&= 308\end{align}$$

Is this correct?

Yes, this is correct. The answer should be $\left<\beta,v\right> \cdot \left<\gamma,w\right>$, where $\langle\,,\,\rangle$ is the usual inner product:

$$\vec \beta \cdot \vec v \times \vec \gamma \cdot \vec w = 308.$$
```{r}
v = c(1,-1,5); w = c(2,0,3); beta = 1:3; gamma = c(2,4,6); beta %*% v * gamma %*% w
```

---

From [this source](https://drive.google.com/file/d/0Bwl-HpVJ_5PeTXBLWkZFSkl2ZFk/view?usp=sharing):

<br>

<img width="700" src="https://cloud.githubusercontent.com/assets/9312897/23309287/15f82fea-fa7c-11e6-941d-c7179d18afa6.png">

---


Applied to the case in [this question](http://math.stackexchange.com/q/2158892/152225), the change of basis matrix is $\small\begin{bmatrix}3&4&-1\\0&3&7\\1&3&0.5\end{bmatrix}$, and its inverse $\small\begin{bmatrix}0.7&0.2&-1.1\\-0.3&-0.1&0.8\\0.1&0.2&-0.3\end{bmatrix}$. The vectors $v$ and $w$ in the new coordinate system are 

$v =\small\begin{bmatrix}0.7&0.2&-1.1\\-0.3&-0.1&0.8\\0.1&0.2&-0.3\end{bmatrix}\begin{bmatrix}1\\2\\3\end{bmatrix} =\begin{bmatrix}-2.3\\1.9\\-0.5\end{bmatrix}$ and $w=\small\begin{bmatrix}0.7&0.2&-1.1\\-0.3&-0.1&0.8\\0.1&0.2&-0.3\end{bmatrix}\begin{bmatrix}1\\0\\0\end{bmatrix}=\begin{bmatrix}0.7\\-0.3\\0.1\end{bmatrix}$. 

Therefore,

$$\begin{align}\large v\otimes w=\left(-.23\tilde x + 1.9\tilde y -0.5 \tilde z\right)\otimes \left(0.7\tilde x -0.3\tilde y + 0.1\tilde z\right)\\[2ex]=-1.6\;\tilde x\otimes \tilde x + 1.3\;\tilde x\otimes \tilde y  -0.3 \;\tilde x\otimes \tilde z + 0.6\;\tilde y\otimes \tilde x -0.5\;\tilde y\otimes \tilde y+ 0.1\;\tilde y\otimes \tilde z -0.3\;\tilde z\otimes \tilde x +0.2 \;\tilde z\otimes \tilde y-0.1\;\tilde z\otimes \tilde z\end{align}$$

So what's the point?

Starting off defining the tensor product of two vector spaces ($V\otimes W$) with the same bases, we end up calculating the [outer product of two vectors](http://math.stackexchange.com/q/796709/152225):

$$\large v\otimes_o w=\small \begin{bmatrix}-2.3\\1.9\\-0.5\end{bmatrix}\begin{bmatrix}0.7&-0.3&0.1\end{bmatrix}=\begin{bmatrix}-1.6&1.3&-0.3\\0.6&-0.5&0.1\\-0.3&0.2&-0.1\end{bmatrix}$$


This connect this post to [this more general question](http://math.stackexchange.com/q/2158734/152225).

  [1]: https://i.stack.imgur.com/Ga68v.png
  
  
---

From the [answer to this question](http://math.stackexchange.com/q/164975/152225) about Spivak's definition of tensor product as:

$$(S \otimes T)(v_1,...v_n,v_{n+1},...,v_{n+m})= S(v_1,...v_n) * T(v_{n+1},...,v_{n+m})$$

Let's first set some terminology.

Let $V$ be an $n$-dimensional real vector space, and let $V^*$ denote its dual space.  We let $V^k = V \times \cdots \times V$ ($k$ times).

- A _tensor of type $(r,s)$_ on $V$ is a multilinear map $T\colon V^r \times (V^*)^s \to \mathbb{R}$.

- A _covariant $k$-tensor_ on $V$ is a multilinear map $T\colon V^k \to \mathbb{R}$.

In other words, a covariant $k$-tensor is a tensor of type $(k,0)$.  This is what Spivak refers to as simply a "$k$-tensor."

- A _contravariant $k$-tensor_ on $V$ is a multilinear map $T\colon (V^*)^k\to \mathbb{R}$.

In other words, a contravariant $k$-tensor is a tensor of type $(0,k)$.

- We let $T^r_s(V)$ denote the vector space of tensors of type $(r,s)$.  So, in particular,

$$\begin{align*}
T^k(V) := T^k_0(V) & = \{\text{covariant $k$-tensors}\} \\
T_k(V) := T^0_k(V) & = \{\text{contravariant $k$-tensors}\}.
\end{align*}$$
Two important special cases are:
$$\begin{align*}
T^1(V) & = \{\text{covariant $1$-tensors}\} = V^* \\
T_1(V) & = \{\text{contravariant $1$-tensors}\} = V^{**} \cong V.
\end{align*}$$
This last line means that we can regard vectors $v \in V$ as contravariant 1-tensors.  That is, every vector $v \in V$ can be regarded as a linear functional $V^* \to \mathbb{R}$ via
$$v(\omega) := \omega(v),$$
where $\omega \in V^*$.

- The _rank_ of an $(r,s)$-tensor is defined to be $r+s$.

In particular, vectors (contravariant 1-tensors) and dual vectors (covariant 1-tensors) have rank 1.

--------

If $S \in T^{r_1}_{s_1}(V)$ is an $(r_1,s_1)$-tensor, and $T \in T^{r_2}_{s_2}(V)$ is an $(r_2,s_2)$-tensor, we can define their _tensor product_ $S \otimes T \in T^{r_1 + r_2}_{s_1 + s_2}(V)$ by

$$(S\otimes T)(v_1, \ldots, v_{r_1 + r_2}, \omega_1, \ldots, \omega_{s_1 + s_2}) = \\
S(v_1, \ldots, v_{r_1}, \omega_1, \ldots,\omega_{s_1})\cdot T(v_{r_1 + 1}, \ldots, v_{r_1 + r_2}, \omega_{s_1 + 1}, \ldots, \omega_{s_1 + s_2}).$$

Taking $s_1 = s_2 = 0$, we recover Spivak's definition as a special case.

**Example:** Let $u, v \in V$.  Again, since $V \cong T_1(V)$, we can regard $u, v \in T_1(V)$ as $(0,1)$-tensors.  Their tensor product $u \otimes v \in T_2(V)$ is a $(0,2)$-tensor defined by
$$(u \otimes v)(\omega, \eta) = u(\omega)\cdot v(\eta)$$

--------

As I suggested in the comments, every bilinear map -- i.e. every rank-2 tensor, be it of type $(0,2)$, $(1,1)$, or $(2,0)$ -- can be regarded as a matrix, and vice versa.

Admittedly, sometimes the notation can be constraining.  That is, we're used to considering vectors as _column vectors_, and dual vectors as _row vectors_.  So, when we write something like $$u^\top A v,$$
our notation suggests that $u^\top \in T^1(V)$ is a dual vector and that $v \in T_1(V)$ is a vector.  This means that the bilinear map $V \times V^* \to \mathbb{R}$ given by
$$(v, u^\top) \mapsto u^\top A v$$
is a type $(1,1)$-tensor.

**Example:** Let $V = \mathbb{R}^3$.  Write $u = (1,2,3) \in V$ in the standard basis, and $\eta = (4,5,6)^\top \in V^*$ in the dual basis.  For the inputs, let's also write $\omega = (x,y,z)^\top \in V^*$ and $v = (p,q,r) \in V$.  Then
$$\begin{align*}
(u \otimes \eta)(\omega, v) & = u(\omega) \cdot \eta(v) \\
& = \begin{pmatrix}
 1 \\
 2 \\
 3
\end{pmatrix} (x,y,z)
\cdot
(4,5,6) \begin{pmatrix}
 p \\
 q \\
 r
\end{pmatrix} \\
& = (x + 2y + 3z)(4p + 5q + 6r) \\
& = 4px + 5 qx + 6rx \\
& \ \ \ \ \ 8py + 10qy + 12py \\
& \ \ \ \ \ 12pz + 15qz + 18rz \\
& = (x,y,z)\begin{pmatrix}
 4 & 5 & 6 \\
 8 & 10 & 12 \\
 12 & 15 & 18
\end{pmatrix}\begin{pmatrix}
 p \\
 q \\
 r
\end{pmatrix} \\
& = \omega \begin{pmatrix}
 4 & 5 & 6 \\
 8 & 10 & 12 \\
 12 & 15 & 18
\end{pmatrix} v.
\end{align*}$$

**Conclusion:** The tensor $u \otimes \eta \in T^1_1(V)$ is the bilinear map $(\omega, v)\mapsto \omega A v$, where $A$ is the $3 \times 3$ matrix above.

The Wikipedia article you linked to would then _regard_ the matrix $A$ as being equal to the tensor product $u \otimes \eta$.

--------

Finally, I should point out two things that you might encounter in the literature.

First, some authors take the definition of an $(r,s)$-tensor to mean a multilinear map $V^s \times (V^*)^r \to \mathbb{R}$ (note that the $r$ and $s$ are reversed).  This also means that some indices will be raised instead of lowered, and vice versa.  You'll just have to check each author's conventions every time you read something.

Second, note that there is also a notion of tensor products _of vector spaces_.  Many textbooks, particularly ones focused on abstract algebra, regard this as the central concept.  I won't go into this here, but note that there is an isomorphism
$$T^r_s(V) \cong \underbrace{V^* \otimes \cdots \otimes V^*}_{r\text{ copies}} \otimes \underbrace{V \otimes \cdots \otimes V}_{s \text{ copies}}.$$

Confusingly, some books on differential geometry _define_ the tensor product of vector spaces in this way, but I think this is becoming rarer.


---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**