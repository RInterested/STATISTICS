---
title: 'Lagrange multiplier'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---


### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Lagrange Multiplier:

---

We want to optimize the value of the surface $z = x \times y$ with the constraint that $y = 3 - \frac{4}{3}x$:

<br>

<img height="600" width="500" src="https://cloud.githubusercontent.com/assets/9312897/16436652/97b580c2-3d6d-11e6-8629-6bd9fe82e52a.png">

The level curves or contour lines on the function $z$ are points with the same value of the function. When it comes to consumer spending, for example, they are referred to **indifference lines.**

The indifference line that kisses the constraint curve $y$ at one single point will contain either a maximum or a minimal value under the constraint. At the point of contact both the indifference curve in question and the line of constraint will have the same tangent. This implies that both the gradient of the function and the normal to the constraint will be on the same line at the point of contact: scalar to each other.

We try to solve a system of two equations:

$$\begin{align}
\nabla z & = \lambda \nabla y\\[2ex]
g &=0
\end{align}$$

with $g$ being $y$ with $0$ on one side of the equation.

<img height="400" width="400" src="https://cloud.githubusercontent.com/assets/9312897/16436796/e7642c62-3d6e-11e6-926a-a83ed3130cdc.png">

In the indifference line touching the constraint at one single point, 

$$f(x,y) = x\times y = \text{constant}_1.$$

The constraint line can be expresssed as $$g(x,y) = y + \frac{4}{3}x- 3 = 0.$$

On the other hand, the direction of maximum change will be given by the gradients, and they will be orthogonal to the contour lines:

$$\nabla \left(f(x,y)=\text{constant}_1\right)=\left(\frac{dz}{dx},\frac{dz}{dy}\right)$$

$$\nabla (g(x,y)=0)=\left(\frac{dg}{dx},\frac{dg}{dy}\right)$$.

Since the gradients of $f(\cdot)$ and $g(\cdot)$ are going in the same or opposite directions:

$$\nabla f(x,y) = \lambda \nabla g(x,y)$$

or

$$\nabla f(x,y) - \lambda \nabla g(x,y)= 0$$

which means that:

$$\frac{\partial f(x,y)}{dx} - \lambda \frac{\partial g(x,y)}{dx}=0$$

and

$$\frac{\partial f(x,y)}{dy} - \lambda \frac{\partial g(x,y)}{dy}=0$$

To obtain the Lagrangian:

$$\mathscr{L}=f(x,y)+\lambda \,g(x,y)$$

or

$$\mathscr{L}=x\times y+\lambda \times  \left(y + \frac{4}{3}x-3\right)$$

and we optimize by taking the partial derivatives:

$$\frac{\mathscr{L}}{dx}=y+\frac{4}{3}\times\lambda=0$$

$$\frac{\mathscr{L}}{dy}=x+\lambda=0$$

$$\frac{\mathscr{L}}{d\lambda}=y+\frac{4}{3}x-3=0$$

Notice that the last equation is the initial constraint.

This results in the optimal point: $\left(\frac{9}{8},\frac{3}{2}\right)$.


---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
