---
title: 'Orthogonal does not mean Independent or Uncorrelated'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---


### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Orthogonal, Uncorrelated and Independent Random Variables:

---

From [this post](http://stats.stackexchange.com/a/16315/67822):

Orthogonality is defined as $E[XY^⋆]=0$

So:

If $Y=X^2$ with symmetric pdf they they are dependent yet orthogonal.

If $Y=X^2$ but pdf zero for negative values, then they dependent but not orthogonal.

Therefore, orthogonality does not imply independence.

See a proof of orthogonality of these two random variables [here](http://math.stackexchange.com/a/2040727/152225).

Another way of seeing this is by considering

$$\text{Cov(X,Y)}=\sigma_{X,Y}=\mathbb E[XY]-\mathbb E[X] \mathbb E[Y]$$

Since in the case of $X \sim N(0,1),$ the $\mathbb E[X]=0,$ zero covariance is equivalent to orthogonality. See also [this post](https://stats.stackexchange.com/a/452543/67822).


---

From [this post](https://alecospapadopoulos.wordpress.com/2014/08/16/orthogonality-and-correlation/)

>I realized today that orthogonality and non-correlation (in the linear sense) of two random variables $X$ and $Y$ are strongly linked in our minds – and they shouldn’t. The fact is that whether orthogonality, i.e. $\mathbb E(XY)=0$, implies correlation, or non-correlation, depends on whether the expected values of the variables are non-zero or not. I believe in the power of tables - so here it is, and easily verifiable from the definition $$\text{Cov}(X,Y) =\mathbb E(XY) – \mathbb E(X) \mathbb E(Y)$$

<br>

<img width=900 src="https://cloud.githubusercontent.com/assets/9312897/23197481/a7b6e93e-f88e-11e6-85de-f7193799562e.png">

To illustrate with an example the first scenario, consider two Gaussian normal variables with expectations $\mathbb E[X] \neq 0 =-1$ and $\mathbb E[X] \neq 0=1$ and covariance of $1.$ This implies that $\mathbb E[XY]=\sigma_{X,Y}+\mathbb E[X] \mathbb E[Y]=0.$

```{r}
library(mvtnorm)
library(squash)
library(plot3D)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
set.seed(0)

n <- 10000

sigma <- matrix(c(1,1,1,1),nrow=2,byrow=T)
mean <- c(-1,1)

bivar <- rmvnorm(n,mean,sigma)
X <- bivar[,1]
Y <- bivar[,2]
print(paste('The mean of X is: ', round(mean(X)), ' and the mean of Y is: ', round(mean(Y)),'.'))
print(paste('The covariance of X and Y is: ', round(cov(X,Y)),'.'))
print(paste('The expectation of XY is: ', round((X %*% Y) / n),'.'))
```
Here is the bivariate histogram:

```{r}
##  Create cuts:
x_c <- cut(X, 50)
y_c <- cut(Y, 50)
##  Calculate joint counts at cut levels:
z <- table(x_c, y_c)
##  Plot as a 3D histogram:
hist3D(z=z, col = jet.col(100, alpha = 0.8), 
       colkey = F, scale = T, border='gray90',
       ticktype = 'detailed', zlab='', xlab='X', ylab='Y',
       axes3D='gray', frequency=T)
```


As for the second scenario, and keeping the same non-zero expectations we can produce non-orthogonal random variables with zero correlation: $\mathbb E[XY]=\sigma_{X,Y}+\mathbb E[X] \mathbb E[Y]=-1.$

```{r}
Sig <- matrix(c(1,0,0,1),nrow=2,byrow=T)
bivar <- rmvnorm(n,mean,Sig)

X <- bivar[,1]
Y <- bivar[,2]
print(paste('The mean of X is: ', round(mean(X)), ' and the mean of Y is: ', round(mean(Y)),'.'))
print(paste('The covariance of X and Y is: ', round(cov(X,Y)),'.'))
print(paste('The expectation of XY is: ', round((X %*% Y) / n),'.'))

##  Create cuts:
x_c <- cut(X, 50)
y_c <- cut(Y, 50)
##  Calculate joint counts at cut levels:
z <- table(x_c, y_c)
##  Plot as a 3D histogram:
hist3D(z=z, col = jet.col(100, alpha = 0.8), 
       colkey = F, scale = T, border='gray90',
       ticktype = 'detailed', zlab='', xlab='X', ylab='Y',
       axes3D='gray', frequency=T)
```


The third scenario of either one of the random variables with zero expectation, and with both orthogonality and no correlation can be exemplified as before with $X \sim N(0,1)$ and $Y=X^2$: $\mathbb E[XY]=\sigma_{X,Y}+\mathbb E[X] \mathbb E[Y]=0.$ The covariance is zero as exposed [here](https://stats.stackexchange.com/a/452543/67822):

> The general form of the covariance depends on the first three moments of the distribution.  To facilitate our analysis, we suppose that $X$ has mean $\mu$, variance $\sigma^2$ and skewness $\gamma$.  The covariance of interest exists if $\gamma < \infty$ and does not exist otherwise.  Using the [relationship between the raw moments and the cumulants](https://en.wikipedia.org/wiki/Cumulant#Cumulants_and_moments), you have the general expression:

$$\begin{equation} \begin{aligned}
\mathbb{C}(X,X^2)
&= \mathbb{E}(X^3) - \mathbb{E}(X) \mathbb{E}(X^2) \\[6pt]
&= ( \mu^3 + 3 \mu \sigma^2 + \gamma \sigma^3 ) - \mu ( \mu^2 + \sigma^2 ) \\[6pt]
&= 2 \mu \sigma^2 + \gamma \sigma^3. \\[6pt]
\end{aligned} \end{equation}$$

> The special case for an unskewed distribution with zero mean (e.g., the centred normal distribution) occurs when $\mu = 0$ and $\gamma = 0$, which gives zero covariance.

```{r}
n <- 1e6
X <- rnorm(n)
Y <- X^2
print(paste('The mean of X is: ', round(mean(X)), ' and the mean of Y is: ', round(mean(Y)),'.'))
print(paste('The covariance of X and Y is: ', round(cov(X,Y)),'.'))
print(paste('The expectation of XY is: ', round((X %*% Y) / n),'.'))
```

A [similar example](https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf) could be $X \sim \text{Unif}(-1,1)$ and $Y=\vert X \vert.$ I am not sure if they are mathematically uncorrelated and orthogonal - definitely not independent:

```{r}
X <- runif(1e6,min=-1,max=1)
Y <- ifelse(X>0,X,-X)
plot(X[1:100],Y[1:100],pch=16)
rug(X[1:100],side=1,col="grey")
rug(Y[1:100],side=2,col="grey")
```


---

These concepts are nicely explained in [this post](http://stats.stackexchange.com/a/171347/67822):

__Independence__ is a statistical concept. Two [random variables](https://en.wikipedia.org/wiki/Random_variable) $X$ and $Y$ are statistically independent if their joint distribution is the product of the marginal distributions, i.e.
$$
f(x, y) = f(x) f(y)
$$
if each variable has a density $f$, or more generally
$$
F(x, y) = F(x) F(y)
$$
where $F$ denotes each random variable's cumulative distribution function.

__Correlation__ is a weaker but related statistical concept. The (Pearson) correlation of two random variables is the expectancy of the product of the standardized variables, i.e.
$$
\newcommand{\E}{\mathbf E}
\rho = \E \left [
\frac{X - \E[X]}{\sqrt{\E[(X - \E[X])^2]}}
\frac{Y - \E[Y]}{\sqrt{\E[(Y - \E[Y])^2]}}
\right ].
$$
The variables are *uncorrelated* if $\rho = 0$. It can be shown that two random variables that are independent are necessarily uncorrelated, but not vice versa.

__Orthogonality__ is a concept that originated in geometry, and was [generalized](https://en.wikipedia.org/wiki/Orthogonality#Definitions) in linear algebra and related fields of mathematics. In linear algebra, orthogonality of two vectors $u$ and $v$ is defined in [inner product spaces](https://en.wikipedia.org/wiki/Inner_product_space), i.e. [vector spaces](https://en.wikipedia.org/wiki/Vector_space) with an inner product $\langle u, v \rangle$, as the condition that
$$
\langle u, v \rangle = 0.
$$
The inner product can be defined in different ways (resulting in different inner product spaces). If the vectors are given in the form of sequences of numbers, $u = (u_1, u_2, \ldots u_n)$, then a typical choice is the [dot product](https://en.wikipedia.org/wiki/Dot_product), $\langle u, v \rangle = \sum_{i = 1}^n u_i v_i$.

***

Orthogonality is therefore not a statistical concept per se, and the confusion you observe is likely due to different translations of the linear algebra concept to statistics:

a) Formally, a space of random variables can be considered as a vector space. It is then possible to define an inner product in that space, in different ways. [One](http://stats.stackexchange.com/a/134317/17023) common [choice](http://stats.stackexchange.com/a/29172/17023) is to define it as the covariance:
$$
\langle X, Y \rangle = \mathrm{cov} (X, Y)
= \E [ (X - \E[X]) (Y - \E[Y]) ].
$$
Since the correlation of two random variables is zero exactly if the covariance is zero, *according to this definition* uncorrelatedness is the same as orthogonality. (Another possibility is to define the inner product of random variables simply as the [expectancy of the product](http://stats.stackexchange.com/a/16315/17023).)

b) Not all the [variables we consider in statistics](http://stats.stackexchange.com/a/156554/17023) are random variables. Especially in linear regression, we have independent variables which are not considered random but predefined. Independent variables are usually given as sequences of numbers, for which orthogonality is naturally defined by the dot product (see above). We can then investigate the statistical consequences of regression models where the independent variables are or are not orthogonal. In this context, orthogonality does not have a specifically statistical definition, and even more: it does not apply to random variables.

*Addition responding to Silverfish's comment:* Orthogonality is not only relevant with respect to the original regressors but also with respect to contrasts, because (sets of) simple contrasts (specified by contrast vectors) can be seen as transformations of the design matrix, i.e. the set of independent variables, into a new set of independent variables. Orthogonality for contrasts is [defined](https://en.wikipedia.org/wiki/Contrast_(statistics)#Definitions) via the dot product. If the original regressors are mutually orthogonal and one applies orthogonal contrasts, the new regressors are mutually orthogonal, too. This ensures that the set of contrasts can be seen as describing a decomposition of variance, e.g. into main effects and interactions, the idea underlying [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance).

Since according to variant a), uncorrelatedness and orthogonality are just different names for the same thing, in my opinion it is best to avoid using the term in that sense. If we want to talk about uncorrelatedness of random variables, let's just say so and not complicate matters by using another word with a different background and different implications. This also frees up the term orthogonality to be used according to variant b), which is highly useful especially in discussing multiple regression. And the other way around, we should avoid applying the term correlation to independent variables, since they are not random variables.

***

[Rodgers et al.'s](https://web.archive.org/web/20100709201307/http://www.psych.umn.edu/faculty/waller/classes/FA2010/Readings/rodgers.pdf) presentation is largely in line with this view, especially as they understand orthogonality to be distinct from uncorrelatedness. However, they do apply the term correlation to non-random variables (sequences of numbers). This only makes sense statistically with respect to the [sample correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#For_a_sample) $r$. I would still recommend to avoid this use of the term, unless the number sequence is considered as a sequence of [realizations](https://en.wikipedia.org/wiki/Realization_(probability)) of a random variable.

I've scattered links to the answers to the two related questions throughout the above text, which should help you put them into the context of this answer.


---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
