<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###COORDINATE-FREE APPROACH:

From [this youtube lecture](https://www.youtube.com/watch?v=4l-qzZOZt50&t=2395s).


####Preliminary definitions:

A [**field**](https://en.wikipedia.org/wiki/Field_(mathematics)) is a set $k$ with the triad $(k, +, \cdot)$ denoting two maps:

$$+: k \times k \rightarrow k$$

and

$$\cdot: k\times k \rightarrow k$$

that satisfy

**CANI**  conditions: closure and commutativity (abelian), associativity, neutral element, inverse for every element. CANI is also satisfied by the addition and multiplication operations, except for having to remove in the multiplication case the neutral element of addition $k\{0\}$.

---

A [**ring**](https://en.wikipedia.org/wiki/Ring_(mathematics)) $(R, +, \cdot)$ is similar but CANI only applies to $+$. For the multiplication operation the inverses (and commutativity) gone. So every field is a ring.

Example: $(\mathbb Z, +, \cdot)$ - the inverse of an inverse is not in the set.

Example: $m\times n$ matrices over $\mathbb R$ are not commutative, and not all of them have inverses.

---

A $k$ ($k$ is a field) **vector space** $(V, \color{red}{+}, \color{red}{\cdot})$ has two operations defined, and different from the operations ($+$ and $\cdot$) in the field, such that

$$+: V \times V \rightarrow V$$

and

$$\cdot: k\times V \rightarrow V$$

[fulfilling](http://math.stackexchange.com/q/1133260/152225) CANI for the $\color{red}{+}$ operation, but also ADDU: associativity, two distributive laws (over the $+$ of the field elements; or over the $\color{red}{+}$ of the vector sum operation), and scalar identity.

A **MODULE** is the equivalent of a vector space over a RING, as opposed to a FIELD.

---

A [homomorphism](https://en.wikipedia.org/wiki/Homomorphism) is a structure-preserving linear map $\large f$ between two algebraic structures of the same type (such as two groups, two rings, or two vector spaces). In the case of two vector spaces, each equipped with $+$ and $\cdot$ operations ($V$ with $\color{red}{+}$ and $\color{red}{\cdot}$ and $W$ with $\color{orange}{+}$ and $\color{orange}{\cdot}$):

$$f: V \rightarrow W$$ fulfills:

$$\forall v_1, v_2 \in V: f(v_1 \color{red}{+} v_2) = f(v_1) \color{orange}{+} f(v_2)$$

and


$$\forall \lambda \in k, v\in V: f(\lambda \color{red}{\cdot} v) = \lambda\color{orange}{\cdot}f(v).$$

A bijective linear map is called a vector space **ISOMORPHISM**.

Two vector spaces $V$ and $W$ are isomorphic $V\cong W$ if $\exists$ and isomorphism: $f: V\rightarrow W.$

An **ENDOMORPHISM** is a linear map of $V$ onto itself. Hence, $\text{End}(V) := \text{Hom}(V,V).$

An **AUTOMORPHISM** is an **invertible** linear map of $V$ on itself. Hence, $\text{Aut}(V) := \{f: V \overset{\sim}\rightarrow V | \text{invertible}\}$

Automorphisms are a subset $\text{Aut}(V) \subset \text{End}(V)$ of endomorphisms.

---


We can define the set of **ALL LINEAR MAPS** from $V \rightarrow W$ as $\text{Hom}(V,W):= \{f:V \overset{\sim}\rightarrow W\}$ with the $\sim$ denoting *linear* map.

Is this set of all linear maps a vector space? Yes, by defining,

$$\boxed{\color{blue}{+}}: \text{Hom}(V, W) \times \text{Hom}(V,W) \rightarrow \text{Hom}(V,W)$$ by taking the pair of linear functions and mapping it to

$$(f,g)\mapsto f\;\boxed{+}\;g,$$ 

where $f\;\boxed{+}\;g$ is again a map from $V$ to $W$ defined by $v\mapsto f(v) \color{orange}{+} g(v)$

and also defining 

$$\boxed{\color{blue}{\cdot}}:(\lambda\,\boxed{\color{blue}{\cdot}}\,g) (V)= \lambda \color{orange}{\cdot} g(V)$$

---

####$V$ STAR, DUAL VECTOR SPACE or $V^*:$

$$\large V^* := \text{Hom}(V, k)$$

Here $k$ is considered a vector space.

So $V^*$ is a set of linear functionals from a vector space to the field $k$, which is considered in this context just another vector space with addition and multiplication inherited from the field operations.

---

####TENSOR:

The definition of the space of $(p,q)$ tensors as the **set** of multilinear maps from the Cartesian product of elements of a vector space and its dual onto the field, equipped with addition and s-multiplication rules, given in [this series at this point in time](https://youtu.be/4l-qzZOZt50?t=33m7s) as follows:

A $(p,q)$ **tensor**, $T$ is a **MULTILINEAR MAP** that takes $p$ copies of $V^*$ and $q$ copies of $V$ and maps multilinearly (linear in each entry) to $k:$

$$\bbox[20px, border:2px solid red]{T: \underset{p}{\underbrace{V^*\times \cdots \times V^*}}\times \underset{q}{\underbrace{V\times\times \cdots  V\times V}} \overset{\sim}\rightarrow K\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(1)}$$

The **$(p,q)$ TENSOR SPACE** is defined as a set:

$$\bbox[20px, border:2px solid red]{\begin{align}T^p_q\,V &= \underset{p}{\underbrace{V\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V}} \color{darkorange}{\otimes} \underset{q}{\underbrace{V^*\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V^*}}:=\{T\, |\, T\, \text{ is a (p,q) tensor}\}\quad\quad(2)\\[3ex]&=\{T: \underset{p}{\underbrace{V^*\times \cdots \times V^*}}\times \underset{q}{\underbrace{V\times \cdots \times V}} \overset{\sim}\rightarrow K\}\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\;(3)\end{align}}$$ 

This expression symbolizing the set of all tensors where $T$ is $(p,q)$, equipped this with pointwise addition and s-multiplication.

This is (not surprisingly) consistent with the [Wikipedia definition of tensors as multilinear maps](https://en.wikipedia.org/wiki/Tensor#As_multilinear_maps).

[This answer](http://math.stackexchange.com/a/2141663/152225) in MathSE expains this notation:

Let's first look at a very special type of tensor, namely the (0,1) tensor. What is it? Well, it is the tensor product of $0$ copies of members of $V$ and one copy of members of $V^*$. That is, it is a member of $V^*$.

But what is a member of $V^*$? Well, by the very definition of $V^*$ is is a linear function $\phi:V\to K$. Let's write this explicitly:
$$T^0_1V = V^* = \{\phi:V\to K|\phi \text{ is linear}\}$$
You see, already at this point, where we didn't even use a tensor product, we get a $V^*$ on one side, and a $V$ on the other, simply by inserting the definition of $V^*$.

From this, it is obvious why $(0,q)$-tensors have $q$ copies of $V^*$ in the tensor product $(2)$, but $q$ copies of $V$ in the domain of the multilinear function in $(3)$.

OK, but why do you have a $V^*$ in the map in $(3)$ for each factor $V$ in the tensor product? After all, vectors are not functions, are they?

Well, in some sense they are: There is a natural linear map from $V$ to its double dual $V^{**}$, that is, the set of linear functions from $V^*$ to $K$. Indeed, for finite dimensional vector spaces, you even have that $V^{**} \cong V$. This natural map is defined by the condition that applying the image of $v$ to $\phi\in V^*$ gives the same value as applying $\phi$ to $v$. I suspect that the lecture assumes finite dimensional vector spaces. In that case, you can *identify* $V$ with $V^{**}$, and therefore you get
$$T^1_0V = V = V^{**} = \{T:V^*\to K|T \text{ is linear}\}$$
Here the second equality is exactly that identification.

Now again it should be obvious why $p$ copies of $V$ in the tensor product $(2)$ give $p$ factors of $V^*$ for the domain of the multilinear functions in $(3)$.

On the relations of those terms to the Kronecker product:

The tensor product $\color{darkorange}{\otimes}$ in $(2)$ is a tensor product not of (co)vectors, but of (co)vector *spaces*. The result of that tensor product describes not one tensor, but the set of all tensors of a given type. The tensors are then elements of the corresponding set. And given a basis of $V$, the tensors can then be specified by giving their coefficients in that basis.

This is completely analogous to the vector space itself. We have the vector space, $V$, this vector space contains vectors $v\in V$, and given a basis $\{e_i\}$ of $V$, we can write the vector in components, $v = \sum_i v^i e_i$.

Similarly for $V^*$, we can write each member $\phi\in V^*$ in the dual basis $\omega^i$ (defined by $\omega^i(e_j)=\delta^i_j$) as $\sum_i \phi_i \omega^i$. An alternative way to get the components $\phi_i$ is to notice that $\phi(e_k) = \sum_i \phi_i \omega^i(e_k) = \sum_i \phi_i \delta^i_k = \phi_k$. That is, the components of the covector are just the function values at the basis vectors.

This way one also sees immediately that $\phi(v) = \sum_i \phi(v^i e_i) = \sum_i v^i\phi(e_i) = \sum_i v^i \phi_i$, which is *sort of* like an inner product, but not exactly, because it behaves differently at change of basis.

Now let's look at a $(0,2)$ tensor (a tensor with $0$ vectors and $2$ covectors), that is, a bilinear function $f:V\times V\to K$ - I tentatively would look like at it as $\left(<\cdot,f>,<\cdot,f>\right).$ 
Note that $f\in V^*\color{darkorange}{\otimes} V^*$, because $V^*\color{darkorange}{\otimes} V^*$ is by definition the set of all such functions (see eq. $(3)$). Now by being a bilinear function, one again only needs to know the values at the basis vectors. In the following equation $v$ and $w$ are two arbritary vectors, and $f$ is the tensor with components $f_{ij}$. The $e_i$ and $e_j$ are the basis vectors of $V$, so that $v = \sum_i v^i\, e_i$ and $w = \sum_j\,w^j\,e_j:$

$$\begin{align}f(v,w) &= f(\sum_i v^i e_i, \sum_j w^j e_j)\\[2ex] 
&= \sum_{i,j}v^i w^j f(e_i,e_j)\\[2ex]
&\underset{*}{=} \sum_{i,j}\,f_{ij}\,v^i\,w^j \in K
\end{align}$$

$*$ we can define as components $f_{ij} = f(e_i,e_j)$ and get $f(v,w)=\sum_{i,j}f_{ij}v^iw^j$.

This goes also for general tensors: A single tensor $T\in T^p_qV$ ($p$ vectors and $q$ covectors) is a multilinear function $T:(V^*)^p\times V^q\to K$, and it is completely determined by the values you get when inserting basis vectors and basis covectors everywhere, giving the components (note that components are **numbers!**)

$$T^{i\ldots j}_{k\ldots l}=T(\underbrace{\omega^i,\ldots,\omega^j}_{p},\underbrace{e_k,\ldots,e_l}_{q})\in K$$

OK, we now have components, but we have still not defined the tensor product *of tensors.* This is a way to generate new tensors from old tensors. Another way is to add tensors, which is a map from the Cartesian product $T^p_q\,V\,\times\,T^p_q\,V\rightarrow T^p_q\,V.$ Notice that the valence of the tensor does not change.


#####Tensor product:


Be $x\in T^p_qV$, and $y\in T^r_sV$. That is, $x$ is a function that takes $p$ covectors and $q$ vectors, and gives a scalar, while $y$ takes $r$ covectors and $s$ vectors to a scalar. Then the tensor product $x\color{blue}{\otimes} y$ is a function that takes $p+r$ covectors and $q+s$ vectors, feeds the first $p$ covectors and the first $q$ vectors to $x$, and the remaining $r$ covectors and $s$ vectors to $y$, and them multiplies the result. That is,
$$(x\color{blue}{\otimes} y)(\phi_1,\ldots,\phi_{p+r},v_1,\ldots,v_{q+s}) = x(\phi_1,\ldots,\phi_p,v_1,\ldots,v_q)\underset{\text{scalar}}{\cdot} y(\phi_{p+1},\ldots,\phi_{p+r},v_{q+1},\ldots,v_{q+s})$$
It is not hard to check that this function is indeed also multilinear, and therefore $x\color{blue}{\otimes} y\in T^{p+r}_{q+s}V$.

And now finally, we get to the question what the components of $x\color{blue}{\otimes} y$ are. Well, the components of $x\color{blue}{\otimes} y$ are just the function values when inserting basis vectors and basis covectors, and when you do that and use the definition of the tensor product, you find that indeed, the components of the tensor product are the Kronecker product of the components of the factors.

Also, it can be shown that $T^p_qV$ is a vector space in its own right, and therefore the $(p,q)$-tensors can be written as the linear combination of a basis that is $1$ exactly for one combination of basis vectors and basis covectors and $0$ for all other combinations. However it can then easily be seen that this is just the tensor product of the corresponding dual covectors/vectors. Since furthermore in that basis, the coefficients on the basis vectors are just the components of the tensor as introduced before, we finally arrive at the formula
$$T = \sum T^{i\ldots j}_{k\ldots l}\underbrace{e_i\color{blue}{\otimes}\dots\color{blue}{\otimes} e_j}_{p}\color{blue}{\otimes}\underbrace{\omega^k\color{blue}{\otimes}\dots,\color{blue}{\otimes}\;\omega^l}_{q}$$


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
