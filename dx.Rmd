---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###THE ROLE OF $dx$ AS A FUNCTIONAL:
<br>

Following [Prof. Shifrin's youtube explanation](https://youtu.be/Rci22jC_pS8), and [XylyXylyX
's Invitation to K forms](https://youtu.be/yvH1ECuc7YU).

The idea is that the operator $dx^i\in V^*$ (I am using superscript following the convention that it is a covector, and as in XylyXylyX's video) would select the $i$-th coordinate of the matrix of partial derivatives of a function $f$ at a point $\vec a$.

$$\begin{bmatrix}\frac{\partial f}{\partial x^i }(\vec a)&\cdots &\frac{\partial f}{\partial x^n }(\vec a)\end{bmatrix}$$

so that the [one-form](https://en.wikipedia.org/wiki/One-form) $\text d f(\cdot)$, after $\vec a$ is input into the $(\cdot),$ would "eat" a vector $\vec v,$ to produce the partial derivative of $f$ at point $\vec a$ in the direction of $\vec v,$ notated as $D_{\vec v} f(\vec a).$ 

$$\text{d}f(\vec a) (\vec v)= \sum \frac{\partial f}{\partial x^i}(\vec a)\; dx^i\, (\vec v)$$


This would be the illustration:


<img src="https://user-images.githubusercontent.com/9312897/30996914-36eea4ca-a492-11e7-8f79-ec7a4150779b.png">



---

From [this post](http://math.blogoverflow.com/2014/11/03/more-than-infinitesimal-what-is-dx/).

<p>With the additional datum of position, vectors become <strong>tangent vectors</strong>. If \(v\) is a vector and \(p\) is a point, then we will use \(v_p\) will denote the tangent vector corresponding to \(v\) at the point \(p\). The reason they are called tangent vectors comes from their use in differential geometry, where a vast, powerful generalization of lines, planes, and other &#8220;Euclidean&#8221; spaces, called a &#8220;(smooth) manifold,&#8221; is used. A circle is an example of such a space, and the tangent line to a point on the circle can be thought of as the set of all tangent vectors to the circle at that point. For the sake of intuition, we will stay within Euclidean space.</p>

<p>Suppose \(f:\mathbb{R}^2\to\mathbb{R}\) is a <a href="http://en.wikipedia.org/wiki/Smoothness">smooth</a> function from the real plane to the real line. At each point \(p\in\mathbb{R}^2\), we can take the <a href="http://en.wikipedia.org/wiki/Directional_derivative">directional derivative</a> of \(f\) in whatever direction we like. For example, if we have coordinates \((x,y)\), then \(\left.\frac{\partial f}{\partial x}\right|_p\) is the directional derivative at \(p\) of \(f\) in the direction of increasing \(x\). If we wanted to take the directional derivative at \(p\) of \(f\) in the direction of decreasing \(x\), then we would just negate to obtain  \(-\left.\frac{\partial f}{\partial x}\right|_p\).</p>

<p>In fact, if we pick any tangent vector at \(p\), it will correspond to a directional derivative at \(p\) in the direction and with the corresponding magnitude of that tangent vector. Following the above example, taking the directional derivative at \(p\) in the direction of increasing \(x\) is the same as applying \(\left.\frac{\partial}{\partial x}\right|_p\), and going the opposite direction is the same as applying \(-\left.\frac{\partial}{\partial x}\right|_p\). Taking this a step further, we claim that tangent vectors and directional derivatives are the same thing! This may seem shocking, but all it says is that every tangent vector corresponds to a directional derivative with the same magnitude and direction. If we are working in the plane, then we can therefore identify \(\left.\frac{\partial}{\partial x}\right|_p\) with \(\begin{bmatrix}1 & 0\end{bmatrix}^T_p\) and \(\left.\frac{\partial}{\partial y}\right|_p\) with \(\begin{bmatrix} 0 & 1\end{bmatrix}^T_p\), where \(v^T\) denotes the <a href="http://en.wikipedia.org/wiki/Transpose">transpose</a> of a vector \(v\).</p>

<p>We use the transpose to use column vectors for tangent vectors. By doing this, we can let row vectors act on them by left multiplication. Note that left multiplying a column vector by a row vector is the same as taking the dot product between those two vectors. Thus, row vectors can be viewed as linear functions from column vectors to the space of real numbers. Attaching these <a href="http://en.wikipedia.org/wiki/Dual_space">linear functionals</a> to a point as we did with tangent vectors, we obtain <strong>cotangent vectors</strong>, or <strong>covectors</strong>. Hinting at the direction of conversation, in the plane we will denote the covector \(\alpha_p\) that satisfies \(\alpha_p\left(\left.\frac{\partial}{\partial x}\right|_p\right)=1\) and \(\alpha_p\left(\left.\frac{\partial}{\partial y}\right|_p\right)=0\) by \(\mathrm{d}x_p\).</p>

<p>The concept of a <a href="http://en.wikipedia.org/wiki/Vector_field">vector field</a> from multivariate calculus is best seen in the context of tangent vectors. Using our terminology, a <strong>vector field</strong> is simply a function that takes in a point in \(\mathbb{R}^n\) and outputs a tangent vector at that point. Similarly, we define a <strong>differential \(1\)-form</strong> as a function that takes in a point in \(\mathbb{R}^n\) and outputs a covector at that point. For example, the map $$\mathrm{d}x:p\mapsto\mathrm{d}x_p$$ is a differential \(1\)-form. Thus, \(\mathrm{d}x\) is simply the differential \(1\)-form that takes each point \(p\) of the space to the cotangent vector \(\mathrm{d}x_p\) at \(p\) that satisfies \(\mathrm{d}x_p\left(\left.\frac{\partial}{\partial x}\right|_p\right)=1\) and \(\mathrm{d}x_p(v_p)=0\) for all \(v_p\not\in\left\langle\left.\frac{\partial}{\partial x}\right|_p\right\rangle\).</p>

<p>How could these possibly be interpreted as infinitesimals? I think Spivak explains this best:</p>

<p>&#8220;Classical differential geometers (and classical analysts) did not hesitate to talk about &#8216;infinitely small&#8217; changes \(\mathrm{d}x^i\) of the coordinates \(x^i\), just as Leibnitz (sic) had. No one wanted to admit that this was nonsense, because true results were obtained when these infinitely small quantities were divided into each other (provided one did it in the right way).</p>

<p>&#8220;Eventually it was realized that the closest one can come to describing an infinitely small change is to describe a direction in which this change is supposed to occur, i.e., a tangent vector. Since \(\mathrm{d}f\) is supposed to be an infinitesimal change of \(f\) under an infinitesimal change of the point, \(\mathrm{d}f\) must be a function of this change, which means that \(\mathrm{d}f\) should be a function on tangent vectors. The \(\mathrm{d}x^i\) themselves then metamorphosed into functions, and it became clear that they must be distinguished from the tangent vectors \(\partial/\partial x^i\).&#8221; </p>


---

And from [this answer](https://math.stackexchange.com/a/664725/152225):


Let $f(x,y) = x^2y$.

The derivative of a function gives the best linear approximation to the function at a point.  This remains true in higher dimensions.  The derivative of the function $f$ above is 

$df = \begin{bmatrix} \frac{\partial f}{\partial x} & \frac{\partial f}{\partial y}\end{bmatrix} = \begin{bmatrix} 2xy & x^2\end{bmatrix}$.

The conceptual meaning of the derivative is this:  

$$f(x+\Delta x, y+\Delta y) \approx f(x,y) + df(\begin{bmatrix} \Delta x \\ \Delta y\end{bmatrix}) = x^2y + \begin{bmatrix} 2xy & x^2\end{bmatrix}\begin{bmatrix} \Delta x \\ \Delta y\end{bmatrix}  = x^2y+2xy\Delta x + x^2\Delta y$$

In other words, at each point $(x,y)$ the derivative is a linear map which takes a small change $\begin{bmatrix} \Delta x \\ \Delta y\end{bmatrix}$ away from the point $(x,y)$ and returns the approximate change in $f$ resulting from that.

Now say someone told me that a certain function $g$ with $g(0,0)=0$ had derivative $\begin{bmatrix} y\cos(xy) & x\cos(xy)\end{bmatrix}$, and I wanted to figure out what $g$ was.  In this case I could probably just solve the differential equations $\frac{\partial g}{\partial x} = y\cos(xy)$ and $\frac{\partial g}{\partial y} = y\cos(xy)$ by inspection, but this would not always be possible.

Let us stick to the somewhat easier problem of approximating $g(1,1)$.  Here is my idea for doing that:  I will pick a path from $(0,0)$ (whose value I know) to $(1,1)$.  I will split that path up into millions of vector changes.  Then I will use what I know about the derivative to approximate the change in $g$ over each of those small changes and add them up.  This should give me a pretty reasonable approximation.

In this case, I can see that I can pick the path $\gamma:[0,1] \to \mathbb{R}^2$ given by $\gamma(t) = (t,t)$.  Splitting this into $k$ pieces, I have the following approximations:

$$g(\frac{1}{k},\frac{1}{k}) \approx g(0,0) + dg|_{(0,0)}\left(\begin{bmatrix} \frac{1}{k} \\ \frac{1}{k}\end{bmatrix}\right)$$. 

So then 

$$g(\frac{2}{k},\frac{2}{k}) \approx g(\frac{1}{k},\frac{1}{k}) + dg|_{(\frac{1}{k},\frac{1}{k})}\left(\begin{bmatrix} \frac{1}{k} \\ \frac{1}{k}\end{bmatrix}\right) \approx g(0,0) + dg|_{(0,0)}\left(\begin{bmatrix} \frac{1}{k} \\ \frac{1}{k}\end{bmatrix}\right) + dg|_{(\frac{1}{k},\frac{1}{k})}\left(\begin{bmatrix} \frac{1}{k} \\ \frac{1}{k}\end{bmatrix}\right)$$.

Continuing on in this way , we will see that 

$$g(1,1) \approx g(0,0) + \sum_{i=0}^k dg\big|_{\frac{i}{k}}\left(\begin{bmatrix} \frac{1}{k} \\ \frac{1}{k}\end{bmatrix}\right)$$

It makes sense to give some name to this process.  We define the limit of the sum above to be the integral of the covector field $dg$ along the path $\gamma$.  Refer to my other post for the general definition, instead of just a particular example like this.

So far we have defined the integral only for derivatives of functions, and we have defined it exactly in such a way that the following fundamental theorem of calculus holds:

$$g(P_1) - g(P_0) = \int_\gamma dg$$ for any path $\gamma$ from $P_0$ to $P_1$.  But the definition of the integral never used the fact that we were integrating the derivative of a function:  it only mattered that we are integrating a covector field (i.e. a gadget which eats change vectors and spits out numbers).  So we can use exactly the same definition to give the integral of a general covector field $\begin{bmatrix} f(x,y) & g(x,y)\end{bmatrix}$, which may or may not be the differential of a function.  

(There are certainly covector fields which are not derivatives of functions.  For example, $\begin{bmatrix} x & x\end{bmatrix}$ could not be the differential of a function, for if it were we would have $\frac{\partial f}{\partial x} = x$ and $\frac{\partial f}{\partial y} = x$.  But then the mixed partials of $f$ would not be equal, contradicting Clairout's theorem.)

$dx$ is the constant covector field $\begin{bmatrix} 1 & 0\end{bmatrix}$, and $dy$ is the constant covector field $\begin{bmatrix} 0 & 1\end{bmatrix}$.  So we can write any covector field $\begin{bmatrix} f(x,y) & g(x,y)\end{bmatrix}$ as $f(x,y)dx + g(x,y)dy$.  Integrating this thing along a curve is PRECISELY what you defined as a line integral in your first multivariable calculus course.  


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
