---
title: 'Tensors Linearize Multi-linear functions'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Tensors linearize multi-linear functions:

---

Tensors are not multi-dimensional arrays. Further, the representation of a tensor as a matrix in the most simple examples is also misleading. Why? Because the idea of a tensor is to linearlize a multi-linear function.

For instance, a bilinear function (like any other multi-linear functions) is [not linear](https://math.stackexchange.com/a/1650713/152225). Take for example the area of a rectangle $A = s \times l$: Doubling the side of a rectangle, doubles the area, and the same happens with the length; however, the function is bilinear, such that doubling the side and the length result in an area of the rectangle $4$ times larger ([here](https://math.stackexchange.com/a/2064240/152225)). The dot product is such another example: the total cost of some purchases is the dot product of the number of units of each item purchased by the cost of each unit. A dubling of the price of each item, and a concomitant doubling of the number of units purchased of each item will increase four-fold the overall


```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The dot product fulfills the criteria for a [bi-variate function](https://proofwiki.org/wiki/Dot_Product_Operator_is_Bilinear), which amount to being linear on the left:

$$\langle au + bv, w \rangle = a \langle u, w \rangle + b \langle v, w \rangle$$

and on the right:

$$\langle u, av + bw \rangle = a \langle u, v \rangle + b \langle u, w \rangle.$$

```{r}
u = 1:3
v = 4:6
w = 7:9
a = 5
b = 8
# Linear on the left:
(a * u + b * v) %*% w
a * (u %*% w) + b * (v %*% w)
# Linear on the right
u %*% (a * v + b * w)
a * (u %*% v) + b * (u %*% w)
```

Linearity preserves scalar multiplication in both variables of a function $f: X \times Y \to \mathbb R$:


$$ f(\alpha x , \alpha y) = \alpha f (x,y) $$

along with the property of preserving vector addition.

However, if $f$ were bilinear, then:

$$ f(\alpha x , \alpha y) = \alpha^2 f(x,y) $$



```{r}
(a * v) %*% (a * w)
a^2 * (v %*% w)

```

A bi-linear form can be expressed as a matrix $A$, and the function be interpreted as multiplying on the left by a row vector $\vec v^\top$ and on the right by a column vector $\vec w$ as in 

$$\vec v^\top A \vec w$$

as such any operations on a row in $A$ will necessarily alter all the columns: the bi-linearity of the function is manifest in this way. Naturally, it can still be claimed that the matrix represents a $(1,1)$ tensor, but this misses out the idea behind the tensor product: the linearization of the multi-linear function. 


---

```{r}
m = matrix(c(rnorm(30)), 6, byrow=T)
m
M = rbind(m[c(1:3),], 5 * m[4,] + m[5,], m[6,])
M
det(M)
5 * det(m[c(1:4,6),]) + det(m[c(1:3,5:6),])

```


---

#### SUBHEADER

---



---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
