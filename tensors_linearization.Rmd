---
title: 'Tensors Linearize Multi-linear functions'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Tensors linearize multi-linear functions:

---

Tensors are not multi-dimensional arrays. Further, the representation of a tensor as a matrix in the most simple examples is also misleading. Why? Because the idea of a tensor is to linearlize a multi-linear function.

For instance, a bilinear function (like any other multi-linear functions) is [not linear](https://math.stackexchange.com/a/1650713/152225). Take for example the area of a rectangle $A = s \times l$: Doubling the side of a rectangle, doubles the area, and the same happens with the length; however, the function is bilinear, such that doubling the side and the length result in an area of the rectangle $4$ times larger ([here](https://math.stackexchange.com/a/2064240/152225)). The dot product is such another example: the total cost of some purchases is the dot product of the number of units of each item purchased by the cost of each unit. A dubling of the price of each item, and a concomitant doubling of the number of units purchased of each item will increase four-fold the overall


```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The dot product fulfills the criteria for a [bi-variate function](https://proofwiki.org/wiki/Dot_Product_Operator_is_Bilinear), which amount to being linear on the left:

$$\langle au + bv, w \rangle = a \langle u, w \rangle + b \langle v, w \rangle$$

and on the right:

$$\langle u, av + bw \rangle = a \langle u, v \rangle + b \langle u, w \rangle.$$

```{r}
u = 1:3
v = 4:6
w = 7:9
a = 5
b = 8
# Linear on the left:
(a * u + b * v) %*% w
a * (u %*% w) + b * (v %*% w)
# Linear on the right
u %*% (a * v + b * w)
a * (u %*% v) + b * (u %*% w)
```

Linearity preserves scalar multiplication in both variables of a function $f: X \times Y \to \mathbb R$:


$$ f(\alpha x , \alpha y) = \alpha f (x,y) $$

However, if $f$ is bilinear

$$ f(\alpha x , \alpha y) = \alpha^2 f(x,y) $$



```{r}
(a * v) %*% (a * w)
a^2 * (v %*% w)

```



The property of a multilinear function is

$$T(v_1, . . . , av_i + bv'_i , . . . , v_r)
= aT(v_1, . . . , v_i, . . . , v_r) + bT(v_1, . . . , v'_i, . . . , v_r$$

The determinant is a prime example of multilinearity:


---

```{r}
m = matrix(c(rnorm(30)), 6, byrow=T)
# Multiplying the fourth row by 5
M =  rbind(m[c(1:3),], 5 * m[4,] + m[5,], m[6,])
det(M)
# Extracting the scalar 5 and splitting the matrix to add two det:
5 * det(m[c(1:4,6),]) + det(m[c(1:3,5:6),])
```


The wedge product is a determinant. For instance, the area subtended by the vectors $v=[2,2], w=[1,3]$ can be calculated as the determinant or, equivalently, the 2-form $\omega=1 dx \wedge dy$ acting on the vectors $v,w$, 

$$\omega(v,w)= \det\begin{bmatrix}2&2\\1&3\end{bmatrix}=4$$

which in tensor notation corresponds to $\small 1 e^1\otimes e^2 - 1e^2 \otimes e^1 + 0 e^1 \otimes e^1 + 0 e^2 \otimes e^2$, a tensor in $V^{\ast} \otimes V^{\ast}$. This is the linearization of the determinant, a multi-linear map. The [Leibniz formula](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants) for the determinant linearizes the determinant.

A bi-linear form can be expressed as a matrix $A$, and the function be interpreted as multiplying on the left by a row vector $\vec v^\top$ and on the right by a column vector $\vec w$ as in 

$$\vec v^\top A \vec w$$

as such any linear operations on a row in $A$ will necessarily alter all the columns: the bi-linearity of the function is manifest in this way. Naturally, it can still be claimed that the matrix represents a $(1,1)$ tensor, but this avoids the idea behind the tensor product: the linearization of the multi-linear function. 

The linearization breaks down a bi-linear form (matrix) into individual $e_iâŠ—e_j$ basis. In matrix form, it is impossible to change a row (dual) by linear algebra operations without affecting all the columns (hence bi-linear), but when each entry has its own tensored basis, we can dial (linear change) each one independent of the others by basic linear algebra operations - hence, it becomes linearlized.

The process of linearization includes getting the "free product" between vector spaces, and then obtaining the quotient by the bilinearity operations we want to preserve (see [here](https://youtu.be/K7f2pCQ3p3U?si=eh9H6thBrk9X4J3H)):

$$\begin{align}
(v_1 + v_2, w) - (v_1,w) - (v_2,w)\\
(v,w_1+w_2) - (v,w_1) - (v,w_2)\\
(sv,w) - s(v,w)\\
(v,sw) - s(v,w)
\end{align}$$

---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
