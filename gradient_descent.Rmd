<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###GRADIENT DESCENT NEURAL NETWORKS:
<br>

The general notation of a neural network is as follows:


<img height="700" width="900" src="https://cloud.githubusercontent.com/assets/9312897/15808527/6b48a01a-2b47-11e6-868c-a97a9b0a392b.png">

####Example with regression after [this youtube video](https://youtu.be/GlcnxUlrtek):

<img height="600" width="500" src="https://cloud.githubusercontent.com/assets/9312897/15833481/017cfe16-2bf5-11e6-9378-a770db1c9c78.png">

We need to calculate $\Large \frac {\partial J}{\partial \Theta}$. The weights $\Large \Theta$ are spread in two matrices: $\Large \Theta^{1}$ and $\Large \Theta^{2}$:


$\color{red}{\Large \Theta^{1}=\begin{bmatrix}\theta_{11}^{(1)} & \theta_{12}^{(1)} & \theta_{13}^{(1)}\\\theta_{21}^{(1)} & \theta_{22}^{(1)} & \theta_{23}^{(1)} \end{bmatrix}}$ and $\color{green}{\Large \Theta^{2}=\begin{bmatrix}\theta_{11}^{(2)} \\ \theta_{21}^{(2)} \\ \theta_{31}^{(2)} \end{bmatrix}}$

We need to calculate:

$\color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}} = \begin{bmatrix}\frac{\partial J}{\partial \theta_{11}^{(1)}} & \frac{\partial J}{\partial \theta_{12}^{(1)}} & \frac{\partial J}{\partial \theta_{13}^{(1)}}\\ \frac{\partial J}{\partial \theta_{21}^{(1)}} & \frac{\partial J}{\partial \theta_{22}^{(1)}} & \frac{\partial J}{\partial \theta_{23}^{(1)}}\end{bmatrix}}$ and $\color{green}{\Large \frac {\partial J}{\partial \Theta^{(2)}}=\begin{bmatrix}\frac{\partial J}{\partial\theta_{11}^{(2)}} \\ \frac{\partial J}{\partial\theta_{21}^{(2)}} \\ \frac{\partial J}{\partial\theta_{31}^{(2)}} \end{bmatrix}}$



The cost function adds cost from each example. In what follows we will use the cost function of a regression model as $\frac{1}{2}\sum (y-\hat y)^2$, although it would be more accurate as $\frac{1}{2m}\sum (y-\hat y)^2$.

For the case of a multi-class logistic regression NN the cost function is:

$\large J(\Theta)=-\frac{1}{m}\big[\displaystyle\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)  \big]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^l)^2$

$K$ is the number of classes, and $s_l$ is the number of units in a layer not counting the bias unit (always $1$).

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=\frac{\partial \sum \frac{1}{2}(y-\hat y)^2}{\partial \Theta^{(2)}}=\sum \frac{\partial \frac{1}{2}(y-\hat y)^2}{\partial \Theta^{(2)}}$ by he sum rule in differentiation that says that $\frac{d}{dx}(u+v)=\frac{du}{dx}+\frac{dv}{dx}$

Focusing on the expression inside the sum, and applying the chain rule, $(f\circ g)'=(f'\circ g)\cdot g'$ or $\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}$.

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=(y-\hat y)\cdot \frac{-\partial \hat y}{\partial \Theta^{(2)}}$

Since $\hat y$ is the sigmoid activation function of $z^{(3)}$, which is $g\big(z^{(3)}\big)$ we can apply the chain rule again:

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=(y-\hat y)\cdot \frac{-\partial \hat y}{\partial z^{(3)}}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=-(y-\hat y)\cdot g'(z^{(3)}) \cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}$

---

The derivative of the sigmoid activation function with respect to $z$ is:

$\Large\color{blue}{ g'(z)=\frac{e^{-z}}{(1+e^{-z})^2}=\big(\frac{1}{1+e^{-z}}\big)\times \big(1 - \frac{1}{1+e^{-z}}\big)}$ because:

$\large g(z)=\frac{u}{1+e^{-zv}}$; hence $\large g'(z)=\frac{u'v-uv'}{v^2}=\frac{0v-1(-e^{-z})}{(1+e^{-z})^2}$.

---

Now, $\Large z^{(3)}=a^{(2)}\Theta^{(2)}$, and hence $a$ is the slope of $z$ with respect to $z$ for each synapse.  Therefore $\large \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\begin{bmatrix}a_{11}^{(2)}&a_{12}^{(2)}&a_{13}^{(2)}\\a_{21}^{(2)}&a_{22}^{(2)}&a_{23}^{(2)}\\a_{31}^{(2)}&a_{32}^{(2)}&a_{33}^{(2)}\end{bmatrix}$

---

<img height="600" width="500" src="https://cloud.githubusercontent.com/assets/9312897/15807589/1fdf269c-2b30-11e6-9f97-ad17bcd8a993.png">



So we end up with:

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=\color{gray}{(y-\hat y)}\cdot \color{purple}{\frac{-\partial \hat y}{\partial z^{(3)}}}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\color{gray}{(y-\hat y)}\cdot \color{purple}{-g'(z^{(3)})}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\color{orange}{\delta^{(3)}}\frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=(a^{2})^T\,\color{orange}{\delta^{(3)}}=$

<br>

$=\large \begin{bmatrix}a_{11}^{(2)}&a_{12}^{(2)}&a_{13}^{(2)}\\a_{21}^{(2)}&a_{22}^{(2)}&a_{23}^{(2)}\\a_{31}^{(2)}&a_{32}^{(2)}&a_{33}^{(2)}\end{bmatrix} \begin{bmatrix}\delta^{(3)}_1\\\delta^{(3)}_2\\\delta^{(3)}_3\end{bmatrix}=\begin{bmatrix}a_{11}^{(2)}\delta^{(3)}_1+a_{12}^{(2)}\delta^{(3)}_2+a_{13}^{(2)}\delta^{(3)}_3\\a_{21}^{(2)}\delta^{(3)}_1+a_{22}^{(2)}\delta^{(3)}_2+a_{23}^{(2)}\delta^{(3)}_3\\a_{31}^{(2)}\delta^{(3)}_1+a_{32}^{(2)}\delta^{(3)}_2+a_{33}^{(2)}\delta^{(3)}_3\end{bmatrix}$

<br>

Then $\Large\color{orange}{\delta^{(3)}}=\color{gray}{(y-\hat y)}\cdot \color{purple}{g'(z^{(3)})}$

Therefore, the matrix multiplication also takes care of the $\sum$ part in front of the first derivative formula.

---

As for...

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=\delta^{(3)}\frac{\partial z^{(3)}}{\partial a^{(2)}}\frac{\partial a^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial \Theta^{(1)}}$

since, $\Large\frac{\partial z^{(3)}}{\partial a^{(2)}}=\Theta^{(2)}$.

Next,

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial z^{(2)}}\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T g'(z^{(2)})\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}$

Finally, 

$\Large\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}=X$. Therefore,

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=X^T \color{orange}{\delta^{(3)}} (\Theta^{(2)})^T \color{purple}{g'(z^{(2)})}=X^T \color{brown}{\delta^{(2)}}.$

---

NOTE: Andrew Ng defines the first $\delta$ as:

$\Large \delta^{(4)}=a_j^{(4)}-y_j=(h_\Theta(x))_j-y_j)$ for every $j$ node in the layer, which in the case above it is $j=1$. Vectorized:

$\Large \delta^{(4)}= a^{(4)}-y$

$\Large \delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}\,g'(z^{(3)})$

$\Large \delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}\,g'(z^{(2)})$

stopping at $\delta^{(2)}$.

So, he is effectively introducing an additional $\delta^{(4)}$, including only the "residuals". In addition, the $\Large\Theta$ factors seem to be included in the $\delta$'s, althoiugh he also defines:

$\Large\Delta^{(l)}_{ij}:=\Delta^{(l)}_{ij}+a_j^{(l)}\delta_i^{(l+1)}$

Further, he defines:

$\Large D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ if $j\neq0$.

$\Large D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}$ if $j=0$, which is the bias node.

The claim is that...

$\Large \frac{\partial J(\Theta)}{\partial \Theta_{ij}^{(l)}}=D_{ij}^{(l)}$

<br>
##CODE for the example above with just one hidden layer:

Feedforward:

```
a1 = col of 1's plus X
z2 = a1 * Theta1'
a2 = col of 1's plus h(z2)
z3 = a2 * Theta2'
a3 = h(z3)
```
Cost function:

```
J = (1 / m) * sum(sum((-Y) * log(a3) - (1 - Y) * log(1 - a3)))
```

Backpropagation:


$\Large \delta_3 = a_3 - y$

$\Large \delta_2 = \delta_3\cdot \Theta_2 \cdot \frac{e^{-(1+e^{{-\text{col 1's} + z_2}}}}{(1+e^{{-\text{col 1's} + z_2}})^2}$


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
