<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###GRADIENT DESCENT NEURAL NETWORKS:
<br>

<img height="600" width="500" src="https://cloud.githubusercontent.com/assets/9312897/15808346/c501d422-2b43-11e6-9114-d96899097833.png">

We need to calculate $\Large \frac {\partial J}{\partial \Theta}$. The weights $\Large \Theta$ are spread in two matrices: $\Large \Theta^{1}$ and $\Large \Theta^{2}$:


$\color{red}{\Large \Theta^{1}=\begin{bmatrix}\theta_{11}^{(1)} & \theta_{12}^{(1)} & \theta_{13}^{(1)}\\\theta_{21}^{(1)} & \theta_{22}^{(1)} & \theta_{23}^{(1)} \end{bmatrix}}$ and $\color{green}{\Large \Theta^{2}=\begin{bmatrix}\theta_{11}^{(2)} \\ \theta_{21}^{(2)} \\ \theta_{31}^{(2)} \end{bmatrix}}$

We need to calculate:

$\color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}} = \begin{bmatrix}\frac{\partial J}{\partial \theta_{11}^{(1)}} & \frac{\partial J}{\partial \theta_{12}^{(1)}} & \frac{\partial J}{\partial \theta_{13}^{(1)}}\\ \frac{\partial J}{\partial \theta_{21}^{(1)}} & \frac{\partial J}{\partial \theta_{22}^{(1)}} & \frac{\partial J}{\partial \theta_{23}^{(1)}}\end{bmatrix}}$ and $\color{green}{\Large \frac {\partial J}{\partial \Theta^{(2)}}=\begin{bmatrix}\frac{\partial J}{\partial\theta_{11}^{(2)}} \\ \frac{\partial J}{\partial\theta_{21}^{(2)}} \\ \frac{\partial J}{\partial\theta_{31}^{(2)}} \end{bmatrix}}$



The cost function adds cost from each example:

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=\frac{\partial \sum \frac{1}{2}(y-\hat y)^2}{\partial \Theta^{(2)}}=\sum \frac{\partial \frac{1}{2}(y-\hat y)^2}{\partial \Theta^{(2)}}$ by he sum rule in differentiation that says that $\frac{d}{dx}(u+v)=\frac{du}{dx}+\frac{dv}{dx}$

Focusing on the expression inside the sum, and applying the chain rule, $(f\circ g)'=(f'\circ g)\cdot g'$ or $\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}$.

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=(y-\hat y)\cdot \frac{-\partial \hat y}{\partial \Theta^{(2)}}$

Since $\hat y$ is the sigmoid activation function of $z^{(3)}$, which is $g\big(z^{(3)}\big)$ we can apply the chain rule again:

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=(y-\hat y)\cdot \frac{-\partial \hat y}{\partial z^{(3)}}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=-(y-\hat y)\cdot g'(z^{(3)}) \cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}$

---

The derivative of the sigmoid activation function with respect to $z$ is:

$\Large\color{blue}{ g'(z)=\frac{e^{-z}}{(1+e^{-z})^2}}$ because:

$\large g(z)=\frac{u}{1+e^{-zv}}$; hence $\large g'(z)=\frac{u'v-uv'}{v^2}=\frac{0v-1(-e^{-z})}{(1+e^{-z})^2}$.

---

Now, $\Large z^{(3)}=a^{(2)}\Theta^{(2)}$, and hence $a$ is the slope of $z$ with respect to $z$ for each synapse.  Therefore $\large \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\begin{bmatrix}a_{11}^{(2)}&a_{12}^{(2)}&a_{13}^{(2)}\\a_{21}^{(2)}&a_{22}^{(2)}&a_{23}^{(2)}\\a_{31}^{(2)}&a_{32}^{(2)}&a_{33}^{(2)}\end{bmatrix}$

---

<img height="600" width="500" src="https://cloud.githubusercontent.com/assets/9312897/15807589/1fdf269c-2b30-11e6-9f97-ad17bcd8a993.png">



So we end up with:

$\Large \color{green}{\frac{\partial J}{\partial \Theta^{(2)}}}=\color{gray}{(y-\hat y)}\cdot \color{purple}{\frac{-\partial \hat y}{\partial z^{(3)}}}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\color{gray}{(y-\hat y)}\cdot \color{purple}{-g'(z^{(3)})}\cdot \frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=\color{orange}{\delta^{(3)}}\frac{\partial z^{(3)}}{\partial \Theta^{(2)}}=(a^{2})^T\,\color{orange}{\delta^{(3)}}=$

<br>

$=\large \begin{bmatrix}a_{11}^{(2)}&a_{12}^{(2)}&a_{13}^{(2)}\\a_{21}^{(2)}&a_{22}^{(2)}&a_{23}^{(2)}\\a_{31}^{(2)}&a_{32}^{(2)}&a_{33}^{(2)}\end{bmatrix} \begin{bmatrix}\delta^{(3)}_1\\\delta^{(3)}_2\\\delta^{(3)}_3\end{bmatrix}=\begin{bmatrix}a_{11}^{(2)}\delta^{(3)}_1+a_{12}^{(2)}\delta^{(3)}_2+a_{13}^{(2)}\delta^{(3)}_3\\a_{21}^{(2)}\delta^{(3)}_1+a_{22}^{(2)}\delta^{(3)}_2+a_{23}^{(2)}\delta^{(3)}_3\\a_{31}^{(2)}\delta^{(3)}_1+a_{32}^{(2)}\delta^{(3)}_2+a_{33}^{(2)}\delta^{(3)}_3\end{bmatrix}$

<br>

Therefore, the matrix multiplication also takes care of the $\sum$ part in front of the first derivative formula.

---

As for...

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=\delta^{(3)}\frac{\partial z^{(3)}}{\partial a^{(2)}}\frac{\partial a^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial \Theta^{(1)}}$

since, $\Large\frac{\partial z^{(3)}}{\partial a^{(2)}}=\Theta^{(2)}$.

Next,

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T \frac{\partial a^{(2)}}{\partial z^{(2)}}\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}=\delta^{(3)} (\Theta^{(2)})^T g'(z^{(2)})\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}$

Finally, 

$\Large\frac{\partial z^{(2)}}{\partial \Theta^{(1)}}=X$. Therefore,

$\Large \color{red}{\Large \frac{\partial J}{\partial \Theta^{(1)}}}=X^T \color{orange}{\delta^{(3)}} (\Theta^{(2)})^T \color{purple}{g'(z^{(2)})}=X^T \color{yellow}{\delta^{(2)}}.$

<br>

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
