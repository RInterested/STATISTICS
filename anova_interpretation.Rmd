<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###INTERPRETATION OF THE RESULTS OF AN ANOVA TEST IN R:
<br>

From [Statistics Make Me Cry](http://www.statsmakemecry.com/smmctheblog/stats-soup-anova-ancova-manova-mancova):

An "Analysis of Variance" (ANOVA) tests three or more groups for mean differences based on a continuous (i.e. scale or interval) response variable (a.k.a. dependent variable). The term "factor" refers to the variable that distinguishes this group membership. Race, level of education, and treatment condition are examples of factors.

There are two main types of ANOVA: (1) "one-way" ANOVA compares levels (i.e. groups) of a single factor based on single continuous response variable (e.g. comparing test score by 'level of education') and (2) a "two-way" ANOVA compares levels of two or more factors for mean differences on a single continuous response variable(e.g. comparing test score by both 'level of education' and 'zodiac sign'). In practice, you will see one-way ANOVAs more often and when the term ANOVA is generically used, it often refers to a one-way ANOVA. Henceforth in this blog entry, I use the term ANOVA to refer to the one-way flavor.

The essential computation is the F-test:

$$F=\frac{\text{variance between treatments}}{\text{variance within treatments}}=\large\frac{\frac{\text{Sum Sqs}_{\text{treat'ts}}}{\text{no. treat'ts}-1}}{\frac{\text{Sum Sqs}_{\text{err's}}}{\text{no. cases}-\text{no. treat'ts}}}$$,

Let's create three normally distributed samples with the same variance, but different means, and run an anova in R:

```{r}
set.seed(1); n = 50; x1 = rnorm(n, 10, 3); x2 = rnorm(n, 15, 3); x3 = rnorm(n, 20, 3)
dataframe = data.frame(x1,x2,x3)
boxplot(dataframe, col=c("wheat4", "tan", "tan3"))
```

Now let's calculate the anova manually:

```{r}
datamatrix = as.matrix(dataframe)
(global.mean = mean(datamatrix))                                    # The overall mean
(group.means = apply(datamatrix, 2, mean))                          # The respective means for each group
(SST = sum((datamatrix - global.mean)^2))                           # Sum squares total
(SSBetween = sum(nrow(datamatrix) * (group.means-global.mean)^2))   # Between groups
(SSWithin = sum(scale(datamatrix, center = T, scale = F)^2))        # Sum Squares Residuals
(MeanSqBetween = SSBetween / (ncol(datamatrix) - 1))                # Between/df(groups - 1) [3 - 1]
(MeanSqWithin = SSWithin / (length(datamatrix) - ncol(datamatrix))) # Within/df(subjects - no.groups) [150-3]
(F_value = MeanSqBetween / MeanSqWithin)                            # F value
```

Compare to the R function. First let's ready up the data:

```{r}
datalong = stack(dataframe)
fit = aov(values ~ ind, datalong); summary(fit)
```

---

####PAIRWISE COMPARISONS:

Additional analyses are required after an omnibus test. The `function pairwise.t.test()` computes the pair-wise comparisons between group means with corrections for multiple testing: 

`pairwise.t.test(reponse, factor, p.adjust = method, alternative = c("two.sided",
"less", "greater"))`

**Response** is a vector of observations (the response variable), **factor** a list of
factors and **p.adjust** is the correction method (e.g., “Bonferroni”).

```{r}
options(scipen=999)
pairwise.t.test(datalong$values, datalong$ind, p.adjust="bonferroni")

```

All the pairwise comparisons are statistically significant.

Alternatively, a Tukey test can be undertaken:

```{r}
TukeyHSD(fit)
```

The differences pairwise and the p-values are shown.

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
