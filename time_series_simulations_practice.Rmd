---
title: 'Simulating Time Series ACF PACF'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Time Series Simulations and practice:

---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The workhorse function to simulate time series is `arima.sim()` (see [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.sim.html)). However, the application of a convolutional filter precludes a straightforward generation of time series with a given mean value $\mu.$ 

```
`mean` **affects** the intercept, but it is not the intercept:
> x <- arima.sim(model = list(order = c(0,0,1), ma=0.9), n = 10000, mean = -15)
> y <- arima.sim(model = list(ma=0.9), n = 10000, mean = -15)
> mean(x)
[1] -28.48915
> mean(y)
[1] -28.47643

> See the discussion of why the mean is not, as could be expected, equal to $\mu$ in MA in [here](https://stats.stackexchange.com/q/294748/67822).

`sd` is the sd of the innovations generated by rnorm()
```

For instance in `arima.sim(model = list(order = c(1,0,0), ar=0.9), n = 100)` (same as `arima.sim(model = list(order = c(1,0,0), ar=0.9), n = 100)`), the simulation is for an AR(1) process with coefficient $0.9.$

The function from `{forecast}` to fit an already existing data set, or in the cases below, to recover the parameters of the simulation is `Arima()`. To use it, it is important to remember the order:

```
`order = c(p,d,q)`:
`p` is the order (number of time lags) of the autoregressive model
`d` is the degree of differencing (the number of times the data have had past values subtracted)
`q` is the order of the moving-average model.
```


### WHITE NOISE:

White noise is stationary. From [here](https://rinterested.github.io/statistics/time_series.html):

$$Y_t = \varepsilon_t$$ with $\epsilon_i \sim N(0,\sigma^2)$. This is a stationary process.


```{r, fig.width=10, fig.height=4}
library(cowplot) 
library(fpp2)  

set.seed(1)
n = 500
a = rnorm(100)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF WHITE NOISE")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF WHITE NOISE")
```

---

### RANDOM WALK:

##### RANDOM WALK WITH NO DRIFT:

From [here](https://youtu.be/ouahL4HbwBE?si=Qin9EGQjfubpA5z2).

This is the same as an AR(1) process with $\rho = 1.$ The $\rho = 1$ turns the equation for the AR process $Y_t = \rho \, Y_{t-1} + \epsilon_t$ into the equation below.

$$\large Y_t = Y_{t-1}+\epsilon_t=\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_1$$

A random walk is not stationary because the variance is time-dependent: $$\text{var}[y_t] = \text{var}[y_o]+\sum_1^t \text{var}[\epsilon_i]= t\, \text{var}[\epsilon].$$

However, the expectation is zero: $\mathbb E[Y_t]=0.$

To make the process stationary, the time series has to undergo differencing. The resultant times series after differencing will be white noise.


```{r, fig.width=10, fig.height=4}
set.seed(1)
n = 500
a = ts(cumsum(rnorm(n)))
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF RANDOM WALK NO DRIFT")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF RANDOM WALK NO DRIFT")

da <- diff(a, differences = 1)
r1d <- ggAcf(da, lag.max = 50) + labs(title = "ACF RANDOM WALK - DIFFERENCING")
r2d <- ggPacf(da, lag.max = 50) + labs(title = "PACF RANDOM WALK - DIFFERENCING")
plot_grid(r1, r2, r1d, r2d, ncol = 2)
```

1. The first $50$ lags yield statistically significant correlations. See [here](https://towardsdatascience.com/how-to-detect-random-walk-and-white-noise-in-time-series-forecasting-bdb5bbd4ef81).

2. From [here](https://www.reddit.com/r/AskStatistics/comments/7i1vuq/comment/dqwie9l/?utm_source=share&utm_medium=web2x&context=3). After differencing, the ACF and PACF are very similar. They are white noise.

> The ACF at lag $k$ is equivalent to the correlation between values $k$-lag apart (i.e. $x_{t+k} and $x_t$); the PACF is the same, except you first subtract out the projection of each of those onto the space spanned by $x_{t - 1}, \dots,  x_{t-(k+1)}$. This projection has an expected value of zero with white noise data, so the theoretical relationship between ACF and PACF in that case is equivalence. However, real (or simulated) data is a bit messy, and the estimated ACF/PACF values are not actually zero. But with larger sample size, the magnitude of the projection drops to zero faster than the ACF does, so even though the estimated ACF and PACF go toward zero, the PACF more quickly approximates the ACF. 
>
> What approximately equal ACF and PACF can tell you is that the data appear to be white noise. You can also infer this from the largely non-significant values; the few that are out of bounds are the product of randomness.


---

##### RANDOM WALK WITH DRIFT:

From [here](https://www.reddit.com/r/AskStatistics/comments/7i1vuq/comment/dqw8khg/?utm_source=share&utm_medium=web2x&context=3):

> A random walk (with drift) in the logs (geometric Brownian motion) is not a perfect description of stock prices, for a variety of reasons -- but it's not a terrible first approximation, and it's the basis of the Black Scholes formula

From [here](https://youtu.be/Nxcqi7UZenc?si=YcHJsT2NrzCRxLOB):

$$\large Y_t = \alpha + Y_{t-1}+\epsilon_t$$

where $\epsilon_t \sim N(0, \sigma^2).$

The $\mathbb E[Y_t] = \alpha \, t$ and the $\text{var}(Y_t) = t \, \sigma^2$

$\large y_t = y_{t-1}+\epsilon_t=\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_1$

From [here](https://stackoverflow.com/a/24274349/4089351): The resulting simulation will:

> a realization of a random walk with variance `t*variance` and mean `t*drift`, where `t` is the index

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 500
drift = 2
variance = 1
a = cumsum(rnorm(n, mean=drift, sd=sqrt(variance)))
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF RANDOM WALK WITH DRIFT")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF RANDOM WALK NO DRIFT")

da <- diff(a, differences = 1)
r1d <- ggAcf(da, lag.max = 50) + labs(title = "ACF RANDOM WALK DRIFT - DIFFERENCING")
r2d <- ggPacf(da, lag.max = 50) + labs(title = "PACF RANDOM WALK DRIFT - DIFFERENCING")
plot_grid(r1, r2, r1d, r2d, ncol = 2)
```

After differencing it also returns white noise. See [here](https://towardsdatascience.com/how-to-detect-random-walk-and-white-noise-in-time-series-forecasting-bdb5bbd4ef81).

---

##### AUTOREGRESSIVE PROCESS (AR):

From [here](https://en.wikipedia.org/wiki/Autoregressive_model) and [here](https://youtu.be/AN0a58F6cxA?si=n0HMEy1OWdlZxoLu):


For AR(1) processes the equation is:

$$Y_t = \rho  Y_{t-1}+\epsilon_t$$

In general, for AR(p):

$$\large Y_t = \sum_{i=1}^p \rho_i\, Y_{t - i} + \epsilon_t$$

Notice that, differently from MA models (see below) the mean is not specified, and assumed to be zero. If it is not zero (see [here](https://youtu.be/S9RJjKiKxeM?si=IXGvHrFGVGYWoY9q)), under the assumption of stationarity of the time series, the mean has to be subtracted on both sides of the equation and from each term:

$$\large Y_t - \mu= \sum_{i=1}^p \rho_i\, \left(Y_{t - i}-\mu\right) + \epsilon_t$$

See also [this answer](https://stats.stackexchange.com/a/386349/67822). For instance in AR(2), subtracting the mean from both sides:

$$Y_t - \mu = \rho_1(Y_{t-1} - \mu) + \rho_2(Y_{t-2} - \mu) +\epsilon_t$$

Now moving $\mu$ to the RHS:

$$Y_t = \mu -\rho_1\,\mu - \rho_2 \,\mu +\rho_1Y_{t-1}+ \rho_2 Y_{t-2}+ \epsilon_t$$
and grouping $c = \mu(1 - \rho_1 - \rho_2)$

$$Y_t = c + \rho_1Y_{t-1}+ \rho_2 Y_{t-2}+ \epsilon_t$$



From [here](https://youtu.be/L2b2dCO_-PA?si=N1G6gOlkYd3RjCiI) for AR(1),

$$\mathbb E[Y_t]=0$$

and from [here](https://youtu.be/9mXV3mjPmw8?si=KsDpAj7pWW7DY-Yl):

$$\text{Var}(Y_t)= \frac{\sigma^2}{1-\rho^2}$$

From [here](https://rinterested.github.io/statistics/time_series.html) for AR(1):

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
a = arima.sim(n=n,list(ar=c(0.15)), innov=rnorm(n))
Arima(a, order = c(1,0,0), include.mean = T)
r1 <- ggAcf(a, lag.max = 100) + labs(title = "ACF AR(1)")
r2 <- ggPacf(a, lag.max = 100) + labs(title = "PACF AR(1)")
plot_grid(r1, r2, ncol = 2)
```

An example with positive rho values with `arima` from [here](https://edoras.sdsu.edu/~babailey/stat673/lab1.html)

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
a <- arima.sim(n=n, model=list(ar=c(0.1,0.8)))
Arima(a, order = c(2,0,0), include.mean = T)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(2)")
plot_grid(r1, r2, ncol = 2)
```

another example:

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
a <- arima.sim(model = list(order = c(1,0,0), ar=0.9), n = n)
Arima(a, order = c(1,0,0), include.mean = T)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(2)")
plot_grid(r1, r2, ncol = 2)
```


1. The ACF shows geometric decay.
2. The PACF shows significant autocorrelations for a number of lags equal to the order.

From [here](https://stackoverflow.com/a/67371090/4089351).

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
AR2.sim <- function(n, ar1, ar2, sd)
{
  Xt = c(0,0)
  for (i in 2:n)
  {
    Xt[i+1] = ar1*Xt[i] + ar2*Xt[i-1] + rnorm(1, mean=0,sd=sd)
  }
  Xt
}
a = AR2.sim(n, -0.5, 0.3, 1)
Arima(a, order = c(2,0,0), include.mean = T)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(2)")
plot_grid(r1, r2, ncol = 2)
```

and a second example:

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
a <- arima.sim(n=n, model=list(ar=c(-0.5,0.3)))
Arima(a, order = c(2,0,0), include.mean = T)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(2)")
plot_grid(r1, r2, ncol = 2)
```

##### Simulating an AR(1) process with $\mu \neq 0:$

Based on [this answer](https://stats.stackexchange.com/a/305239/67822), if instead of a mean we want an intercept as in the model:

$$X_{t} = 5 + 0.5X_{t-1}+\epsilon_t$$

the mean $\mu$ will have to actually be $\mu = 10,$ because, according to the derivation above upon subtracting the mean on both sides from $X_t$ and $X_{t-1},$ we are going to need $\rho=0.5$ times $mu$ to equal the intercept $5$. Therefore, $\text{interc} = \rho\times \mu \implies \mu=\text{interc}/\rho$ so that the expression above is the same as

$$X_t - 10 = 0.5(X_{t-1} - 10) + \epsilon_t .$$
Now, defining $Y_t = X_t - 10,$

$$Y_t = .5 Y_{t-1} + \epsilon_t$$

and we just add $10$ at the end:

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 10^4
yt <- arima.sim(list(order=c(1,0,0), ar=.5), n=n)
a <- yt + 10 
Arima(a, order = c(1,0,0), include.mean = T)

r1 <- ggAcf(yt, lag.max = 50) + labs(title = "ACF AR(1)")
r2 <- ggPacf(yt, lag.max = 50) + labs(title = "PACF AR(1)")
plot_grid(r1, r2, ncol = 2)

r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(1)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(1)")
plot_grid(r1, r2, ncol = 2)
```


However, using the code in [here](https://people.stat.sc.edu/wang528/Stat%20720/STAT720%20Notes.pdf) and [here](https://stackoverflow.com/a/41089875/4089351) it is much easier to avoid altogether the convolution filter, and code as a recurrent filter the generation of the time series as in:

```
# Toy example:
> x = rnorm(10)
> theta1 = 0.4
> theta2 = 0.7
> fil = c(theta1, theta2)
> filter(x, "recursive", filter=fil)
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1] -0.5719836  0.7566848 -0.2348018  1.6904645  0.1553653  2.0795214  2.6251119  2.9932320
 [9]  3.5073989  3.9858098
> (t1 = x[1])
[1] -0.5719836
> (t2 = t1 * theta1 + x[2])
[1] 0.7566848
> (t3 = t1 * theta2 + t2 * theta1 + x[3])
[1] -0.2348018
> (t4 = t2 * theta2 + t3 * theta1 + x[4])
[1] 1.690465
```

```{r, fig.width=10, fig.height=4}
set.seed(2024)
library(forecast)
n <- 10^4
theta1 <- 0.78
theta2 <- 0.1
mu <- 70 # Desired mean
sd <- 1 # Desired SD
a <- mu + filter(x = c(0, rnorm(n, 0, 1)), filter = c(theta1,theta2), method = "recursive")[-1]
# Recovering the initial parameters:
Arima(a, order = c(2,0,0), include.mean = T)

r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF AR(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF AR(2)")
plot_grid(r1, r2, ncol = 2)
```




---

##### MOVING AVERAGE (MA):

From [here](https://en.wikipedia.org/wiki/Moving-average_model):


For MR(q):

$$\large Y_t =\mu+ \sum_{i=1}^q \theta_i\, \epsilon_{t - 1} + \varepsilon_t$$

where $\mu$ is the mean of the series.

For MA(1) models, the $$\mathbb E[Y_t]=0$$

and $$\text{Var}(Y_t)=\sigma^2\left( 1 + \theta^2\right)$$

From [here](https://rinterested.github.io/statistics/time_series.html) for MA(1):

```{r, fig.width=10, fig.height=4}
set.seed(2024)
n = 1000
a <- arima.sim(model=list(ma=0.9), n=n)
Arima(a, order = c(0,0,1), include.mean = T)
r1 <- ggAcf(a, lag.max = 100) + labs(title = "ACF MA(1)")
r2 <- ggPacf(a, lag.max = 100) + labs(title = "PACF MA(1)")
plot_grid(r1, r2, ncol = 2)
```


1. Significant lags equal to the order on ACF.
2. Geometric decay of PACF.

The characteristic MA pattern on the PACF is alternating and diminishing over time.

As with AR() processes, applying `filter()` can generate the time series without need for `arima.sim()` and with the advantage of being able to plug in a mean. The code is from page 23 of [this document](https://people.stat.sc.edu/wang528/Stat%20720/STAT720%20Notes.pdf).

Here is how it works:

```
# Toy example:
> x = rnorm(10)
> phi1 = 0.4
> phi2 = 0.7
> fil = c(phi1, phi2)
> filter(x, method='convolution', side=1, filter=fil)
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1]         NA  0.4732177  1.1772258 -0.2822853  0.3438599  0.4165922  0.7885656  1.1353353
 [9]  0.4015954 -1.0864283
> phi2 * x[1] + phi1 * x[2]
[1] 0.4732177
> phi2 * x[2] + phi1 * x[3]
[1] 1.177226
```

```{r, fig.width=10, fig.height=4}
# Moving Average Model MA(2)
n <- 10^4
phi1 <- 0.78
phi2 <- 0.1
mu <- 70
sd <- 1
a <- mu + filter(rnorm(n, 0, 1), method="convolution", sides=1, c(1,phi1, phi2))[-(1:50)]
Arima(a, order = c(0,0,2), include.mean = T)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF MA(2)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF MA(2)")
plot_grid(r1, r2, ncol = 2)
```

### ARMA:

From [here](https://rpubs.com/odenipinedo/ARIMA-Models-in-R#:~:text=Simulating%20ARMA%20models,data%20from%20an%20ARMA%20model.):

```{R}
n <- 10^4
a <- arima.sim(model = list(order = c(2, 0, 1), ar = c(1, -.9), ma = .8), n = n)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF ARMA(2,1)")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF ARMA(2,1)")
plot_grid(r1, r2, ncol = 2)
Arima(a, order = c(2,0,1), include.mean = T)
```

#### ARMA with SEASONALITY:

Find the explanation in [here](https://online.stat.psu.edu/stat510/lesson/4/4.1). The code is extracted from [here](https://robjhyndman.com/hyndsight/simulating-from-a-specified-seasonal-arima-model/index.html)

> The seasonal ARIMA model incorporates both non-seasonal and seasonal factors in a multiplicative model. One shorthand notation for the model is
>
$$(p, d, q) \times (P, D, Q)S$$
>
> with p = non-seasonal AR order, d = non-seasonal differencing, q = non-seasonal MA order, P = seasonal AR order, D = seasonal differencing, Q = seasonal MA order, and S = time span of repeating seasonal pattern. Without differencing operations, the model could be written more formally as
>
> $$\Phi(B^S)\phi(B)(x_t-\mu)=\Theta(B^S)\theta(B)w_t$$



```{R}
library(forecast)
model <- Arima(ts(rnorm(100),freq=12), 
           order=c(1,1,1), 
           seasonal=c(1,1,1),
           fixed=c(phi=0.5, theta=-0.4, Phi=0.3, Theta=-0.2))
model
a <- simulate(model, nsim=1000)
autoplot(a)
r1 <- ggAcf(a, lag.max = 50) + labs(title = "ACF ARMA(2,0,1) x (1, 1, 1) 12")
r2 <- ggPacf(a, lag.max = 50) + labs(title = "PACF ARMA(2,0,1) x (1, 1, 1) 12")
plot_grid(r1, r2, ncol = 2)
```

---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
