---
title: 'Simulating Time Series ACF PACF'
output: 
  html_document:
    theme: readable
    includes:
      in_header: "favicon.html" 
    css: custom.css
---

### **NOTES ON STATISTICS, PROBABILITY and MATHEMATICS**

<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="40" WIDTH="50" src="logo.PNG"></a>

---

### Time Series Simulations and practice:

---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The workhorse function to simulate time series is `arima.sim()` (see [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.sim.html)). However, the application of a convolutional filter precludes a straightforward generation of time series with a given mean value $\mu.$ 

```
`mean` **affects** the intercept, but it is not the intercept:
> x <- arima.sim(model = list(order = c(0,0,1), ma=0.9), n = 10000, mean = -15)
> y <- arima.sim(model = list(ma=0.9), n = 10000, mean = -15)
> mean(x)
[1] -28.48915
> mean(y)
[1] -28.47643

> See the discussion of why the mean is not, as could be expected, equal to $\mu$ in MA in [here](https://stats.stackexchange.com/q/294748/67822).

`sd` is the sd of the innovations generated by rnorm()
```

For instance in `arima.sim(model = list(order = c(1,0,0), ar=0.9), n = 100)` (same as `arima.sim(model = list(order = c(1,0,0), ar=0.9), n = 100)`), the simulation is for an AR(1) process with coefficient $0.9.$

The function from `{forecast}` to fit an already existing data set, or in the cases below, to recover the parameters of the simulation is `Arima()`. To use it, it is important to remember the order:

```
`order = c(p,d,q)`:
`p` is the order (number of time lags) of the autoregressive model
`d` is the degree of differencing (the number of times the data have had past values subtracted)
`q` is the order of the moving-average model.
```


### WHITE NOISE:

White noise is stationary. From [here](https://rinterested.github.io/statistics/time_series.html):

$$Y_t = \varepsilon_t$$ with $\epsilon_i \sim N(0,\sigma^2)$. This is a stationary process.


```{r, fig.width=15, fig.height=8}
library(cowplot) 
library(fpp2)  
library(forecast)
library(patchwork)
library(ggplot2)

set.seed(1)
n = 500
y =ts(rnorm(100))
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF WHITE NOISE")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF WHITE NOISE")
p1 / ( p2 | p3)
```

---

### RANDOM WALK:

##### RANDOM WALK WITH NO DRIFT:

From [here](https://youtu.be/ouahL4HbwBE?si=Qin9EGQjfubpA5z2).

This is the same as an AR(1) process with $\rho = 1.$ The $\rho = 1$ turns the equation for the AR process $Y_t = \rho \, Y_{t-1} + \epsilon_t$ into the equation below.

$$\large Y_t = Y_{t-1}+\epsilon_t=\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_1$$

A random walk is not stationary because the variance is time-dependent: $$\text{var}[y_t] = \text{var}[y_o]+\sum_1^t \text{var}[\epsilon_i]= t\, \text{var}[\epsilon].$$

However, the expectation is zero: $\mathbb E[Y_t]=0.$

To make the process stationary, the time series has to undergo differencing. The resultant times series after differencing will be white noise.


```{r, fig.width=15, fig.height=8}
set.seed(1)
n = 500
y = ts(cumsum(rnorm(n)))

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF RANDOM WALK NO DRIFT")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF RANDOM WALK NO DRIFT")
p1 / ( p2 | p3)

dy <- diff(y, differences = 1)

p1 = autoplot(dy) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(dy, lag.max=15) + ggtitle("ACF RANDOM WALK - DIFFERENCING")
p3 = ggPacf(dy, lag.max=15) + ggtitle("PACF RANDOM WALK - DIFFERENCING")
p1 / ( p2 | p3)
```

1. The first $50$ lags yield statistically significant correlations. See [here](https://towardsdatascience.com/how-to-detect-random-walk-and-white-noise-in-time-series-forecasting-bdb5bbd4ef81).

2. From [here](https://www.reddit.com/r/AskStatistics/comments/7i1vuq/comment/dqwie9l/?utm_source=share&utm_medium=web2x&context=3). After differencing, the ACF and PACF are very similar. They are white noise.

> The ACF at lag $k$ is equivalent to the correlation between values $k$-lag apart (i.e. $x_{t+k} and $x_t$); the PACF is the same, except you first subtract out the projection of each of those onto the space spanned by $x_{t - 1}, \dots,  x_{t-(k+1)}$. This projection has an expected value of zero with white noise data, so the theoretical relationship between ACF and PACF in that case is equivalence. However, real (or simulated) data is a bit messy, and the estimated ACF/PACF values are not actually zero. But with larger sample size, the magnitude of the projection drops to zero faster than the ACF does, so even though the estimated ACF and PACF go toward zero, the PACF more quickly approximates the ACF. 
>
> What approximately equal ACF and PACF can tell you is that the data appear to be white noise. You can also infer this from the largely non-significant values; the few that are out of bounds are the product of randomness.


---

##### RANDOM WALK WITH DRIFT:

From [here](https://www.reddit.com/r/AskStatistics/comments/7i1vuq/comment/dqw8khg/?utm_source=share&utm_medium=web2x&context=3):

> A random walk (with drift) in the logs (geometric Brownian motion) is not a perfect description of stock prices, for a variety of reasons -- but it's not a terrible first approximation, and it's the basis of the Black Scholes formula

From [here](https://youtu.be/Nxcqi7UZenc?si=YcHJsT2NrzCRxLOB):

$$\large Y_t = \alpha + Y_{t-1}+\epsilon_t$$

where $\epsilon_t \sim N(0, \sigma^2).$

The $\mathbb E[Y_t] = \alpha \, t$ and the $\text{var}(Y_t) = t \, \sigma^2$

$\large y_t = y_{t-1}+\epsilon_t=\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_1$

From [here](https://stackoverflow.com/a/24274349/4089351): The resulting simulation will:

> a realization of a random walk with variance `t*variance` and mean `t*drift`, where `t` is the index

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 500
drift = 2
variance = 1
y = ts(cumsum(rnorm(n, mean=drift, sd=sqrt(variance))))

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF RANDOM WALK WITH DRIFT")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF RANDOM WALK WITH DRIFT")
p1 / ( p2 | p3)

dy <- diff(y, differences = 1)

p1 = autoplot(dy) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(dy, lag.max=15) + ggtitle("ACF RANDOM WALK WITH DRIFT - differencing")
p3 = ggPacf(dy, lag.max=15) + ggtitle("PACF RANDOM WALK WITH DRIFT - differencing")
p1 / ( p2 | p3)
```

After differencing it also returns white noise. See [here](https://towardsdatascience.com/how-to-detect-random-walk-and-white-noise-in-time-series-forecasting-bdb5bbd4ef81).

---

#### AUTOREGRESSIVE PROCESS (AR):

From [here](https://en.wikipedia.org/wiki/Autoregressive_model) and [here](https://youtu.be/AN0a58F6cxA?si=n0HMEy1OWdlZxoLu):


For AR(1) processes the equation is:

$$Y_t = \rho  Y_{t-1}+\epsilon_t$$

In general, for AR(p):

$$\large Y_t = \sum_{i=1}^p \rho_i\, Y_{t - i} + \epsilon_t$$

Notice that, differently from MA models (see below) the mean is not specified, and assumed to be zero. If it is not zero (see [here](https://youtu.be/S9RJjKiKxeM?si=IXGvHrFGVGYWoY9q)), under the assumption of stationarity of the time series, the mean has to be subtracted on both sides of the equation and from each term:

$$\large Y_t - \mu= \sum_{i=1}^p \rho_i\, \left(Y_{t - i}-\mu\right) + \epsilon_t$$

See also [this answer](https://stats.stackexchange.com/a/386349/67822). For instance in AR(2), subtracting the mean from both sides:

$$Y_t - \mu = \rho_1(Y_{t-1} - \mu) + \rho_2(Y_{t-2} - \mu) +\epsilon_t$$

Now moving $\mu$ to the RHS:

$$Y_t = \mu -\rho_1\,\mu - \rho_2 \,\mu +\rho_1Y_{t-1}+ \rho_2 Y_{t-2}+ \epsilon_t$$
and grouping $c = \mu(1 - \rho_1 - \rho_2)$

$$Y_t = c + \rho_1Y_{t-1}+ \rho_2 Y_{t-2}+ \epsilon_t$$



From [here](https://youtu.be/L2b2dCO_-PA?si=N1G6gOlkYd3RjCiI) for AR(1),

$$\mathbb E[Y_t]=0$$

and from [here](https://youtu.be/9mXV3mjPmw8?si=KsDpAj7pWW7DY-Yl):

$$\text{Var}(Y_t)= \frac{\sigma^2}{1-\rho^2}$$

From [here](https://rinterested.github.io/statistics/time_series.html) for AR(1):

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
y = arima.sim(n=n,list(ar=c(0.15)), innov=rnorm(n))
Arima(y, order = c(1,0,0), include.mean = T)

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(1)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(1)")
p1 / ( p2 | p3)
```

An example with positive rho values with `arima` from [here](https://edoras.sdsu.edu/~babailey/stat673/lab1.html)

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
y <- arima.sim(n=n, model=list(ar=c(0.1,0.8)))
Arima(y, order = c(2,0,0), include.mean = T)

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(2)")
p1 / ( p2 | p3)
```

another example:

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
y <- arima.sim(model = list(order = c(1,0,0), ar=0.9), n = n)
Arima(y, order = c(1,0,0), include.mean = T)

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(2)")
p1 / ( p2 | p3)
```


1. **The ACF shows geometric decay.**
2. **The last significant lag in the PACF indicates the order of the AR model.**
3. **A sinusoidal decay indicates either a seasonal or cyclic component.**

From [here](https://stackoverflow.com/a/67371090/4089351).

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
AR2.sim <- function(n, ar1, ar2, sd)
{
  Xt = c(0,0)
  for (i in 2:n)
  {
    Xt[i+1] = ar1*Xt[i] + ar2*Xt[i-1] + rnorm(1, mean=0,sd=sd)
  }
  Xt
}
y = ts(AR2.sim(n, -0.5, 0.3, 1))
Arima(y, order = c(2,0,0), include.mean = T)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(2)")
p1 / ( p2 | p3)
```

and a second example:

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
y <- arima.sim(n=n, model=list(ar=c(-0.5,0.3)))
Arima(y, order = c(2,0,0), include.mean = T)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(2)")
p1 / ( p2 | p3)
```

##### Simulating an AR(1) process with $\mu \neq 0:$

Based on [this answer](https://stats.stackexchange.com/a/305239/67822), if instead of a mean we want an intercept as in the model:

$$X_{t} = 5 + 0.5X_{t-1}+\epsilon_t$$

the mean $\mu$ will have to actually be $\mu = 10,$ because, according to the derivation above upon subtracting the mean on both sides from $X_t$ and $X_{t-1},$ we are going to need $\rho=0.5$ times $mu$ to equal the intercept $5$. Therefore, $\text{interc} = \rho\times \mu \implies \mu=\text{interc}/\rho$ so that the expression above is the same as

$$X_t - 10 = 0.5(X_{t-1} - 10) + \epsilon_t .$$
Now, defining $Y_t = X_t - 10,$

$$Y_t = .5 Y_{t-1} + \epsilon_t$$

and we just add $10$ at the end:

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 10^4
yt <- arima.sim(list(order=c(1,0,0), ar=.5), n=n)
y <- yt + 10 
Arima(y, order = c(1,0,0), include.mean = T)

p1 = autoplot(yt) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(yt, lag.max=15) + ggtitle("ACF AR(1)")
p3 = ggPacf(yt, lag.max=15) + ggtitle("PACF AR(1)")
p1 / ( p2 | p3)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(1)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(1)")
p1 / ( p2 | p3)
```


However, using the code in [here](https://people.stat.sc.edu/wang528/Stat%20720/STAT720%20Notes.pdf) and [here](https://stackoverflow.com/a/41089875/4089351) it is much easier to avoid altogether the convolution filter, and code as a recurrent filter the generation of the time series as in:

```
# Toy example:
> x = rnorm(10)
> theta1 = 0.4
> theta2 = 0.7
> fil = c(theta1, theta2)
> filter(x, "recursive", filter=fil)
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1] -0.5719836  0.7566848 -0.2348018  1.6904645  0.1553653  2.0795214  2.6251119  2.9932320
 [9]  3.5073989  3.9858098
> (t1 = x[1])
[1] -0.5719836
> (t2 = t1 * theta1 + x[2])
[1] 0.7566848
> (t3 = t1 * theta2 + t2 * theta1 + x[3])
[1] -0.2348018
> (t4 = t2 * theta2 + t3 * theta1 + x[4])
[1] 1.690465
```

```{r, fig.width=15, fig.height=8}
set.seed(2024)
library(forecast)
n <- 10^4
theta1 <- 0.78
theta2 <- 0.1
mu <- 70 # Desired mean
sd <- 1 # Desired SD
y <- ts(mu + filter(x = c(0, rnorm(n, 0, 1)), filter = c(theta1,theta2), 
                    method = "recursive")[-1])
# Recovering the initial parameters:
Arima(y, order = c(2,0,0), include.mean = T)

p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF AR(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF AR(2)")
p1 / ( p2 | p3)
```


---

##### MOVING AVERAGE (MA):

From [here](https://en.wikipedia.org/wiki/Moving-average_model):


For MR(q):

$$\large Y_t =\mu+ \sum_{i=1}^q \theta_i\, \epsilon_{t - 1} + \varepsilon_t$$

where $\mu$ is the mean of the series.

For MA(1) models, the $$\mathbb E[Y_t]=0$$

and $$\text{Var}(Y_t)=\sigma^2\left( 1 + \theta^2\right)$$

From [here](https://rinterested.github.io/statistics/time_series.html) for MA(1):

```{r, fig.width=15, fig.height=8}
set.seed(2024)
n = 1000
y <- arima.sim(model=list(ma=0.9), n=n)
Arima(y, order = c(0,0,1), include.mean = T)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF MA(1)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF MA(1)")
p1 / ( p2 | p3)
```


1. Significant lags equal to the order on ACF.
2. Geometric decay of PACF.

The characteristic MA pattern on the PACF is alternating and diminishing over time.

As with AR() processes, applying `filter()` can generate the time series without need for `arima.sim()` and with the advantage of being able to plug in a mean. The code is from page 23 of [this document](https://people.stat.sc.edu/wang528/Stat%20720/STAT720%20Notes.pdf).

Here is how it works:

```
# Toy example:
> x = rnorm(10)
> phi1 = 0.4
> phi2 = 0.7
> fil = c(phi1, phi2)
> filter(x, method='convolution', side=1, filter=fil)
Time Series:
Start = 1 
End = 10 
Frequency = 1 
 [1]         NA  0.4732177  1.1772258 -0.2822853  0.3438599  0.4165922  0.7885656  1.1353353
 [9]  0.4015954 -1.0864283
> phi2 * x[1] + phi1 * x[2]
[1] 0.4732177
> phi2 * x[2] + phi1 * x[3]
[1] 1.177226
```

```{r, fig.width=15, fig.height=8}
# Moving Average Model MA(2)
n <- 10^4
phi1 <- 0.78
phi2 <- 0.1
mu <- 70
sd <- 1
y <- ts(mu + filter(rnorm(n, 0, 1), method="convolution", sides=1, c(1,phi1, phi2))[-(1:50)])
Arima(y, order = c(0,0,2), include.mean = T)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF MA(2)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF MA(2)")
p1 / ( p2 | p3)
```

---

### ARMA:

The model is

$$y_t = c + \phi_1y_{t-1} + \cdots + \phi_py_{t-p} + \theta_1\epsilon_{t-1} + \cdots + \epsilon_t  $$

or in backshift notation,

$$y_t = c + \phi_1By_{t} + \cdots + \phi_pB^py_{t} + \theta_1B\epsilon_{t} + \cdots +\theta_qB^q \epsilon_t  $$

moving $y_t$ to the LHS:

$$(1 - \phi_1B - \cdots - \phi_pB^p)y_{t}=c+(\theta_1B + \cdots +\theta_qB^q) \epsilon_t$$

---

### ARIMA:

An ARIMA(1,1,1) model is

$$(1 - \phi_1B)\color{red}{(1-B)}y_t=c+(1+\theta_1B)\epsilon_t$$

where the part in red is the first difference.

There are two ways of writing the ARIMA model in general (from [here](https://youtu.be/A_pQ3gfVTaI?si=069xJ2p2DnZKV_5j&t=462)):

##### Intercept form:

$$(1 - \phi_1 B-\cdots  -\phi_p B^p)y_t' =c +(1 + \theta_1B+ \cdots + \theta_qB^q)\epsilon_t$$
where $$y_t' = (1-B)^dy_t$$

##### Mean form:

$$(1 - \phi_1 B-\cdots  -\phi_p B^p)(y_t'-\mu) =(1 + \theta_1B+ \cdots + \theta_qB^q)\epsilon_t$$

where $\mu$ is the mean of $y_t'.$ Therefore,

$$c= \mu(1-\phi_1-\cdots-\phi_p)$$

From [here](https://robjhyndman.com/hyndsight/arimaconstants/#:~:text=arima()%20function%20automates%20the,d%3D0%20d%3D0.):

>$$c(1−ϕ_1B−⋯−ϕ_pB_p)(1−B)^dy_t=c+(1+θ_1B+⋯+θ_qB_q)e_t$$
>
>where $c=μ(1−ϕ_1−⋯−ϕ_p)$ and $\mu$ is the mean of $(1-B)^d y_t.$
>
> Thus, the inclusion of a constant in a non-stationary ARIMA model is equivalent to inducing a polynomial trend of order $d$ in the forecast function. (If the constant is omitted, the forecast function includes a polynomial trend of order $d−1$.) When $d=0,$ we have the special case that $μ$ is the mean of $y_t.$
>
> Including constants in ARIMA models using R 
>
>`arima()`
>
> By default, the `arima()` command in R sets $c=μ=0$ when $d>0$ and provides an estimate of $\mu$ when $d=0.$ The parameter $μ$ is called the 'intercept' in the R output. It will be close to the sample mean of the time series, but usually not identical to it as the sample mean is not the maximum likelihood estimate when $p+q>0.$
>
>The `arima()` command has an argument `include.mean` which only has an effect when $d=0$ and is `TRUE` by default. Setting `include.mean=FALSE` will force $μ=0.$
>
> `Arima()`
>
>The `Arima()` command from the forecast package provides more flexibility on the inclusion of a constant. It has an argument `include.mean` which has identical functionality to the corresponding argument for `arima()`. It also has an argument `include.drift` which allows $μ\neq 0$ when $d=1.$ For $d>1,$ no constant is allowed as a quadratic or higher order trend is particularly dangerous when forecasting. The parameter $\mu$ is called the 'drift' in the R output when $d=1.$
>
>There is also an argument `include.constant` which, if `TRUE`, will set `include.mean=TRUE` if 
$d=0$ and `include.drift=TRUE` when $d=1.$ If `include.constant=FALSE`, both `include.mean` and `include.drift` will be set to `FALSE`. If `include.constant` is used, the values of `include.mean=TRUE` and `include.drift=TRUE` are ignored.
>
>When $d=0$ and `include.drift=TRUE`, the fitted model from `Arima()` is
>
>$$(1−ϕ_1B−⋯−ϕ_pB^p)(y_t−a−bt)=(1+θ_1B+⋯+θ_qB^q)e_t$$
>
>In this case, the R output will label $a$ as the 'intercept' and $b$ as the 'drift' coefficient.


---

From [here](https://rpubs.com/odenipinedo/ARIMA-Models-in-R#:~:text=Simulating%20ARMA%20models,data%20from%20an%20ARMA%20model.):

```{r, fig.width=15, fig.height=8}
n <- 10^4
y <- arima.sim(model = list(order = c(2, 0, 1), ar = c(1, -.9), ma = .8), n = n)
p1 = autoplot(y) + 
  geom_point() + labs(x = "", y = "") + xlim(c(0, 100))
p2 = ggAcf(y, lag.max=15) + ggtitle("ACF ARMA(2,1)")
p3 = ggPacf(y, lag.max=15) + ggtitle("PACF ARMA(2,1)")
p1 / ( p2 | p3)
Arima(y, order = c(2,0,1), include.mean = T)
```

#### ARIMA with SEASONALITY:

Find the explanation in [here](https://online.stat.psu.edu/stat510/lesson/4/4.1). The code is extracted from [here](https://robjhyndman.com/hyndsight/simulating-from-a-specified-seasonal-arima-model/index.html)

> The seasonal ARIMA model incorporates both non-seasonal and seasonal factors in a multiplicative model. One shorthand notation for the model is
>
$$(p, d, q) \times (P, D, Q)S$$
>
> with p = non-seasonal AR order, d = non-seasonal differencing, q = non-seasonal MA order, P = seasonal AR order, D = seasonal differencing, Q = seasonal MA order, and S = time span of repeating seasonal pattern. Without differencing operations, the model could be written more formally as
>
> $$\Phi(B^S)\phi(B)(x_t-\mu)=\Theta(B^S)\theta(B)w_t$$
>
>The non-seasonal components are:
>
>$$\phi(B)=1-\phi_1B-\ldots-\phi_pB^p$$
>$$\theta(B) = 1+ \theta_1B+ \ldots + \theta_qB^q$$
>
>The seasonal components are:
>
>$$\Phi(B^S) = 1- \Phi_1 B^S - \ldots - \Phi_PB^{PS}$$
>$$\Theta(B^S) = 1 + \Theta_1 B^S + \ldots + \Theta_Q B^{QS}$$

```{r, fig.width=15, fig.height=8}
m <- arima(ts(rnorm(100), freq=4), order=c(1,0,1), seasonal=c(1,1,1),
             fixed = c(phi=0.5, theta=-0.4, Phi=0.3, Theta=-0.2))
nsim <- 100
foo <- simulate(m, nsim=nsim)
autoplot(forecast(foo, h=20)) 

# Increasing the number of samples for a more accurate fitting:
nsim <- 10000
foo <- simulate(m, nsim=nsim)
fit <- Arima(foo, order=c(1,0,1), seasonal=c(1,1,1))
fit
```



---
<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>

**NOTE: These are tentative notes on different topics for personal use - expect mistakes and misunderstandings.**
