<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

###MOMENTS of a RANDOM VARIABLE:
<br>

####DEFINITION:

$\color{blue}{\text{Expected}}$ value of a $\color{red}{\text{power}}$ of a random variable.

We use [LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician) to calculate this expectation: the expected value of a function of the variable (in this case an exponentiation) is obtained by multiplying that function $\times$ the pdf, and integrating:

$\large \mathbb{E}[\color{blue}{X^k}] = \displaystyle\int_{-\infty}^{\infty}\color{blue}{X^k}\,\,\color{green}{\text{pdf}}\,\,\,dx$

$k$ is the number of the moment. For the *mean*:

$\large \mathbb{E}[{X^1}] = \displaystyle\int_{-\infty}^{\infty}X^1\,\,\text{pdf}\,\,\,dx$

There are two types of moments:

1. Raw moment (moment about the origin). Fits perfectly the above definition, of $\mu'_k=\mathbb{E}\,(X)^k= \mathbb{E}\,(X-0)^k$. The mean is the first *raw* moment.

2. Central moment: It is centered around the mean: $\mu_k=\mathbb{E}\,(X-\mu)^k$. This is the moment that we need, for instance, to calculate the variance:

Variance is $var(x) = \mathbb{E}\,(X-\mu)^2 = \displaystyle\int_{-\infty}^{\infty} (X-\mu)^2 \,pdf\,dx$.

Alternatively, it can be defined as the difference between the second and the $\color{red}{\text{squared}}$ first raw moments:

$\mu_2= \mathbb{E}[X^2]\,-\,\mathbb{E}[X]^2.$

<br>

1. Mean: First *raw* moment.
2. Variance: Second *central* moment.
3. Skew (asymmetry): Third *central* moment, $\color{blue}{\text{standardized}}$ i.e. divided by the $\sigma^3$.
4. Kurtosis (peakedness): Fourth *central* moment, but divided by $\sigma^3$, AND subtacting $-3$ from the result.

<br>

####MOMENT GENERATING FUNCTIONS:

Instead of integrating the defining equation to obtain the raw moments, we can 1. First get this "moment generating function (MGF)"; and 2. Derive the MGF to get the raw moments.

#####DEFINITION of MGF:

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}]$

<br>

Expanding the Taylor series of this function:

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}] = \mathbb{E}[e^{\,tX}] = \mathbb{E} \left[ \displaystyle \sum_{n=0}^\infty \frac{(e^{tX})^{(n)}|a}{n!}\,(X-a)^n \right].$

<br>

$\large (e^{\,tX})^{(n)}|a$ being the notation for the $n$ derivative of the function, evaluated at $\large a$. So if $a = 0$, i.e.  Maclaurin series: we evaluate at $a = 0$, which will go into $t$. We derive with respect to $t$:


<br>

$\Large M_X(t) =\mathbb{E}[e^{\,tX}]= \mathbb{E} \left[e^{tX} + \frac{X\,e^{tX} }{1!} \, (t-0) \, + \frac{X^2\,e^{tX} }{2!} \, (t-0)^2 \, + \frac{X^3\,e^{tX} }{3!} \, (t-0)^3 \, + \cdots \right] |a = 0$

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\mathbb{E} \left[e^{0X} + \frac{X\,e^{0X} }{1!} \, (t) \, + \frac{X^2\,e^{0X} }{2!} \, (t)^2 \, + \frac{X^3\,e^{0X} }{3!} \, (t)^3 \, + \cdots \right]$

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\mathbb{E} \left[1 + \frac{X}{1!} \, t \, + \frac{X^2}{2!} \, t^2 \, + \frac{X^3}{3!} \, t^3 \, + \cdots \right]$

<br>

Now, since we are evaluating at $t=0$, every term with a $t$ will be $0$, but the point is to differentiate the power series over and over, so that every time there is just one term without $t$.

Since expectation is linear operator:

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\mathbb{E} \left[1 \right] + \mathbb{E} \left[ \frac{X}{1!} \, t \right] \, + \mathbb{E} \left[ \frac{X^2}{2!} \, t^2 \right] \, + \mathbb{E} \left[ \frac{X^3}{3!} \, t^3 \right] \, + \cdots$

<br>

$\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\mathbb{E} \left[1 \right] +  \frac{\mathbb{E} \left[X\right]}{1!} \, t  \, +  \frac{\mathbb{E} \left[X^2\right]}{2!} \, t^2  \, +  \frac{\mathbb{E} \left[X^3\right]}{3!} \, t^3 \, + \cdots$

<br>

As shown, the moments are just the coefficient that will "survive" after taking the derivative corresponding to the number of the moment.

So the key is to integrate to find the $\mathbb{E}[e^{\,tX}]=\displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\text{pdf}\,\,\,dx$ (by LOTUS the expectation is the integral of the function of $X$ times its pdf), so that we can avoid the more difficult direct calculation of:

<br>
$\large \mathbb{E}[\color{blue}{X^k}] = \displaystyle\int_{-\infty}^{\infty}\color{blue}{X^k}\,\,\color{green}{\text{pdf}}\,\,\,dx$

<br>

For the exponential (remember that pdf of an exponential is $\lambda\,e^{-\lambda x}$):

$\Large M_X(t) =\mathbb{E}[e^{\,tX}]=\displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\text{pdf}\,\,\,dx = \displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\,\lambda\,e^{-\lambda x}\,\,dx = \lambda \displaystyle\int_{-\infty}^{\infty}e^{\,X(t-\lambda)}\,\,\,dx$

<br>
$\Large M_X(t) =\mathbb{E}[e^{\,tX}]=\Large \lambda \left(\left.\frac{e^{\,X(t-\lambda)}}{X(t-\lambda)} \right|_{-\infty}^{\infty}\right)=\frac{\lambda}{t-\lambda} = \lambda\,(t-\lambda)^{-1}.$ If $t- \lambda<0.$
<br>

Let's put it to work to derive the second raw moment (for example):

<br>
$\Large M_X^{2}(t = 0) =\frac{d^2}{dt^2}M_X(t=0)= \frac{d^2}{dt^2}\lambda\,(t-\lambda)^{-1}=\frac{2}{\lambda^2}.$
<br>

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
