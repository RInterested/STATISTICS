<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="80" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>



####DEFINITION OF MOMENT GENERATING FUNCTIONS:

---

Remember that the Taylor series of a function $f(x)$ is the power series:

$$f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3+\cdots$$

The Taylor series of $e^x$ at $a=0$ is:

$$1 + \frac{x^1}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots = 1 + x + + \frac{x^2}{2} + \frac{x^3}{6} + \frac{x^4}{24} + \cdots= \sum_{n=0}^\infty \frac{x^n}{n!}$$

<br>

---

#####DEFINITION of MGF:

Instead of integrating the defining equation to obtain the raw moments, we can 

>1. First get this "moment generating function (MGF)"; and 
2. Derive the MGF to get the raw moments.


<br>

$\color{blue}{\Large M_X(t) = \mathbb{E}[e^{\,tX}]}$

This is calculated differently for discrete distributions (Probability Mass Function, PMF):

$\color{blue}{\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\displaystyle \sum_{i=1}^{\infty}\, e^{tx_i}\,p_i}$

as compared to continuous distributions (PDF):

$\color{blue}{\Large M_X(t) = \mathbb{E}[e^{\,tX}]=\displaystyle \int_{-\infty}^{\infty}\, e^{tx}\,f(x)\, dx}$


Notice that the MGF is the [Laplace transform](https://en.wikipedia.org/wiki/Laplace_transform#Probability_theory) with $-s$ replaced by $t$:

$\mathscr L\{f\}(s)= \mathbb E [e^{-sX}].$

<br>

Expanding the Taylor series of this function:

<br>

$\large M_X(t) = \mathbb{E}[e^{\,tX}] = \mathbb{E} \left[ \displaystyle \sum_{n=0}^\infty \frac{X^{(n)}}{n!}\,(X-a)^n \right]\vert a.$

<br>

If $a = 0$, we have a  Maclaurin series. We derive **with respect to $t$**. Attention: $X$ is a constant. Hence, $\frac{d}{dt}e^{tX}=Xe^{tX}$.


<br>

\begin{align}
 M_X(t) &=\mathbb{E}[e^{\,tX}]\\ &= \mathbb{E} \left[1 + \frac{Xe^{tX}}{1!} \, (t-a) \, + \frac{X^2e^{tX}}{2!} \, (t-a)^2 \, + \frac{X^3e^{tX}}{3!} \, (t-a)^3 \, + \cdots \right] |a = 0\\
&=\mathbb{E} \left[1 + \frac{Xe^{tX}}{1!} \, t \, + \frac{X^2e^{tX}}{2!} \, t^2 \, + \frac{X^3e^{tX}}{3!} \, t^3 \, + \cdots \right]
\end{align}

<br>

Now, since we are evaluating at $t=0$, every term with a $t$ will be $0$, but the point is to differentiate the power series over and over, so that every time there is just one term without $t$. Well... that term withot a $t$ multiplying the first term will still have $e^{tX}$, butit will go away when evaluating at zero: $e^{tX}=0.$ Hence,

\begin{align}
 M_X(t) &=\mathbb{E}[e^{\,tX}]\\ &=\mathbb{E} \left[1 + \frac{Xe^{tX}}{1!} \, t \, + \frac{X^2e^{tX}}{2!} \, t^2 \, + \frac{X^3e^{tX}}{3!} \, t^3 \, + \cdots \right]\\
&=\mathbb{E} \left[1 + \frac{X}{1!} \, t \, + \frac{X^2}{2!} \, t^2 \, + \frac{X^3}{3!} \, t^3 \, + \cdots \right]
\end{align}

Since expectation is linear operator:

<br>

\begin{align}
M_X(t) &= \mathbb{E}[e^{\,tX}]=1 + \mathbb{E} \left[ \frac{X}{1!} \, t \right] \, + \mathbb{E} \left[ \frac{X^2}{2!} \, t^2 \right] \, + \mathbb{E} \left[ \frac{X^3}{3!} \, t^3 \right] \, + \cdots\\
&= \mathbb{E}[\color{red}{e^{\,tX}}]=1 +  \frac{\mathbb{E} \left[X\right]}{1!} \, t  \, +  \frac{\mathbb{E} \left[X^2\right]}{2!} \, t^2  \, +  \frac{\mathbb{E} \left[X^3\right]}{3!} \, t^3 \, + \cdots
\end{align}

<br>

As shown, the moments are just the coefficient that will "survive" after taking the derivative corresponding to the number of the moment:

For the *first moment* and reaching back to Eq.*:

$$\large \mathbb{E}[\color{blue}{X}] = \int_{-\infty}^{\infty}\color{blue}{X^1}\,\,\color{green}{\text{pdf}}\,\,\,dx$$

corresponds to the *first derivative* of the MGF evaluated at zero:


$$M^{'}_X(t=0)= \frac{\mathbb{E} \left[X\right]}{1!}= \mathbb E [\color{blue}{X}]$$

**So what is the point?** We don't have $\color{red}{e^{\,tX}}$ any longer - we now have the first derivative, but $e^{\,tX}$ is easy to integrate, so we use to get the expectation. So the key is to integrate to find the $\mathbb{E}[e^{\,tX}]=\displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\text{pdf}\,\,\,dx$ (by LOTUS the expectation is the integral of the function of $X$ times its pdf), so that we can avoid the more difficult direct calculation of: $\large \mathbb{E}[\color{blue}{X^k}] = \displaystyle\int_{-\infty}^{\infty}\color{blue}{X^k}\,\,\color{green}{\text{pdf}}\,\,\,dx.$

The key is to find the $\color{red}{e^{\,tX}}$ to begin, and then derive over and over.

<br>

For the exponential (remember that pdf of an exponential is $\lambda\,e^{-\lambda x}$):

\begin{align}
M_X(t) &=\mathbb{E}[e^{\,tX}]=\displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\text{pdf}\,\,\,dx\\
&= \displaystyle\int_{-\infty}^{\infty}e^{\,tX}\,\,\,\lambda\,e^{-\lambda x}\,\,dx\\ 
&= \lambda \displaystyle\int_{-\infty}^{\infty}e^{\,X(t-\lambda)}\,\,\,dx\\
&= \lambda \left(\left.\frac{e^{\,X(t-\lambda)}}{X(t-\lambda)} \right|_{-\infty}^{\infty}\right)=\frac{\lambda}{t-\lambda}\\ 
&= \lambda\,(t-\lambda)^{-1}.
\end{align}

Let's put it to work to derive the second raw moment (for example):

<br>
\begin{align} M_X^{2}(t = 0) &=\frac{d^2}{dt^2}M_X(t=0)\\
&= \frac{d^2}{dt^2}\lambda\,(t-\lambda)^{-1}=\frac{2}{\lambda^2}.
\end{align}

<br>

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
