---
output: 
  html_document:
    css: custom.css
---

<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="30" WIDTH="50" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

####RIDGE REGRESSION:
<br>

OLS minimizes the cost function

$$\underset{\vec \beta}{\text{min}}\Vert \vec y- X\vec \beta\Vert^2_2$$

which is to say the 2-norm or square residuals. The Gauss-Markov theorem says that these coefficients are BLUE. They are best (B) because they have the smallest variance; and they are also un-biased (U).

This would not work if there was high correlation between the features or explanatory variables, as for example:

$$\text{\$ spent on vacation} \sim \beta_0 + \beta_1 \text{income} + \beta_2 \text{taxes paid} + \varepsilon$$

where the two explanatory variables are highly correlated. In this case the calculated coefficients will be highly variable between samples.

This is the case where the model matrix is nearly singular, presenting inverse problems.

Using ridge regression the estimated coefficients will be biased, although with a lower variance. Ridge is going to control the magnitude of the coefficients.

The way this is accomplished is by modifying the cost function as


$$\underset{\vec \beta}{\text{min}}\Vert \vec y- X\vec \beta\Vert^2_2\text{ such that }\Vert \vec \beta\Vert^2_2\leq c^2$$

so in the case of $2$ coeffients, we want $c^2 \leq \beta_0^2 + \beta_1^2,$ which is a disc of radius $c:$

![](https://user-images.githubusercontent.com/9312897/88944121-5a2cda80-d25a-11ea-9dd0-2d2b6127d5d1.png)

Naturally, this should only be remotely applicable when there is more than the intercept and a variable. So it would be a sphere. The estimated values will be at the point of tangential contact of the cost function level curves and the constraint of the parameters (sphere).

This is solved with Lagrange multipliers.

If $E=\sum_i \left( y_i - \beta_o-\beta_1\right)^2$

$$F=E + \lambda \left(\beta_0^2 + \beta_1^2 - c^2 \right)$$

and we look for 


$$\underset{\vec \lambda,\beta_0,\beta_1}{\text{min}}F$$

by trying different $\lambda$'s. We could try by differentiating with respect to $\lambda,\beta_0,\beta_1$ and setting to zero, but it is usually done numerically. The computation takes into account the R-squared values with different lambdas, which in turn, output different optimal betas. Once we settle on a lambda, $\lambda  c^2 $ becomes a constant. So at that point the minimization will be of the function


$$F=E + \lambda \left(\beta_0^2 + \beta_1^2  \right)$$

or

$$\underset{\vec \beta}{\text{min}}\Vert \vec y- X\vec \beta\Vert^2_2 \lambda \Vert\vec \beta \Vert^2_2$$

the solution will be

$$\vec \beta^R=\left( X^\top X+\lambda I \right)^{-1}X^\top \vec y$$

---

The same with some more math:

In ridge regression we minimize:

$\bf (y - X\beta)^\prime(y - X\beta) + \lambda \beta^\prime \beta$

What are the normal equations in this case?

$\bf (X^\prime X + \lambda I)\beta = X^\prime y$

Proof:

From [this entry](http://stats.stackexchange.com/a/164546/67822), let's consider the square root of $\lambda$ to be $\nu$, and construct the *row* augmented model $[p \times p]$ matrix, with $p$ being the number of columns:

$$\bf X_{*} = \pmatrix{X \\ \nu I}$$

And the $\bf y$ vector augmented by a corresponding $p$ number of zeros - $y_{*}$.

Now the cost function will be:

$\bf (y_{*} - X_{*}\beta)^\prime(y_{*} - X_{*}\beta)  = (y - X\beta)^\prime(y - X\beta) + \lambda \beta^\prime \beta \tag{1}$

because there will be $p$ additional terms of the form $(0 - \nu \beta_i)^2 = \lambda \beta_i^2$

Inspecting the LHS of Eq.1, the normal equations are:

$\bf (X_{*}^\prime X_{*})\beta = X_{*}^\prime y_{*}\tag{2}$

Since $y_{*}$ just has zeros tagged to the end the RHS of Eq.2 is the same as $\bf X^\prime y$, and on the LHS, $\nu^2 I=\lambda I$ results in:

$\bf (X^\prime X + \lambda I)\beta = X^\prime y.$

---

>By adjoining $\bf \nu I$ to $\bf X$, thereby lengthening them, we are placing the vectors in avlarger space $\mathbb R^{n+p}$ by including $p$ "imaginary", mutually orthogonal directions. The first column of $\bf X$ is given a small imaginary component of size $\nu$, thereby lengthening it and moving it out of the space generated by the original $p$ columns. The second, third, ..., $p^{\text{th}}$ columns are similarly lengthened and moved out of the original space by the same amount $\nu$--but all in different new directions. Consequently, any collinearity present in the original columns will immediately be resolved. Moreover, the larger $\nu$ becomes, the more these new vectors approach the individual $p$ imaginary directions: they become more and more orthonormal. Consequently, the solution of the normal equations will immediately become possible and it will rapidly become numerically stable as $\nu$ increases from zero.


---

####LASSO REGRESSION:

On the other hand, in Lasso regression we change the constraint so that some coefficient are nullified. Geometrically, the points jutting out are "hit" more often, and in those corners a number of coefficients will be zero, and the variable associated with it will be nullified (sparsification or feature selection).

![](https://user-images.githubusercontent.com/9312897/88947305-5e5af700-d25e-11ea-8f5a-8b85c6aefc0f.png)


by minimizing the cost function with the l-$1$ norm:

$$\underset{\vec \beta}{\text{min}}\Vert \vec y- X\vec \beta\Vert^2_2\text{ such that }\Vert \vec \beta\Vert^1\leq c$$

It will be calculated through Lagrange multiplier, but there will not be an explicit formula for Lasso.

---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
