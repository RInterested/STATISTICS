<br>
<a href="http://rinterested.github.io/statistics/index.html">
<img HEIGHT="20" WIDTH="40" src="https://cloud.githubusercontent.com/assets/9312897/10556783/a27a26fa-745c-11e5-9b1a-2d5bd737387a.PNG"></a>
<br><br>

The essence of the problem addressed by logistic regression is the binary nature of the dependent variable. We want to estimate the probability of success/failure of the dependent variable ($p(Y=1)$), given the explanatory variables: $p(Y=1|X_1,X_2,\cdots, X_n)$. The explanatory variables can be categorical or continuous - it doesn't matter.


Logistic regression addresses the following issues:

1. There is a need to bound at $1$ since we are estimating a probability, but lines are not naturally bounded. So we transform probability into odds:

$\Large \text{odds}=\frac{p(Y=1)}{1-p(Y=1)}$.

In addition, this also helps turning a sigmoid curve of the typical cummulative probability distribution (e.g. normal) into a linear relation.

2. Now, when the probability goes down to zero, the odds also tend to zero; yet there is no floor restriction in a line.

This problem can be addressed by expressing the linear relation in log scale: $log(\text{odds})$, which will tend to $-\infty$ as the odds tend to zero. This is the **logit** or log-odds function.

The logistic regression model is therefore:


$\Large \color{red}{\text{log}} \left[\color{blue}{\text{odds(p(Y=1))}}\right]=\color{red}{\text{log}}\left(\frac{\hat p\,(Y=1)}{1-\hat p\,(Y=1)}\right) = X\beta = \beta_o + \beta_1 x_1 + \beta_2 x_2 +\cdots+ \beta_p x_p$



Consequently,

$\Large \color{blue}{\text{odds(Y=1)}} = \frac{p\,(Y=1)}{1\,-\,p\,(Y=1)} = e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p} = e^{\,\beta_o}\,e^{\,\beta_1x_1}\,e^{\,\beta_2x_2}\,\cdots\,e^{\,\beta_px_p}$

Therefore a unit increase in $x_1$ increases the odds $e^{\,\beta_1}$ times. The factors $e^{\,\beta_i}$ are the **ODDS RATIO**s. On the other hand, $\beta_i$ (the coefficients) are the **LOG ODDS-RATIO**:

<br>
<img HEIGHT="900" WIDTH="900" src="https://cloud.githubusercontent.com/assets/9312897/13310362/745ac3ac-db53-11e5-8147-5dcd27f73224.png">
<br>

For CATEGORICAL VARIABLES,  are the **ODDS RATIO** denote how much the presence or absence of a factor variable increases the odds of a positive dependent variable. 

For CONTINOUS VARIABLES the odds ratio tell us how much the odds increase multiplicatively with a one-unit change in the independent variable. To calculate the difference in odds you raise the OR to the power of the difference between the observations.

To get the exponentiated coefficients and their confidence intervals in R, we use cbind to bind the coefficients and confidence intervals column-wise:

```
exp(cbind(OR = coef(mylogit), confint(mylogit)))
```

See [here](http://www.ats.ucla.edu/stat/r/dae/logit.htm).


<br><br>

#### Translation into probabilities:

<br>

$\large p(Y = 1) = (1-p(Y=1))\times\,e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}$

<br>

$\large p(Y = 1)\,\times (1 \,+\,e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}) = e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}$

<br>

$\Large \color{green}{\text{p(Y = 1)}} = \frac{\color{blue}{\text{odds(Y=1)}}}{1\,+\,\color{blue}{\text{odds(Y=1)}}}=\frac{e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}}{1 \,+\,e^{\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}}=\frac{1}{1 \,+\,e^{-(\,\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}}$


<br><br>


###CATEGORICAL INDEPENDENT VARIABLES:

Here is an example extracted form [this site](https://online.stat.psu.edu/stat504/node/225/):

```{r}
# Define an explanatory variable with two levels:
parentsmoke <- as.factor(c(1,0))
# create response vector:
response <- cbind(yes=c(816,188), no=c(3203,1168))
response
smoke.logistic <- glm(response~parentsmoke, family=binomial(link=logit))
smoke.logistic
summary(smoke.logistic)
```

This information gives us the fitted model:

$$\text{logit}(\hat\pi_i)=\log\frac{\hat\pi_i}{1-\hat\pi_i}=\hat\beta_0+\hat\beta_1X_i=-1.837+0.459\,(\text{parents smoke = 1})$$
where `parentsmoke` is a dummy variable ((e.g. design variable) that takes value $1$ if at least one parents is smoking and $0$ if neither is smoking.  

`parent smoking =  0 is the baseline`. We are modeling here probability of "children smoking". Estimated $\beta_0=1.827$
with a standard error of $0.078$ is significant and it says that log-odds of a child smoking versus not smoking if neither parents is smoking (the baseline level) is $-1.827$ and it's statistically significant.

Estimated $\beta_1=0.459$ with a standard error of $0.088$ is significant and it says that log-odds-ratio of a child smoking versus not smoking if at least one parent is smoking versus neither parents is smoking (the baseline level) is $0.459$ and it's statistically significant. $\exp(0.459)=1.58$ are the estimated odds-ratios.

Why not just ran a Chi square test:

```{r}
prop.test(response, correct = F)
```

It does detect differences in proportions but [it doesn't address](https://online.stat.psu.edu/stat504/node/150/):

- Modeling the "risk" of student smoking as a function of parents' smoking behavior.
- Describing the differences between student smokers and nonsmokers as a function of parents smoking behavior via descriptive discriminate analyses.
-Predicting the probabilities that individuals fall into two categories of the binary response as a function of some explanatory variables, e.g. what is the probability that a student is a smoker given that neither of his/her parents smokes.
- Predicting that a student is a smoker given that neither of his/her parents smokes, i.e. probabilities that individuals fall into two categories of the binary response as a function of some explanatory variables, we want to classify new students into "smoking" or "nonsmoking" group depending on parents smoking behavior.
we want to develop a social network model, adjust for "bias", 
These are just some of the possibilities of logistic regression, which cannot be handled within a framework of goodness-of-fit only.

Here is the same model splitting mothers and fathers as predictors:

```{r}
kids.smoke <- matrix(c(188,599,816-205,1168,1456,3203-1456), nrow = 3)
dimnames(kids.smoke) = list(Parents = c("None", "Mother", "Father"), Kids = c("Smokers", "Non-smokers"))
kids.smoke
bad.example <- as.factor(c(0,"1Mother","2Father"))
smoke.logistic <- glm(kids.smoke~bad.example, family=binomial(link=logit))
smoke.logistic
summary(smoke.logistic)
```

---

An example with credit defaults with some calculations extracted from [here](https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/):

```{r}
set.seed(123)

gender <- factor(round(runif(100,0,1)),
                labels=c("female", "male"))
age <- round(runif(100,20,80))
profession <- factor(round(runif(100,1,4)),
             labels=c("student", "worker", "teacher", "self-employed"))


# Let's make males, teachers and self-employed more likely to default:

credit.default <-  round(((as.numeric(gender)/2 + as.numeric(profession)/4)/2) * runif(100,0.5,1))

df <- data.frame(credit.default,gender,age,profession)
head(df)

catlogis <- glm(credit.default ~ gender+age+profession, df, family=binomial(link="logit"))
summary(catlogis)
require(pROC)
auc(df$credit.default, predict(catlogis, type="response"))
```

We can generate an ANOVA to analyze the table of variance, paying attention at the difference between the `NULL` variance and the `Resid. Deviance` to see which variable reduce this residual variance in comparison to the model with only an intecept. The p-values also help select variables. We want a drop in the deviance and AIC.

```{r}
catlogis <- glm(credit.default ~ gender+profession, df, family=binomial(link="logit"))
summary(catlogis)
auc(df$credit.default, predict(catlogis, type="response"))


genlogis <- glm(credit.default ~ gender, df, family=binomial(link="logit"))
summary(genlogis)
auc(df$credit.default, predict(genlogis, type="response"))


fitted.results <- predict(genlogis, type="response")
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != df$credit.default)
print(paste('Accuracy', 1-misClasificError))

library(ROCR)
pr <- prediction(predict(genlogis, type="response"), df$credit.default)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


proflogis <- glm(credit.default ~ profession, df, family=binomial(link="logit"))
summary(proflogis)
auc(df$credit.default, predict(proflogis, type="response"))

pr <- prediction(predict(proflogis, type="response"), df$credit.default)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

---

Another example from [this video](https://www.youtube.com/watch?v=qcvAqAH60Yw&t=242s):

```{r}
set.seed(420)
n.samples <- 100
weight <- sort(rnorm(n.samples,172,29))
obese <- ifelse(runif(n.samples) < (rank(weight)/100), yes=1, no=0)
plot(weight,obese)
glm.fit <- glm(obese ~ weight, family="binomial")
lines(weight, glm.fit$fitted.values)
par(pty="s")
roc(obese, glm.fit$fitted.values, plot=T)
roc.info <- roc(obese, glm.fit$fitted.values)
roc.df <- data.frame(ttp=roc.info$sensitivities*100, fpp=(1-roc.info$specificities) *100, thresholds=roc.info$thresholds)
head(roc.df)
tail(roc.df)
```

Notice that the first row of `head(roc.df)` with a threshold for labeling "obese" of $-\inf$ the ttp (true positive percentage) will be of $100\%,$ and so will be the fpp (false positive percentage), and it corresponds to the upper-right hand part of the ROC curve. The bottom-left part of the curve corresponds to the last row of `tail(roc.df)`.

We can isolate part of the data, for example with ttp between $60$ and $80$, to select an optimal threshold value:

```{r}
roc.df[roc.df$ttp > 60 & roc.df$ttp < 80,]
```

We can plot the ROC differently:

```{r}
par(pty="s")
roc(obese, glm.fit$fitted.values, plot=T, legacy.axes=T, percent=T,
    xlab='False positive percentage', ylab='True positive percentage',
    col="#377eb8",lwd=4, print.auc=T)
```

To calculate the partial AUC of the beginning, where only a small number of false positives are allowed...

```{r}
par(pty="s")
roc(obese, glm.fit$fitted.values, plot=T, legacy.axes=T, percent=T,
    xlab='False positive percentage', ylab='True positive percentage',
    col='blue',lwd=4, print.auc=T, print.auc.x=45, partial.auc=c(100,90),
    auc.polygon=T, auc.polygon.col='turquoise')
```

---

###TESTING A LOGISTIC REGRESSION MODEL:


1. GOODNESS-OF-FIT: Model calibration

The Hosmer-Lemeshow goodness of fit test divides up in boxes the predicted probabilities (in R the function is `fitted` as opposed to `predict`), and runs a chi-square test comparing to the percentage of cases that have $Y=1$ among those with predicted probability within a certain interval. See [here](http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/):

The Hosmer-Lemeshow goodness of fit test is based on dividing the sample up according to their predicted probabilities, or risks. Specifically, based on the estimated parameter values $\hat{\beta}_{0},\hat{\beta}_{1},..,\hat{\beta}_{p},$ for each observation in the sample the probability that $Y=1$ is calculated, based on each observation's covariate values:

$$\hat{\pi} = \frac{\exp(\hat{\beta}_{0}+\hat{\beta}_{1}X_{1}+..+\hat{\beta}_{p}X_{p})}{1+\exp(\hat{\beta}_{0}+\hat{\beta}_{1}X_{1}+..+\hat{\beta}_{p}X_{p})}$$

The observations in the sample are then split into $g$ groups (we come back to choice of g later) according to their predicted probabilities. Suppose (as is commonly done) that $g=10.$ Then the first group consists of the observations with the lowest $10\%$ predicted probabilities. The second group consists of the $10\%$ of the sample whose predicted probabilities are next smallest, etc etc.

Suppose for the moment that all of the observations in the first group had a predicted probability of $0.1.$ Then, if our model is correctly specified, we would expect the proportion of these observations who have $Y=1$ to be $10\%.$ Of course, even if the model is correctly specified, the observed proportion will deviate to some extent from 10%, but not by too much. If the proportion of observations with $Y=1$ in the group were instead $90\%,$ this is suggestive that our model is not accurately predicting probability (risk), i.e. an indication that our model is not fitting the data well.

In practice, as soon as some of our model covariates are continuous, each observation will have a different predicted probability, and so the predicted probabilities will vary in each of the groups we have formed. To calculate how many Y=1 observations we would expect, the Hosmer-Lemeshow test takes the average of the predicted probabilities in the group, and multiplies this by the number of observations in the group. The test also performs the same calculation for $Y=0,$ and then calculates a Pearson goodness of fit statistic

$$\sum^{1}_{k=0} \sum^{g}_{l=1} \frac{(o_{kl}-e_{kl})^{2}}{e_{kl}}$$

where $o_{0l}$ denotes the number of observed $Y=0$ observations in the $l$-th group, $o_{1l}$ denotes the number of observed $Y=1$ observations in the $l$-th group, and $e_{0l}$ and $e_{1l}$ similarly denote the expected number of zeros.

In a 1980 paper Hosmer-Lemeshow showed by simulation that (provided $p+1<g$) their test statistic approximately followed a chi-squared distribution on $g-2$ degrees of freedom, when the model is correctly specified. This means that given our fitted model, the p-value can be calculated as the right hand tail probability of the corresponding chi-squared distribution using the calculated test statistic. If the p-value is small, this is indicative of poor fit.

It should be emphasized that a large p-value does not mean the model fits well, since lack of evidence against a null hypothesis is not equivalent to evidence in favor of the alternative hypothesis. In particular, if our sample size is small, a high p-value from the test may simply be a consequence of the test having lower power to detect mis-specification, rather than being indicative of good fit.

2. C-STATISTIC or AUC (area under the curve of the ROC): Discriminatory power of the model

It is based on selecting different probabilities from $0$ to $1$ and calculating sensitivity and specificity in an ROC curve, and then measuring the AUC. See [here](http://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/):

Our model or prediction rule is perfect at classifying observations if it has 100% sensitivity and 100% specificity. Unfortunately in practice this is (usually) not attainable. So how can we summarize the discrimination ability of our logistic regression model? For each observation, our fitted model can be used to calculate the fitted probabilities $P(Y=1|X_{1},..,X_{p}).$ On their own, these don't tell us how to classify observations as positive or negative. One way to create such a classification rule is to choose a cut-point $c,$ and classify those observations with a fitted probability above $c$ as positive and those at or below it as negative. For this particular cut-off, we can estimate the sensitivity by the proportion of observations with $Y=1$ which have a predicted probability above $c,$ and similarly we can estimate specificity by the proportion of $Y=0$ observations with a predicted probability at or below $c.$ If we increase the cut-point $c,$ fewer observations will be predicted as positive. This will mean that fewer of the $Y=1$ observations will be predicted as positive (reduced sensitivity), but more of the $Y=0$ observations will be predicted as negative (increased specificity). In picking the cut-point, there is thus an intrinsic trade off between sensitivity and specificity. Now we come to the ROC curve, which is simply a plot of the values of sensitivity against one minus specificity, as the value of the cut-point $c$ is increased from $0$ through to $1.$ Example:

```{r}
set.seed(63126)
n <- 1000
x <- rnorm(n)
pr <- exp(x)/(1+exp(x))
y <- 1*(runif(n) < pr)
mod <- glm(y~x, family="binomial")
predpr <- predict(mod,type=c("response"))
library(pROC)
roccurve <- roc(y ~ predpr)
plot(roccurve)
```

A popular way of summarizing the discrimination ability of a model is to report the area under the ROC curve. We have seen that a model with discrimination ability has an ROC curve which goes closer to the top left hand corner of the plot, whereas a model with no discrimination ability has an ROC curve close to a $45$ degree line. Thus the area under the curve ranges from $1,$ corresponding to perfect discrimination, to $0.5,$ corresponding to a model with no discrimination ability. The area under the ROC curve is also sometimes referred to as the **c-statistic** (c for concordance).

```{r}
auc(roccurve)
```

The ROC curve (AUC) has a somewhat appealing interpretation. It turns out that the AUC is the probability that if you were to take a random pair of observations, one with $Y=1$ and one with $Y=0,$ the observation with $Y=1$ has a higher predicted probability than the other. The AUC thus gives the probability that the model correctly ranks such pairs of observations.

In the biomedical context of risk prediction modelling, the AUC has been criticized by some. In the risk prediction context, individuals have their risk of developing (for example) coronary heart disease over the next 10 years predicted. Thus a measure of discrimination which examines the predicted probability of pairs of individuals, one with $Y=1$ and one with $Y=0,$ does not really match the prospective risk prediction setting, where we do not have such pairs.

---

<br><br>

###INTERPRETATION OF LOGISTIC REGRESSION RESULTS:
<br>

***QUESTION:***

(Initially posted [here](http://stats.stackexchange.com/q/136193/67822))

I ran a linear regression of acceptance into college against SAT scores and family / ethnic background. The data are fictional. The question focuses in the gathering and interpretation of odds ratios when leaving the SAT scores aside for simplicity. 

The variables are `Accepted` (0 or 1) and `Background` ("red" or "blue"). I set up the data so that people of "red" background were more likely to get in:

    fit <- glm(Accepted ~ Background, data=dat, family="binomial")
    exp(cbind(Odds_Ratio_RedvBlue = coef(fit), confint(fit)))
    
                            Odds_Ratio_RedvBlue             2.5 %       97.5 %
    (Intercept)             0.7088608                     0.5553459   0.9017961
    Backgroundred           2.4480042                     1.7397640   3.4595454


Questions:

1. Is 0.7 the odd ratio of a person of "blue" background being accepted? I'm asking this because I also get 0.7 for "`Backgroundblue`" if instead I run the following code:

        fit <- glm(Accepted~Background - 1, data=dat, family="binomial")
        exp(cbind(OR=coef(fit), confint(fit)))

2. Shouldn't the odds ratio of "red" being accepted ($\rm Accepted/Red:Accepted/Blue$) just the reciprocal: ($\rm OddsBlue = 1 / OddsRed$)?

***ANSWER:***

I've been working on answering my question by calculating manually the odds and odds ratios:

    Acceptance   blue	         red	        Grand Total
    0	           158	         102	            260
    1	           112	         177	            289
    Total	       270	         279	            549

So the *Odds Ratio* of getting into the school of Red over Blue is: 

$$
\frac{\rm Odds\ Accept\ If\ Red}{\rm Odds\ Acccept\ If\ Blue} = \frac{^{177}/_{102}}{^{112}/_{158}} = \frac {1.7353}{0.7089} = 2.448
$$ 

And this is the `Backgroundred`return of: 

    fit <- glm(Accepted~Background, data=dat, family="binomial")
    exp(cbind(Odds_and_OR=coef(fit), confint(fit)))
    
                          Odds_and_OR                         2.5 %      97.5 %
    (Intercept)             0.7088608                     0.5553459   0.9017961
    Backgroundred           2.4480042                     1.7397640   3.4595454

At the same time, the `(Intercept)`corresponds to the numerator of the *odds ratio*, which is exactly the *odds* of getting in being of 'blue' family background: $112/158 = 0.7089$.

If instead, I run:

    fit2 <- glm(Accepted~Background-1, data=dat, family="binomial")
    exp(cbind(Odds=coef(fit2), confint(fit2)))

                            Odds            2.5 %      97.5 %
    Backgroundblue     0.7088608        0.5553459   0.9017961
    Backgroundred      1.7352941        1.3632702   2.2206569

The returns are precisely the *odds* of getting in being 'blue': `Backgroundblue` (0.7089) and the *odds* of being accepted being 'red':  `Backgroundred` (1.7353). No *Odds Ratio* there. Therefore the two return values are not expected to be reciprocal.

Finally, How to read the results if there are 3 factors in the categorical regressor?

Same manual versus [R] calculation:

I created a different fictitious data set with the same premise, but this time there were three ethnic backgrounds: "red", "blue" and "orange", and ran the same sequence:

First, the contingency table:

    Acceptance	blue	orange	red	  Total
    0	          86	    65	130	    281
    1	          64	    42	162	    268
    Total	     150	   107	292	    549

And calculated the *Odds* of getting in for each ethnic group:

 - Odds Accept If Red = 1.246154;
 - Odds Accept If Blue = 0.744186; 
 - Odds Accept If Orange = 0.646154

As well as the different *Odds Ratios*:

 - OR red v blue = 1.674519; 
 - OR red v orange = 1.928571; 
 - OR blue v red = 0.597186; 
 - OR blue v orange = 1.151717; 
 - OR orange v red = 0.518519; and
 - OR orange v blue = 0.868269  

And proceeded with the now routine logistic regression followed by exponentiation of coefficients:

    fit <- glm(Accepted~Background, data=dat, family="binomial")
    exp(cbind(ODDS=coef(fit), confint(fit)))

                          ODDS     2.5 %   97.5 %
    (Intercept)      0.7441860 0.5367042 1.026588
    Backgroundorange 0.8682692 0.5223358 1.437108
    Backgroundred    1.6745192 1.1271430 2.497853

Yielding the *odds* of getting in for "blues" as the `(Intercept)`, and the *Odds Ratios* of Orange versus Blue in `Backgroundorange`, and the OR of Red v Blue in `Backgroundred` .

On the other hand, the regression without intercept predictably returned just the three independent *odds*:

    fit2 <- glm(Accepted~Background-1, data=dat, family="binomial")
    exp(cbind(ODDS=coef(fit2), confint(fit2)))

                          ODDS     2.5 %    97.5 %
    Backgroundblue   0.7441860 0.5367042 1.0265875
    Backgroundorange 0.6461538 0.4354366 0.9484999
    Backgroundred    1.2461538 0.9900426 1.5715814


---

<a href="http://rinterested.github.io/statistics/index.html">Home Page</a>
